source_path: ../data/fineweb_edu
base_path:  ../output/embed
dataset_name: fineweb_edu_500

tokens:
  generate: True
  tokenizer:
    name: BAAI/bge-large-en-v1.5

# 疑似没用的参数，但先别删，因为我还没有剥离这个,而且不知道后面会不会用到这个标记
sequences:
  seq_len: 5120

# max length of contriever is 512
# 项目里会在某些地方给tensor加上两个额外的token,开头[CLS]和结尾的[SEP].项目里目前只支持bert like embed
chunks:
  generate: True
  chunk_len: null

embeddings:
  generate: true
  batch_size: 32
  # rangehow: useless,现在自动推断
  # embed_dim: 1024
  model:
    name: BAAI/bge-large-en-v1.5
    # rangehow: 不需要设置，**这个文件里的null说明这个参数会被用到，但是得靠文件推断**。
    device: null
    # useless
    # repo_or_dir: ../lib/huggingface_pytorch-transformers_main
    # skip_validation: true
    # source: local
  parallel:
    num_workers: 8
    submitit:
      submitit_path: ../output/submitit
      cluster: null
      partition: learnlab 
      cpus_per_task: 4
      # 一个节点上有几个gpu，通常是8
      gpus_per_node: 8
      slurm_time: 18
      slurm_job_name: embedding-generation
