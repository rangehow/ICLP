source_path: ../data/fineweb_edu # 用于指定数据文件夹，数据文件下所有的jsonl文件都会被使用
base_path:  ../output/embed # 输出路径
dataset_name: fineweb_edu_500 # 只是输出路径的一个标识，输出路径由tokenizer和dataset_name一起形成

tokens:
  generate: True
  tokenizer:
    name: BAAI/bge-large-en-v1.5

# 疑似没用的参数，但先别删，因为我还没有剥离这个,而且不知道后面会不会用到这个标记
sequences:
  seq_len: 5120

# max length of contriever is 512
# 项目里会在某些地方给tensor加上两个额外的token,开头[CLS]和结尾的[SEP].项目里目前只支持bert like embed
chunks:
  generate: True
  chunk_len: null

embeddings:
  generate: True
  batch_size: 256
  # rangehow: useless,现在自动推断
  # embed_dim: 1024
  model:
    name: BAAI/bge-large-en-v1.5
    # rangehow: 不需要设置，**这个文件里的null说明这个参数会被用到，但是已经全部实现了自动推断**。
    device: null
    # useless
    # repo_or_dir: ../lib/huggingface_pytorch-transformers_main
    # skip_validation: true
    # source: local
  parallel:
    num_workers: 1
    submitit:
      submitit_path: ../output/submitit
      cluster: null
      partition: learnlab 
      cpus_per_task: 8
      # 一个节点上有几个gpu，通常是8
      gpus_per_node: 4
      slurm_time: 14-00:00:00
      slurm_job_name: embedding-generation

# 完备性检查，很快，没啥耗时，建议开启。
enable_completeness_check: True