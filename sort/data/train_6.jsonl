{"text": "In 1962 President John F. Kennedy\u2019s administration narrowly averted possible nuclear war with the USSR, when CIA operatives spotted Soviet surface-to-surface missiles in Cuba, after a six-week gap in intelligence-gathering flights.\nIn their forthcoming book Blind over Cuba: The Photo Gap and the Missile Crisis, co-authors David Barrett and Max Holland make the case that the affair was a close call stemming directly from a decision made in a climate of deep distrust between key administration officials and the intelligence community.\nUsing recently declassified documents, secondary materials, and interviews with several key participants, the authors weave a story of intra-agency conflict, suspicion, and discord that undermined intelligence-gathering, adversely affected internal postmortems conducted after the crisis peaked, and resulted in keeping Congress and the public in the dark about what really happened.\nWe asked Barrett, a professor of political science at Villanova University, to discuss the actual series of events and what might have happened had the CIA not detected Soviet missiles on Cuba.\nThe Actual Sequence of Events . . .\n\u201cSome months after the Cuban Missile Crisis, an angry member of the Armed Services Committee of the House of Representatives criticized leaders of the Kennedy administration for having let weeks go by in September and early October 1962, without detecting Soviet construction of missile sites in Cuba. It was an intelligence failure as serious as the U.S. ignorance that preceded the Japanese attack on Pearl Harbor in 1941, he said.\nSecretary of Defense Robert McNamara aggressively denied that there had been an American intelligence failure or ineptitude with regard to Cuba in late summer 1962. McNamara and others persuaded most observers the administration\u2019s performance in the lead-up to the Crisis had been almost flawless, but the legislator was right: The CIA had not sent a U-2 spy aircraft over western Cuba for about a six week period.\nThere were varying reasons for this, but the most important was that the Kennedy administration did not wish to have a U-2 \u201cincident.\u201d Sending that aircraft over Cuba raised the possibility that Soviet surface-to-air missiles might shoot one down. Since it was arguably against international law for the U.S. to send spy aircrafts over another country, should one be shot down, there would probably be the same sort of uproar as happened in May 1960, when the Soviet Union shot down an American U-2 flying over its territory.\nFurthermore, most State Department and CIA authorities did not believe that the USSR would put nuclear-armed missiles into Cuba that could strike the U.S. Therefore, the CIA was told, in effect, not even to request permission to send U-2s over western Cuba. This, at a time when there were growing numbers of reports from Cuban exiles and other sources about suspicious Soviet equipment being brought into the country.As we now know, the Soviets WERE constructing missile sites on what CIA deputy director Richard Helms would call \u201cthe business end of Cuba,\u201d i.e., the western end, in the summer/autumn of 1962. Fortunately, by mid-October, the CIA\u2019s director, John McCone, succeeded in persuading President John F. Kennedy to authorize one U-2 flight over that part of Cuba and so it was that Agency representatives could authoritatively inform JFK on October 16th that the construction was underway.The CIA had faced White House and State Department resistance for many weeks about this U-2 matter.\"\nWhat Could Have Happened . . .\n\u201cWhat if McCone had not succeeded in persuading the President that the U.S. needed to step up aerial surveillance of Cuba in mid-October? What if a few more weeks had passed without that crucial October 14 U-2 flight and its definitive photography of Soviet missile site construction?\nRemember to check out Blind over Cuba: The Photo Gap and the Missile Crisis, which is being published this fall!If McCone had been told \u201cno\u201d in the second week of October, perhaps it would have taken more human intelligence, trickling in from Cuba, about such Soviet activity before the President would have approved a risky U-2 flight.The problem JFK would have faced then is that there would have been a significant number of operational medium-range missile launch sites. Those nuclear-equipped missiles could have hit the southern part of the U.S. Meanwhile, the Soviets would also have progressed further in construction of intermediate missile sites; such missiles could have hit most of the continental United States.If JFK had not learned about Soviet nuclear-armed missiles until, say, November 1st, what would the U.S. have done?There is no definitive answer to that question, but I think it\u2019s fair to say that the President would have been under enormous pressure to authorize\u2014quickly--a huge U.S. air strike against Cuba, followed by an American invasion. One thing which discovery of the missile sites in mid-October gave JFK was some time to negotiate effectively with the Soviet Union during the \u201cThirteen Days\u201d of the crisis. I don\u2019t think there would have been such a luxury if numerous operational missiles were discovered a couple weeks later.No wonder President Kennedy felt great admiration and gratitude toward those at the CIA (with its photo interpreters) and the Air Force (which piloted the key U-2 flight). The intelligence he received on October 16th was invaluable. I think he knew that if that intelligence had not come until some weeks later, there would have been a much greater chance of nuclear war between the U.S. and the Soviet Union.\u201d", "id": "<urn:uuid:7da5e687-07e2-4c8f-9fac-fe3f58c7017a>", "dump": "CC-MAIN-2013-20", "url": "http://tamupress.blogspot.com/2012/07/close-call-what-if-cia-had-not-spotted.html", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9771577715873718, "token_count": 1150, "score": 2.828125, "int_score": 3}
{"text": "Assistant Secretary of State for Inter-American Affairs Edwin Martin seeks a resolution of support from the Organization of American States. Ambassador to the United Nations Adlai Stevenson lays the matter before the U.N. Security Council. The ships of the naval quarantine fleet move into place around Cuba. Soviet submarines threaten the quarantine by moving into the Caribbean area. Soviet freighters bound for Cuba with military supplies stop dead in the water, but the oil tanker Bucharest continues towards Cuba. In the evening Robert Kennedy meets with Ambassador Dobrynin at the Soviet Embassy.\nAfter the Organization of American States endorsed the quarantine, President Kennedy asks Khrushchev to halt any Russian ships heading toward Cuba. The president's greatest concern is that a US Navy vessel would otherwise be forced to fire upon a Russian vessel, possibly igniting war between the superpowers.", "id": "<urn:uuid:3cdc115d-ed7c-46dc-bae1-ab6e1ae42215>", "dump": "CC-MAIN-2013-20", "url": "http://microsites.jfklibrary.org/cmc/oct23/", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9182275533676147, "token_count": 167, "score": 2.515625, "int_score": 3}
{"text": "Public Papers - 1991\nWhite House Fact Sheet on The Strategic Arms Reduction Treaty (START)\nToday, the United States and the Soviet Union signed the Strategic Arms Reduction Treaty. This treaty marks the first agreement between the two countries in which the number of deployed strategic nuclear weapons will actually be reduced. Reductions will take place over a period of 7 years, and will result in parity between the strategic nuclear forces of the two sides at levels approximately 30 percent below currently deployed forces. Deeper cuts are required in the most dangerous and destabilizing systems.\nSTART provisions are designed to strengthen strategic stability at lower levels and to encourage the restructuring of strategic forces in ways that make them more stable and less threatening. The treaty includes a wide variety of very demanding verification measures designed to ensure compliance and build confidence.\nThe treaty sets equal ceilings on the number of strategic nuclear forces that can be deployed by either side. In addition, the treaty establishes an equal ceiling on ballistic missile throw-weight (a measure of overall capability for ballistic missiles). Each side is limited to no more than:\n-- 1600 strategic nuclear delivery vehicles (deployed intercontinental ballistic missiles [ICBM's], submarine launched ballistic missiles [SLBM's], and heavy bombers), a limit that is 36 percent below the Soviet level declared in September 1990 and 29 percent below the U.S. level.\n-- 6000 total accountable warheads, about 41 percent below the current Soviet level and 43 percent below the current U.S. level.\n-- 4900 accountable warheads deployed on ICBM's or SLBM's, about 48 percent below the current Soviet level and 40 percent below the current U.S. level.\n-- 1540 accountable warheads deployed on 154 heavy ICBM's, a 50-percent reduction in current Soviet forces. The U.S. has no heavy ICBM's.\n-- 1100 accountable warheads deployed on mobile ICBM's.\n-- Aggregate throw-weight of deployed ICBM's and SLBM's equal to about 54 percent of the current Soviet aggregate throw-weight.\nBallistic Missile Warhead Accountability\nThe treaty uses detailed counting rules to ensure the accurate accounting of the number of warheads attributed to each type of ballistic missile.\n-- Each deployed ballistic missile warhead counts as 1 under the 4900 ceiling and 1 under the 6000 overall warhead ceiling.\n-- Each side is allowed 10 on-site inspections each year to verify that deployed ballistic missiles contain no more warheads than the number that is attributed to them under the treaty.\nDownloading Ballistic Missile Warheads\nThe treaty also allows for a reduction in the number of warheads on certain ballistic missiles, which will help the sides transition their existing forces to the new regime. Such downloading is permitted in a carefully structured and limited fashion.\n-- The U.S. may download its three-warhead Minuteman III ICBM by either one or two warheads. The Soviet Union has already downloaded it's seven warhead SS - N - 18 SLBM by four warheads.\n-- In addition, each side may download up to 500 warheads on two other existing types of ballistic missiles, as long as the total number of warheads removed from downloaded missiles does not exceed 1250 at any one time.\nThe treaty places constraints on the characteristics of new types of ballistic missiles to ensure the accuracy of counting rules and prevent undercounting of missile warheads.\n-- The number of warheads attributed to a new type of ballistic missile must be no less than the number determined by dividing 40 percent of the missile's total throw-weight by the weight of the lightest RV tested on that missile.\n-- The throw-weight attributed to a new type must be no less than the missile's throw-weight capability at specified reference ranges (11,000 km for ICBM's and 9,500 km for SLBM's).\nSTART places significant restrictions on the Soviet SS - 18 heavy ICBM.\n-- A 50-percent reduction in the number of Soviet SS - 18 ICBM's; a total reduction of 154 of these Soviet missiles.\n-- New types of heavy ICBM's are banned.\n-- Downloading of heavy ICBM's is banned.\n-- Heavy SLBM's and heavy mobile ICBM's are banned.\n-- Heavy ICBM's will be reduced on a more stringent schedule than other strategic arms.\nBecause mobile missiles are more difficult to verify than other types of ballistic missiles, START incorporates a number of special restrictions and notifications with regard to these missiles. These measures will significantly improve our confidence that START will be effectively verifiable.\n-- Nondeployed mobile missiles and non-deployed mobile launchers are numerically and geographically limited so as to limit the possibility for reload and refire.\n-- The verification regime includes continuous monitoring of mobile ICBM production, restrictions on movements, on-site inspections, and cooperative measures to improve the effectiveness of national technical means of intelligence collection.\nBecause heavy bombers are stabilizing strategic systems (e.g., they are less capable of a short-warning attack than ballistic missiles), START counting rules for weapons on bombers are different than those for ballistic missile warheads.\n-- Each heavy bomber counts as one strategic nuclear delivery vehicle.\n-- Each heavy bomber equipped to carry only short-range missiles or gravity bombs is counted as one warhead under the 6000 limit.\n-- Each U.S. heavy bomber equipped to carry long-range nuclear ALCM's (up to a maximum of 150 bombers) is counted as 10 warheads even though it may be equipped to carry up to 20 ALCM's.\n-- A similar discount applies to Soviet heavy bombers equipped to carry long-range nuclear ALCM's. Each such Soviet heavy bomber (up to a maximum of 180) is counted as 8 warheads even though it may be equipped to carry up to 16 ALCM's.\n-- Any heavy bomber equipped for long-range nuclear ALCM's deployed in excess of 150 for the U.S. or 180 for the Soviet Union will be accountable by the number of ALCM's the heavy bomber is actually equipped to carry.\nBuilding on recent arms control agreements, START includes extensive and unprecedented verification provisions. This comprehensive verification regime greatly reduces the likelihood that violations would go undetected.\n-- START bans the encryption and encapsulation of telemetric information and other forms of information denial on flight tests of ballistic missiles. However, strictly limited exemptions to this ban are granted sufficient to protect the flight-testing of sensitive research projects.\n-- START allows 12 different types of on-site inspections and requires roughly 60 different types of notifications covering production, testing, movement, deployment, and destruction of strategic offensive arms.\nSTART will have a duration of 15 years, unless it is superseded by a subsequent agreement. If the sides agree, the treaty may be extended for successive 5-year periods beyond the 15 years.\nNoncircumvention and Third Countries\nSTART prohibits the transfer of strategic offensive arms to third countries, except that the treaty will not interfere with existing patterns of cooperation. In addition, the treaty prohibits the permanent basing of strategic offensive arms outside the national territory of each side.\nAir-Launched Cruise Missiles (ALCM's)\nSTART does not directly count or limit ALCM's. ALCM's are limited indirectly through their association with heavy bombers.\n-- Only nuclear-armed ALCM's with a range in excess of 600 km are covered by START.\n-- Long-range, conventionally armed ALCM's that are distinguishable from nuclear-armed ALCM's are not affected.\n-- Long-range nuclear-armed ALCM's may not be located at air bases for heavy bombers not accountable as being equipped for such ALCM's.\n-- Multiple warhead long-range nuclear ALCM's are banned.\nSea Launched Cruise Missiles (SLCM's)\nSLCMs are not constrained by the treaty. However, each side has made a politically binding declaration as to its plans for the deployment of nuclear-armed SLCM's. Conventionally-armed SLCM's are not subject to such a declaration.\n-- Each side will make an annual declaration of the maximum number of nuclear-armed SLCM's with a range greater than 600 km that it plans to deploy for each of the following 5 years.\n-- This number will not be greater than 880 long-range nuclear-armed SLCM's.\n-- In addition, as a confidence building measure, nuclear-armed SLCM's with a range of 300 - 600 km will be the subject of a confidential annual data exchange.\nThe Soviet Backfire bomber is not constrained by the treaty. However, the Soviet side has made a politically binding declaration that it will not deploy more than 800 air force and 200 naval Backfire bombers, and that these bombers will not be given intercontinental capability.\nThe START agreement consists of the treaty document itself and a number of associated documents. Together they total more than 700 pages. The treaty was signed in a public ceremony by Presidents Bush and Gorbachev in St. Vladimir's Hall in the Kremlin. The associated documents were signed in a private ceremony at Novo Ogaryevo, President Gorbachev's weekend dacha. Seven of these documents were signed by Presidents Bush and Gorbachev. Three associated agreements were signed by Secretary Baker and Foreign Minister Bessmertnykh. In addition, the START negotiators, Ambassadors Brooks and Nazarkin, exchanged seven letters related to START in a separate event at the Soviet Ministry of Foreign Affairs in Moscow.\nMagnitude of START -- Accountable Reductions\nFollowing is the aggregate data from the Memorandum of Understanding, based upon agreed counting rules in START. (Because of those counting rules, the number of heavy bomber weapons actually deployed may be higher than the number shown in the aggregate.) This data is effective as of September 1990\n(TABLE START)and will be updated at entry into force:\nDelivery Vehicles .... 2,246 .... 2,500\nWarheads .... 10,563 .... 10,271\nBallistic Missile Warheads .... 8,210 .... 9,416\nHeavy ICBM's/Warheads .... None .... 308/3080\nThrow-weight (metric tons) .... 2,361.3 .... 6,626.3\nAs a result of the treaty, the above values will be reduced by the following percentages:\nDelivery Vehicles .... 29 percent .... 36 percent\nWarheads .... 43 percent .... 41 percent\nBallistic Missile Warheads .... 40 percent .... 48 percent\nHeavy ICBM's/Warheads .... None .... 50 percent\nThrow-weight (metric tons) .... None .... 46 percent", "id": "<urn:uuid:61022017-5b85-4840-958f-d37f75698705>", "dump": "CC-MAIN-2013-20", "url": "http://bushlibrary.tamu.edu/research/public_papers.php?id=3263&year=1991&month=all", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.924267053604126, "token_count": 2184, "score": 2.625, "int_score": 3}
{"text": "United Nations Special Commission\nUnited Nations Special Commission (UNSCOM) was an inspection regime created by the United Nations to ensure Iraq's compliance with policies concerning Iraqi production and use of weapons of mass destruction after the Gulf War. Between 1991 and 1997 its director was Rolf Ek\u00e9us; from 1997 to 1999 its director was Richard Butler.\nUnited Nations Special Commission (UNSCOM) was an inspection regime created with the adoption of United Nations Security Council Resolution 687 in April 1991 to oversee Iraq's compliance with the destruction of Iraqi chemical, biological, and missile weapons facilities and to cooperate with the International Atomic Energy Agency\u2019s efforts to eliminate nuclear weapon facilities all in the aftermath of the Gulf War. The UNSCOM inspection regime was packaged with several other UN Security Council requirements, namely, that Iraq\u2019s ruling regime formally recognize Kuwait as an independent state and pay out war reparations for the destruction inflicted in the Gulf War, including the firing of Kuwaiti oil supplies and destruction of public infrastructure. Until the UN Security Council saw that Iraq\u2019s weapons programs had been aborted and Iraqi leaders had allowed monitoring systems to be installed, the UN\u2019s aforementioned sanctions would continue to be imposed on Iraq.\nThe commission found corroborating evidence that Rihab Rashid Taha, an Iraqi microbiologist educated in England, had produced biological weapons for Iraq in the 1980s. The destruction of proscribed weapons and the associated facilities was carried out mainly by Iraq, under constant supervision by UNSCOM.\nInspectors withdrew in 1998, and disbanded the following year amid allegations that the United States had used the commission's resources to spy on the Iraqi military. Weapons inspector Scott Ritter later stated that Operation Rockingham had cherry-picked evidence found by the United Nations Special Commission; evidence, he says, that was later used as part of the casus belli for the 2003 invasion of Iraq.\nThe successor of the United Nations Special Commission was the United Nations Monitoring, Verification and Inspection Commission.\nThe United Nations Special Commission (UNSCOM) was headed by Rolf Ek\u00e9us and later Richard Butler. During several visits to Iraq by the United Nations Special Committee (UNSCOM), set up after the 1990 invasion of Kuwait to inspect Iraqi weapons facilities, weapons inspectors were told by Rihab Rashid Taha that the al-Hakam germ warfare center was a chicken-feed plant. \"There were a few things that were peculiar about this animal-feed production plant,\" Charles Duelfer, UNSCOM's deputy executive chairman, later told reporters, \"beginning with the extensive air defenses surrounding it.\"\nThe powers given to UNSCOM inspectors in Iraq were: \u201cunrestricted freedom of movement without advance notice in Iraq\u201d; the \u201cright to unimpeded access to any site or facility for the purpose of the on-site inspection...whether such site or facility be above or below ground\u201d; \u201cthe right to request, receive, examine, and copy any record data, or information...relevant to\u201d UNSCOM\u2019s activities; and the \u201cright to take and analyze samples of any kind as well as to remove and export samples for off-site analysis.\nAcceptance of the intrusion of the UNSCOM\u2019s inspectors on the part of the Iraqi regime was slow coming. But with the threat of punitive military action looming from the international community, and particularly the U.S., Saddam Husain begrudgingly allowed UNSCOM\u2019s inspectors into the country to begin their work.\nBetween 1991 and 1995, UN inspectors uncovered a massive program to develop biological and nuclear weapons. A large amount of equipment was confiscated and destroyed. Iraq by and large refused to cooperate with UNSCOM and its inspections as mandated by UN SC Res. 687 until June 1992, ten months after deadline, at which time the Iraqi government submitted \u201cfull, final and complete reports\u201d on all of its weapons of mass destruction programs. These reports, however, were found to be incomplete and deficient, and at the same time UN inspectors were subjected to harassment and threats on the part of the Iraqi regime.\nUnited Nations Security Council Resolution 699 was also passed in 1991, declaring that Iraq was responsible for all funding of UNSCOM\u2019s inspections in Iraq.\nIn 1995, UNSCOM's principal weapons inspector Dr. Rod Barton showed Taha documents obtained by UNSCOM from Israel that showed the Iraqi government had just purchased 10 tons of growth media from a British company called Oxoid. Growth media is a mixture of sugar, proteins and minerals that allows microscopic life to grow. It is used in hospitals, where swabs from patients are placed in dishes containing growth media for diagnostic purposes. Iraq's hospital consumption of growth media was just 200 kg a year; yet in 1988, Iraq imported 39 tons of it.\nShown this evidence by UNSCOM, Taha admitted to inspectors that she had grown 19,000 litres of botulism toxin; 8,000 litres of anthrax; 2,000 litres of aflatoxins, which can cause liver cancer; clostridium perfringens, a bacterium that can cause gas gangrene; and ricin, a castor bean derivative which can kill by inhibiting protein synthesis. She also admitted conducting research into cholera, salmonella, foot and mouth disease, and camel pox, a disease that uses the same growth techniques as smallpox, but which is safer for researchers to work with. It was because of the discovery of Taha's work with camel pox that the US and British intelligence services feared Saddam Hussein may have been planning to weaponize the smallpox virus. Iraq had a smallpox outbreak in the 1970s and UNSCOM scientists believe the government would have retained contaminated material.\nUNSCOM learned that, in August 1990, after Iraq's invasion of Kuwait, Taha's team was ordered to set up a program to weaponize the biological agents. By January 1991, a team of 100 scientists and support staff had filled 157 bombs and 16 missile warheads with botulin toxin, and 50 bombs and five missile warheads with anthrax. In an interview with the BBC, Taha denied the Iraqi government had weaponized the bacteria. \"We never intended to use it,\" she told journalist Jane Corbin of the BBC's Panorama program. \"We never wanted to cause harm or damage to anybody.\" UNSCOM found the munitions dumped in a river near al-Hakam. UNSCOM also discovered that Taha's team had conducted inhalation experiments on donkeys from England and on beagles from Germany. The inspectors seized photographs showing beagles having convulsions inside sealed containers.\nThe al-Hakam germ warfare center, headed by the British-educated Iraqi biologist Dr. Rihab Rashid Taha, was blown up by UNSCOM in 1996. According to a 1999 report from the U.S. Defense Intelligence Agency, the normally mild-mannered Taha exploded into violent rages whenever UNSCOM questioned her about al-Hakam, shouting, screaming and, on one occasion, smashing a chair, while insisting that al-Hakam was a chicken-feed plant.\nIraq charged that the commission was a cover for US espionage and refused UNSCOM access to certain sites, such as Baath Party headquarters. Although Ek\u00e9us has said that he resisted attempts at such espionage, many allegations have since been made against the agency commission under Butler, charges which Butler has denied. Within the UN establishment in Iraq, UNSCOM was not without its critics, with the UN's humanitarian staff informally calling the inspectors 'UN-Scum'. In return, the UN's humanitarian staff were called \"bunny-huggers\".\nAlso in 1996, the Iraqi ruling regime agreed to the terms of United Security Council Resolution 986, an oil-for-supplies agreement in which Iraq was allowed to sell $2 billion worth of oil every six months as a ways to purchase supplies for its increasingly impoverished and malnourished population. This agreement also allowed the UN to oversee the use and management of oil revenues, and to see that some of the funds went to pay war reparations and for the work of UNSCOM in Iraq during this period. The distribution of supplies purchased with oil revenues was also to be supervised by UN inspectors to ensure fair and equal distribution throughout the Iraqi population.\n 1998 Airstrikes\nSecurity Council Meeting On the evening of 15 December 1998 the Security Council convened to consider two letters from weapons inspectors. The IAEA report by Mohamed El Baradei stated that Iraq \"has provided the necessary level of cooperation to enable... [our] activities to be completed efficiently and effectively\". The UNSCOM report, authored by Richard Butler, deplored the restrictions, lack of disclosure, and concealment. While conceding that \"[i]n statistical terms, the majority of the inspections of facilities and sites under the ongoing monitoring system were carried out with Iraq's cooperation,\" his letter listed a number of instances where unspecificed \"undeclared dual-capable items\" had been discovered, and where inspections had been held up so that buildings could be cleared of sensitive material.\nSince Operation Desert Fox had already begun at the time of the meeting (just hours after the inspectors had been evacuated), the Security Council debated about who was to blame for the military action, rather than whether they should authorize it. The Iraqi representative said:\n|\u201c||I speak to you now while rockets and bombs are falling on the cities and the villages of Iraq... At a time when the Security Council... was discussing [the] reports..., and before the Council reached any conclusion on this subject, the United States and Britain launched their attack against Iraq. The two Powers requested a suspension of the informal meeting of the Security Council and their pretext for aggression was that one of the two reports -- the UNSCOM report -- emphasized the lack of full cooperation by Iraq with UNSCOM... Time and again we have warned against the partiality and lack of objectivity of the United Nations Special Commission... The UNSCOM Executive Chairman singled out in his report yesterday five incidents out of a total of 300 inspection operations... The exaggerated uproar about Iraqi weapons of mass destruction is nothing but a great lie.||\u201d|\nThe Russian ambassador added:\n|\u201c||We believe that although there are certain problems..., the current crisis was created artificially... On the night of 15 December this year, [Butler] presented a report that gave a distorted picture of the real state of affairs and concluded that there was a lack of full cooperation on the part of Iraq. That conclusion was not borne out by the facts. Without any consultations with the Security Council, Richard Butler then evacuated the entire Special Commission staff from Iraq. At the same time, there was an absolutely unacceptable leak of the report to the communications media, which received the text before the members of the Security Council themselves... It is symbolic that precisely at the time when Richard Butler... was attempting to defend the conclusions reached in his report, we were informed about the strike against Iraq, and the justification for that unilateral act was precisely the report which had been presented by the Executive Chairman of the Special Commission.||\u201d|\nThe view of the Council was split, with several countries placing the responsibility on Iraq. The United States declared that \"Iraq's policy of unremitting defiance and non-compliance necessitated the resort to military force\". The United Kingdom stated that the objectives of the action were \"to degrade Iraq's capability to build and use weapons of mass destruction, and to diminish the military threat Iraq poses to its neighbours. The targets chosen, therefore, are targets connected with his military capability, his weapons of mass destruction and his ability to threaten his neighbours.\"\n 1999: End of UNSCOM\nIn December 1999, the UN Security Council passed Resolution 1284, replacing UNSCOM with the United Nations Monitoring, Verification and Inspection Commission. Four countries \u2014 among them Russia, France and China \u2014 abstained from voting on Res. 1284, which led the Iraqi regime to reject the resolution because they saw the resolution as a way for the UN to claim Iraq as a \u201cprotectorate.\u201d\nUNSCOM\u2019s intention of identifying and eliminating Iraqi weapons programs resulted in numerous successes, illustrating the \u201cvalue of a system approach to biological arms verification,\u201d as former UNSCOM Historian Stephen Black has written. But the overall effect of the UN sanctions on Iraqi in the 1990s proved devastating to an already crumbling country. Malnutrition rates among Iraqis increased and infant mortality rates soared, exacting a heavy toll on the people of Iraq not part of the ruling regime\u2019s patrimonial \u201cshadow state.\u201d\n Allegations of CIA infiltration of UNSCOM\nEvidence that UNSCOM had been used by US intelligence to penetrate Iraqi security and track President Saddam Hussein's movements emerged in January 1999. An investigation by the Washington Post claimed that CIA engineers, working as UN technicians, installed equipment to spy on Iraqi sites without Butler's knowledge, and that this explained the unidentified \"burst transmissions\" that had been noted by the inspectors.\nFormer UN weapons inspector Scott Ritter later accused some UNSCOM personnel of spying, and also alleged that the purpose of the spying was to target Saddam in the bombing. Butler, on the other hand, denied allegations that foreign intelligence agencies \"piggybacked\" UNSCOM and questioned the factual accuracy of several of Ritter's statements.\nOn 31 August 1998, Ritter said: \"Iraq still has proscribed weapons capability. There needs to be a careful distinction here. Iraq today is challenging the special commission to come up with a weapon and say where is the weapon in Iraq, and yet part of their efforts to conceal their capabilities, I believe, have been to disassemble weapons into various components and to hide these components throughout Iraq. I think the danger right now is that without effective inspections, without effective monitoring, Iraq can in a very short period of time measure the months, reconstitute chemical biological weapons, long-range ballistic missiles to deliver these weapons, and even certain aspects of their nuclear weaponization program.\"\nAlmost a year later, in June 1999, Ritter responded to an interviewer saying: \"When you ask the question, 'Does Iraq possess militarily viable biological or chemical weapons?' the answer is no! It is a resounding NO. Can Iraq produce today chemical weapons on a meaningful scale? No! Can Iraq produce biological weapons on a meaningful scale? No! Ballistic missiles? No! It is 'no' across the board. So from a qualitative standpoint, Iraq has been disarmed. Iraq today possesses no meaningful weapons of mass destruction capability.\nButler resigned from UNSCOM on 30 June 1999.\n See also\n- In Shifting Sands: The Truth About Unscom and the Disarming of Iraq - documentary film directed by Scott Ritter\n- Iraq disarmament crisis and Iraq disarmament timeline 1990\u20132003\n- UNSCOM personnel: Rolf Ek\u00e9us, Richard Butler (diplomat), Charles A. Duelfer, Scott Ritter, Corinne Heraud, Alexander Coker\n- United Nations Security Council Resolution 687 S-RES-687(1991) page 3 on 3 April 1991 (retrieved 2008-04-10)\n- Zilinskas, Raymond A., \u201cUNSCOM and the UNSCOM Experience in Iraq,\u201d Politics and the Life Sciences, Vol. 14, No. 2 (Aug., 1995), 230-231\n- Tripp, Charles, \u201cA History of Iraq,\u201d (New York: Cambridge University Press, 2008), 250\n- What Happened to Saddam's WMD? Arms Control Today September, 2003\n- Chief U.N. weapons inspector rejects spying allegations CNN January 6, 1999\n- US silence on new Iraq spying allegations BBC News January 7, 1999\n- Black, Stephen, \u201cUNSCOM and the Iraqi Biological Weapons Program: Implications for Arms Control,\u201d Politics and the Life Sciences,\u201d Vol. 18, No. 1 (Mar., 1999), pp. 62-63\n- Tripp, 250-251\n- Zilinskas, 230\n- \"The Inspections Maze\". Christian Science Monitor. 2002. Retrieved 2006-04-28.[dead link]\n- \"Baghdad prevents inspections at Baath party headquarters\". Arabic News.com. 12/11/1998. Retrieved 2006-04-28.\n- Wright, Susan (2002). Biological Warfare and Disarmament. Lanham: Rowman & Littlefield. p. 272. ISBN 0-7425-2469-8.\n- Varadarajan, Siddharth (26 February 1998). \"'UNSCUM' versus 'Bunny-huggers' in Iraq\". The Times of India.\n- Tripp, 252\n- United Nations Security Council PV S-PV-3955 on 1998-12-16 (retrieved 2007-04-04)\n- \"LETTER DATED 15 DECEMBER 1998 FROM THE SECRETARY-GENERAL ADDRESSED TO THE PRESIDENT OF THE SECURITY COUNCIL\". Un.org. Retrieved 2011-06-19.\n- United Nations Security Council S-1998-1172 on 1998-12-15 (retrieved 2007-04-04)\n- Julian Borger (17 December 1998). \"Missile blitz on Iraq\". The Guardian.\n- United Nations Security Council PV S-PV-3955 page 2 on 1998-12-16 (retrieved 2007-04-04)\n- United Nations Security Council PV S-PV-3955 page 3 on 1998-12-16 (retrieved 2007-04-04)\n- United Nations Security Council PV S-PV-3955 page 8 on 1998-12-16 (retrieved 2007-04-04)\n- United Nations Security Council PV S-PV-3955 page 5 on 1998-12-16 (retrieved 2007-04-04)\n- Tripp, 269\n- Black, 68\n- Graham-Brown, Sarah, \u201cSanctioning Iraq: A Failed Policy, Middle East Report, No. 215 (Summer, 2000), pp. 8-10\n- Halliday, Denis J., \u201cThe Impact of the UN Sanctions on the People of Iraq,\u201d Journal of Palestine Studies, Vol. 28, No. 2 (Winter, 1999), pp. 29-33\n- Mark Tran (7 January 1999). \"UN 'spied on Iraq'\". The Guardian.\n- Barton Gellman (2 March 1999). \"U.S. Spied On Iraq Via U.N\". Washington Post.\n- Julian Borger (3 March 1999). \"UN 'kept in dark' about US spying in Iraq\". The Guardian.\n- \"Unscom 'infiltrated by spies'\". BBC News. March 23, 1999. Retrieved 2006-04-28.\n- \"The Lessons and Legacy of UNSCOM, an Interview with Ambassador Richard Butler\". Arms Control Today. June 1999.\n- Arons, Nicholas (June 24, 1999). \"Interview with Scot Ritter\". Federation of American Scientists, June 24, 1999. Retrieved 2008-09-06.", "id": "<urn:uuid:33a84d84-31b8-48b1-b1fd-ec91710fe8fe>", "dump": "CC-MAIN-2013-20", "url": "http://en.wikipedia.org/wiki/UNSCOM", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9486304521560669, "token_count": 3988, "score": 2.90625, "int_score": 3}
{"text": "Wars have given us the Jeep, the computer and even the microwave.\nWill the war in Iraq give us the Tiger?\nMilitary scientists at Edgewood Chemical Biological Center at Aberdeen Proving Ground hope so. The machine - its full name is the Tactical Garbage to Energy Refinery - combines a chute, an engine, chemical tanks and other components, giving it the appearance of a lunar rover. It's designed to turn food and waste into fuel. If it works, it could save scores of American and Iraqi lives.\nAmong the biggest threats that soldiers face in the war in Iraq are the roadside bombs that have killed or maimed thousands since the U.S.-led invasion in 2003. Because some military bases lack a landfill, transporting garbage to dumps miles away in the desert has become a potentially fatal routine for U.S. troops and military contractors.\nThe Tiger would attempt to solve two problems at once: It would sharply reduce those trash hauls and provide the military with an alternative source of fuel.\nIt is the latest in a long line of wartime innovations, from can openers to desert boots. The conflict in Iraq has produced innovations such as \"warlocks,\" which jam electronic signals from cell phones, garage door openers and other electronic devices that insurgents use to detonate roadside bombs, according to Inventors Digest.\n\"In wartime, you're not worried about making a profit necessarily. You're worried about getting the latest technology on the street,\" said Peter Kindsvatter, a military historian at Aberdeen Proving Ground, who added that money is spent more freely for research when a nation is at war. \"Basically, you find yourself in a technology race with your enemy.\"\nThe Tiger, now being tested in Baghdad, would not be the first device to turn garbage into energy - a large incinerator near Baltimore's downtown stadiums does it. But it would be among the first to attempt to do it on a small scale. Its creators say it could one day become widely used in civilian life, following the lead of other wartime innovations.\nDuring World War II, contractors developed the Jeep to meet the military's desire for a light, all-purpose vehicle that could transport supplies.\nThe development of radar technology to spot Nazi planes led to the microwave, according to historians.\nThe World War II era also gave birth to the first electronic digital computer, the Electronic Numerical Integrator and Computer, or ENIAC. Funded by the Defense Department, the machine was built to compute ballistics tables that soldiers used to mechanically aim large guns. For years it was located at Aberdeen Proving Ground.\nThis decade, the Pentagon determined that garbage on military bases poses a serious logistical problem.\n\"When you're over in a combat area and people are shooting at you, you still have to deal with your trash,\" said John Spiller, project officer with the Army's Rapid Equipping Force, which is funding the Tiger project. \"How would you feel if somebody was shooting at you every other time you pushed it down the curb?\"\nHe and other Army officials said they could not recall any specific attacks against troops or contractors heading to dumpsites.For years, large incinerators have burned trash to generate power. Baltimore Refuse Energy Systems Co., the waste-to-energy plant near the stadiums, consumes up to 2,250 tons of refuse a day while producing steam and electricity.\nThe process is so expensive that it has only made sense to do it on a large scale, scientists say.\nThe military has spent almost $3 million on two Tiger prototypes, each weighing nearly 5 tons and small enough to fit into a 20- to 40-foot wide container. The project is being developed by scientists from the Edgewood, Va.-based Defense Life Sciences LLC and Indiana's Purdue University.\nThe biggest challenge was getting the parts to work together, said Donald Kennedy, an Edgewood spokesman. Because the Tiger is a hybrid consisting of a gasifier, bioreactor and generator, much of it is built with off-the-shelf items, including a grinder.\nAnother big challenge: expectations.\n\"When we would initially talk to people about the Tiger system, a large percentage would refuse to believe it could actually work,\" Kennedy wrote in an e-mail. \"Alternatively, a similar percentage would be so intrigued by the idea that they would demand to know when they could buy one for their neighborhood.\"\nThe Tiger works like this: A shredder rips up waste and soaks it in water. A bioreactor metabolizes the sludge into ethanol. A pelletizer compresses undigested waste into pellets that are fed into a gasification unit, which produces composite gas.\nThe ethanol, composite gas and a 10-percent diesel drip are injected into a diesel generator to produce electricity, according to scientists. It takes about six hours for the Tiger to power up. When it works, the device can power a 60-kilowatt generator.\nThe prototypes are being tested at Camp Victory in Baghdad\nInitial runs proved successful. The prototypes have been used to power an office trailer. At their peak, they could power two to three trailers.\nIn recent weeks, the scientists suffered a setback: The above-100 degree temperatures caused a chiller device to overheat and shut off occasionally. A new chiller from Edgewood just arrived at the site, Kennedy said.\nAfter the 90-day testing phase that ends Aug. 10, the Army will decide whether to fund the project further.\nIts developers envision the device being used to respond to crises such as Hurricane Katrina, when there is no lack of garbage but a great need for electricity.\nSpiller, of the Army's Rapid Equipping Force, said he is optimistic.\n\"The mere fact we wrote a check means we think it's got a high chance of success,\" Spiller said.", "id": "<urn:uuid:749f4f2e-01bf-42ab-ac03-1dfa84af34dc>", "dump": "CC-MAIN-2013-20", "url": "http://articles.baltimoresun.com/2008-07-21/news/0807200131_1_garbage-aberdeen-proving-ground-war-in-iraq", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9598866701126099, "token_count": 1205, "score": 2.828125, "int_score": 3}
{"text": "How We Found the Missing Memristor\nThe memristor--the functional equivalent of a synapse--could revolutionize circuit design\nImage: Bryan Christie Design\nTHINKING MACHINE This artist's conception of a memristor shows a stack of multiple crossbar arrays, the fundamental structure of R. Stanley Williams's device. Because memristors behave functionally like synapses, replacing a few transistors in a circuit with memristors could lead to analog circuits that can think like a human brain.\nIt\u2019s time to stop shrinking. Moore\u2019s Law, the semiconductor industry\u2019s obsession with the shrinking of transistors and their commensurate steady doubling on a chip about every two years, has been the source of a 50-year technical and economic revolution. Whether this scaling paradigm lasts for five more years or 15, it will eventually come to an end. The emphasis in electronics design will have to shift to devices that are not just increasingly infinitesimal but increasingly capable.\nEarlier this year, I and my colleagues at Hewlett-Packard Labs, in Palo Alto, Calif., surprised the electronics community with a fascinating candidate for such a device: the memristor. It had been theorized nearly 40 years ago, but because no one had managed to build one, it had long since become an esoteric curiosity. That all changed on 1 May, when my group published the details of the memristor in Nature.\nCombined with transistors in a hybrid chip, memristors could radically improve the performance of digital circuits without shrinking transistors. Using transistors more efficiently could in turn give us another decade, at least, of Moore\u2019s Law performance improvement, without requiring the costly and increasingly difficult doublings of transistor density on chips. In the end, memristors might even become the cornerstone of new analog circuits that compute using an architecture much like that of the brain.\nFor nearly 150 years, the known fundamental passive circuit elements were limited to the capacitor (discovered in 1745), the resistor (1827), and the inductor (1831). Then, in a brilliant but underappreciated 1971 paper, Leon Chua, a professor of electrical engineering at the University of California, Berkeley, predicted the existence of a fourth fundamental device, which he called a memristor. He proved that memristor behavior could not be duplicated by any circuit built using only the other three elements, which is why the memristor is truly fundamental.\nMemristor is a contraction of \u201dmemory resistor,\u201d because that is exactly its function: to remember its history. A memristor is a two-terminal device whose resistance depends on the magnitude and polarity of the voltage applied to it and the length of time that voltage has been applied. When you turn off the voltage, the memristor remembers its most recent resistance until the next time you turn it on, whether that happens a day later or a year later.\nThink of a resistor as a pipe through which water flows. The water is electric charge. The resistor\u2019s obstruction of the flow of charge is comparable to the diameter of the pipe: the narrower the pipe, the greater the resistance. For the history of circuit design, resistors have had a fixed pipe diameter. But a memristor is a pipe that changes diameter with the amount and direction of water that flows through it. If water flows through this pipe in one direction, it expands (becoming less resistive). But send the water in the opposite direction and the pipe shrinks (becoming more resistive). Further, the memristor remembers its diameter when water last went through. Turn off the flow and the diameter of the pipe \u201dfreezes\u201d until the water is turned back on.\nThat freezing property suits memristors brilliantly for computer memory. The ability to indefinitely store resistance values means that a memristor can be used as a nonvolatile memory. That might not sound like very much, but go ahead and pop the battery out of your laptop, right now\u2014no saving, no quitting, nothing. You\u2019d lose your work, of course. But if your laptop were built using a memory based on memristors, when you popped the battery back in, your screen would return to life with everything exactly as you left it: no lengthy reboot, no half-dozen auto-recovered files.\nBut the memristor\u2019s potential goes far beyond instant-on computers to embrace one of the grandest technology challenges: mimicking the functions of a brain. Within a decade, memristors could let us emulate, instead of merely simulate, networks of neurons and synapses. Many research groups have been working toward a brain in silico: IBM\u2019s Blue Brain project, Howard Hughes Medical Institute\u2019s Janelia Farm, and Harvard\u2019s Center for Brain Science are just three. However, even a mouse brain simulation in real time involves solving an astronomical number of coupled partial differential equations. A digital computer capable of coping with this staggering workload would need to be the size of a small city, and powering it would require several dedicated nuclear power plants.\nMemristors can be made extremely small, and they function like synapses. Using them, we will be able to build analog electronic circuits that could fit in a shoebox and function according to the same physical principles as a brain.\nA hybrid circuit\u2014containing many connected memristors and transistors\u2014could help us research actual brain function and disorders. Such a circuit might even lead to machines that can recognize patterns the way humans can, in those critical ways computers can\u2019t\u2014for example, picking a particular face out of a crowd even if it has changed significantly since our last memory of it.\nThe story of the memristor is truly one for the history books. When Leon Chua, now an IEEE Fellow, wrote his seminal paper predicting the memristor, he was a newly minted and rapidly rising professor at UC Berkeley. Chua had been fighting for years against what he considered the arbitrary restriction of electronic circuit theory to linear systems. He was convinced that nonlinear electronics had much more potential than the linear circuits that dominate electronics technology to this day.\nChua discovered a missing link in the pairwise mathematical equations that relate the four circuit quantities\u2014charge, current, voltage, and magnetic flux\u2014to one another. These can be related in six ways. Two are connected through the basic physical laws of electricity and magnetism, and three are related by the known circuit elements: resistors connect voltage and current, inductors connect flux and current, and capacitors connect voltage and charge. But one equation is missing from this group: the relationship between charge moving through a circuit and the magnetic flux surrounded by that circuit\u2014or more subtly, a mathematical doppelg\u00e4nger defined by Faraday\u2019s Law as the time integral of the voltage across the circuit. This distinction is the crux of a raging Internet debate about the legitimacy of our memristor [see sidebar, \u201dResistance to Memristance \u201d].\nChua\u2019s memristor was a purely mathematical construct that had more than one physical realization. What does that mean? Consider a battery and a transformer. Both provide identical voltages\u2014for example, 12 volts of direct current\u2014but they do so by entirely different mechanisms: the battery by a chemical reaction going on inside the cell and the transformer by taking a 110\u00e2\u00bf\u00bfV ac input, stepping that down to 12 V ac, and then transforming that into 12 V dc. The end result is mathematically identical\u2014both will run an electric shaver or a cellphone, but the physical source of that 12 V is completely different.\nConceptually, it was easy to grasp how electric charge could couple to magnetic flux, but there was no obvious physical interaction between charge and the integral over the voltage.\nChua demonstrated mathematically that his hypothetical device would provide a relationship between flux and charge similar to what a nonlinear resistor provides between voltage and current. In practice, that would mean the device\u2019s resistance would vary according to the amount of charge that passed through it. And it would remember that resistance value even after the current was turned off.\nHe also noticed something else\u2014that this behavior reminded him of the way synapses function in a brain.\nEven before Chua had his eureka moment, however, many researchers were reporting what they called \u201danomalous\u201d current-voltage behavior in the micrometer-scale devices they had built out of unconventional materials, like polymers and metal oxides. But the idiosyncrasies were usually ascribed to some mystery electrochemical reaction, electrical breakdown, or other spurious phenomenon attributed to the high voltages that researchers were applying to their devices.\nAs it turns out, a great many of these reports were unrecognized examples of memristance. After Chua theorized the memristor out of the mathematical ether, it took another 35 years for us to intentionally build the device at HP Labs, and we only really understood the device about two years ago. So what took us so long?\nIt\u2019s all about scale. We now know that memristance is an intrinsic property of any electronic circuit. Its existence could have been deduced by Gustav Kirchhoff or by James Clerk Maxwell, if either had considered nonlinear circuits in the 1800s. But the scales at which electronic devices have been built for most of the past two centuries have prevented experimental observation of the effect. It turns out that the influence of memristance obeys an inverse square law: memristance is a million times as important at the nanometer scale as it is at the micrometer scale, and it\u2019s essentially unobservable at the millimeter scale and larger. As we build smaller and smaller devices, memristance is becoming more noticeable and in some cases dominant. That\u2019s what accounts for all those strange results researchers have described. Memristance has been hidden in plain sight all along. But in spite of all the clues, our finding the memristor was completely serendipitous.\nIn 1995, I was recruited to HP Labs to start up a fundamental research group that had been proposed by David Packard. He decided that the company had become large enough to dedicate a research group to long-term projects that would be protected from the immediate needs of the business units. Packard had an altruistic vision that HP should \u201dreturn knowledge to the well of fundamental science from which HP had been withdrawing for so long.\u201d At the same time, he understood that long-term research could be the strategic basis for technologies and inventions that would directly benefit HP in the future. HP gave me a budget and four researchers. But beyond the comment that \u201dmolecular-scale electronics\u201d would be interesting and that we should try to have something useful in about 10 years, I was given carte blanche to pursue any topic we wanted. We decided to take on Moore\u2019s Law.\nAt the time, the dot-com bubble was still rapidly inflating its way toward a resounding pop, and the existing semiconductor road map didn\u2019t extend past 2010. The critical feature size for the transistors on an integrated circuit was 350 nanometers; we had a long way to go before atomic sizes would become a limitation. And yet, the eventual end of Moore\u2019s Law was obvious. Someday semiconductor researchers would have to confront physics-based limits to their relentless descent into the infinitesimal, if for no other reason than that a transistor cannot be smaller than an atom. (Today the smallest components of transistors on integrated circuits are roughly 45 nm wide, or about 220 silicon atoms.)\nThat\u2019s when we started to hang out with Phil Kuekes, the creative force behind the Teramac (tera-operation-per-second multiarchitecture computer)\u2014an experimental supercomputer built at HP Labs primarily from defective parts, just to show it could be done. He gave us the idea to build an architecture that would work even if a substantial number of the individual devices in the circuit were dead on arrival. We didn\u2019t know what those devices would be, but our goal was electronics that would keep improving even after the devices got so small that defective ones would become common. We ate a lot of pizza washed down with appropriate amounts of beer and speculated about what this mystery nanodevice would be.\nWe were designing something that wouldn\u2019t even be relevant for another 10 to 15 years. It was possible that by then devices would have shrunk down to the molecular scale envisioned by David Packard or perhaps even be molecules. We could think of no better way to anticipate this than by mimicking the Teramac at the nanoscale. We decided that the simplest abstraction of the Teramac architecture was the crossbar, which has since become the de facto standard for nanoscale circuits because of its simplicity, adaptability, and redundancy.\nThe crossbar is an array of perpendicular wires. Anywhere two wires cross, they are connected by a switch. To connect a horizontal wire to a vertical wire at any point on the grid, you must close the switch between them. Our idea was to open and close these switches by applying voltages to the ends of the wires. Note that a crossbar array is basically a storage system, with an open switch representing a zero and a closed switch representing a one. You read the data by probing the switch with a small voltage.\nLike everything else at the nanoscale, the switches and wires of a crossbar are bound to be plagued by at least some nonfunctional components. These components will be only a few atoms wide, and the second law of thermodynamics ensures that we will not be able to completely specify the position of every atom. However, a crossbar architecture builds in redundancy by allowing you to route around any parts of the circuit that don\u2019t work. Because of their simplicity, crossbar arrays have a much higher density of switches than a comparable integrated circuit based on transistors.\nBut implementing such a storage system was easier said than done. Many research groups were working on such a cross-point memory\u2014and had been since the 1950s. Even after 40 years of research, they had no product on the market. Still, that didn\u2019t stop them from trying. That\u2019s because the potential for a truly nanoscale crossbar memory is staggering; picture carrying around the entire Library of Congress on a thumb drive.\nOne of the major impediments for prior crossbar memory research was the small off-to-on resistance ratio of the switches (40 years of research had never produced anything surpassing a factor of 2 or 3). By comparison, modern transistors have an off-to-on resistance ratio of 10 000 to 1. We calculated that to get a high-performance memory, we had to make switches with a resistance ratio of at least 1000 to 1. In other words, in its off state, a switch had to be 1000 times as resistive to the flow of current as it was in its on state. What mechanism could possibly give a nanometer-scale device a three-orders-of-magnitude resistance ratio?\nWe found the answer in scanning tunneling microscopy (STM), an area of research I had been pursuing for a decade. A tunneling microscope generates atomic-resolution images by scanning a very sharp needle across a surface and measuring the electric current that flows between the atoms at the tip of the needle and the surface the needle is probing. The general rule of thumb in STM is that moving that tip 0.1 nm closer to a surface increases the tunneling current by one order of magnitude.\nWe needed some similar mechanism by which we could change the effective spacing between two wires in our crossbar by 0.3 nm. If we could do that, we would have the 1000:1 electrical switching ratio we needed.\nOur constraints were getting ridiculous. Where would we find a material that could change its physical dimensions like that? That is how we found ourselves in the realm of molecular electronics.\nConceptually, our device was like a tiny sandwich. Two platinum electrodes (the intersecting wires of the crossbar junction) functioned as the \u201dbread\u201d on either end of the device. We oxidized the surface of the bottom platinum wire to make an extremely thin layer of platinum dioxide, which is highly conducting. Next, we assembled a dense film, only one molecule thick, of specially designed switching molecules. Over this \u201dmonolayer\u201d we deposited a 2- to 3-nm layer of titanium metal, which bonds strongly to the molecules and was intended to glue them together. The final layer was the top platinum electrode.\nThe molecules were supposed to be the actual switches. We built an enormous number of these devices, experimenting with a wide variety of exotic molecules and configurations, including rotaxanes, special switching molecules designed by James Heath and Fraser Stoddart at the University of California, Los Angeles. The rotaxane is like a bead on a string, and with the right voltage, the bead slides from one end of the string to the other, causing the electrical resistance of the molecule to rise or fall, depending on the direction it moves. Heath and Stoddart\u2019s devices used silicon electrodes, and they worked, but not well enough for technological applications: the off-to-on resistance ratio was only a factor of 10, the switching was slow, and the devices tended to switch themselves off after 15 minutes.\nOur platinum devices yielded results that were nothing less than frustrating. When a switch worked, it was spectacular: our off-to-on resistance ratios shot past the 1000 mark, the devices switched too fast for us to even measure, and having switched, the device\u2019s resistance state remained stable for years (we still have some early devices we test every now and then, and we have never seen a significant change in resistance). But our fantastic results were inconsistent. Worse yet, the success or failure of a device never seemed to depend on the same thing.\nWe had no physical model for how these devices worked. Instead of rational engineering, we were reduced to performing huge numbers of Edisonian experiments, varying one parameter at a time and attempting to hold all the rest constant. Even our switching molecules were betraying us; it seemed like we could use anything at all. In our desperation, we even turned to long-chain fatty acids\u2014essentially soap\u2014as the molecules in our devices. There\u2019s nothing in soap that should switch, and yet some of the soap devices switched phenomenally. We also made control devices with no molecule monolayers at all. None of them switched.\nWe were frustrated and burned out. Here we were, in late 2002, six years into our research. We had something that worked, but we couldn\u2019t figure out why, we couldn\u2019t model it, and we sure couldn\u2019t engineer it. That\u2019s when Greg Snider, who had worked with Kuekes on the Teramac, brought me the Chua memristor paper from the September 1971 IEEE Transactions on Circuits Theory. \u201dI don\u2019t know what you guys are building,\u201d he told me, \u201dbut this is what I want.\u201d\nTo this day, I have no idea how Greg happened to come across that paper. Few people had read it, fewer had understood it, and fewer still had cited it. At that point, the paper was 31 years old and apparently headed for the proverbial dustbin of history. I wish I could say I took one look and yelled, \u201dEureka!\u201d But in fact, the paper sat on my desk for months before I even tried to read it. When I did study it, I found the concepts and the equations unfamiliar and hard to follow. But I kept at it because something had caught my eye, as it had Greg\u2019s: Chua had included a graph that looked suspiciously similar to the experimental data we were collecting.\nThe graph described the current-voltage (I-V) characteristics that Chua had plotted for his memristor. Chua had called them \u201dpinched-hysteresis loops\u201d; we called our I-V characteristics \u201dbow ties.\u201d A pinched hysteresis loop looks like a diagonal infinity symbol with the center at the zero axis, when plotted on a graph of current against voltage. The voltage is first increased from zero to a positive maximum value, then decreased to a minimum negative value and finally returned to zero. The bow ties on our graphs were nearly identical [see graphic, \u201dBow Ties\u201d].\nThat\u2019s not all. The total change in the resistance we had measured in our devices also depended on how long we applied the voltage: the longer we applied a positive voltage, the lower the resistance until it reached a minimum value. And the longer we applied a negative voltage, the higher the resistance became until it reached a maximum limiting value. When we stopped applying the voltage, whatever resistance characterized the device was frozen in place, until we reset it by once again applying a voltage. The loop in the I-V curve is called hysteresis, and this behavior is startlingly similar to how synapses operate: synaptic connections between neurons can be made stronger or weaker depending on the polarity, strength, and length of a chemical or electrical signal. That\u2019s not the kind of behavior you find in today\u2019s circuits.\nLooking at Chua\u2019s graphs was maddening. We now had a big clue that memristance had something to do with our switches. But how? Why should our molecular junctions have anything to do with the relationship between charge and magnetic flux? I couldn\u2019t make the connection.\nTwo years went by. Every once in a while I would idly pick up Chua\u2019s paper, read it, and each time I understood the concepts a little more. But our experiments were still pretty much trial and error. The best we could do was to make a lot of devices and find the ones that worked.\nBut our frustration wasn\u2019t for nothing: by 2004, we had figured out how to do a little surgery on our little sandwiches. We built a gadget that ripped the tiny devices open so that we could peer inside them and do some forensics. When we pried them apart, the little sandwiches separated at their weakest point: the molecule layer. For the first time, we could get a good look at what was going on inside. We were in for a shock.\nWhat we had was not what we had built. Recall that we had built a sandwich with two platinum electrodes as the bread and filled with three layers: the platinum dioxide, the monolayer film of switching molecules, and the film of titanium.\nBut that\u2019s not what we found. Under the molecular layer, instead of platinum dioxide, there was only pure platinum. Above the molecular layer, instead of titanium, we found an unexpected and unusual layer of titanium dioxide. The titanium had sucked the oxygen right out of the platinum dioxide! The oxygen atoms had somehow migrated through the molecules and been consumed by the titanium. This was especially surprising because the switching molecules had not been significantly perturbed by this event\u2014they were intact and well ordered, which convinced us that they must be doing something important in the device.\nThe chemical structure of our devices was not at all what we had thought it was. The titanium dioxide\u2014a stable compound found in sunscreen and white paint\u2014was not just regular titanium dioxide. It had split itself up into two chemically different layers. Adjacent to the molecules, the oxide was stoichiometric TiO 2 , meaning the ratio of oxygen to titanium was perfect, exactly 2 to 1. But closer to the top platinum electrode, the titanium dioxide was missing a tiny amount of its oxygen, between 2 and 3 percent. We called this oxygen-deficient titanium dioxide TiO 2-x , where x is about 0.05.\nBecause of this misunderstanding, we had been performing the experiment backward. Every time I had tried to create a switching model, I had reversed the switching polarity. In other words, I had predicted that a positive voltage would switch the device off and a negative voltage would switch it on. In fact, exactly the opposite was true.\nIt was time to get to know titanium dioxide a lot better. They say three weeks in the lab will save you a day in the library every time. In August of 2006 I did a literature search and found about 300 relevant papers on titanium dioxide. I saw that each of the many different communities researching titanium dioxide had its own way of describing the compound. By the end of the month, the pieces had fallen into place. I finally knew how our device worked. I knew why we had a memristor.\nThe exotic molecule monolayer in the middle of our sandwich had nothing to do with the actual switching. Instead, what it did was control the flow of oxygen from the platinum dioxide into the titanium to produce the fairly uniform layers of TiO 2 and TiO 2-x . The key to the switching was this bilayer of the two different titanium dioxide species [see diagram, \u201dHow Memristance Works\u201d]. The TiO 2 is electrically insulating (actually a semiconductor), but the TiO 2-x is conductive, because its oxygen vacancies are donors of electrons, which makes the vacancies themselves positively charged. The vacancies can be thought of like bubbles in a glass of beer, except that they don\u2019t pop\u2014they can be pushed up and down at will in the titanium dioxide material because they are electrically charged.\nNow I was able to predict the switching polarity of the device. If a positive voltage is applied to the top electrode of the device, it will repel the (also positive) oxygen vacancies in the TiO 2-x layer down into the pure TiO 2 layer. That turns the TiO 2 layer into TiO 2-x and makes it conductive, thus turning the device on. A negative voltage has the opposite effect: the vacancies are attracted upward and back out of the TiO 2 , and thus the thickness of the TiO 2 layer increases and the device turns off. This switching polarity is what we had been seeing for years but had been unable to explain.\nOn 20 August 2006, I solved the two most important equations of my career\u2014one equation detailing the relationship between current and voltage for this equivalent circuit, and another equation describing how the application of the voltage causes the vacancies to move\u2014thereby writing down, for the first time, an equation for memristance in terms of the physical properties of a material. This provided a unique insight. Memristance arises in a semiconductor when both electrons and charged dopants are forced to move simultaneously by applying a voltage to the system. The memristance did not actually involve magnetism in this case; the integral over the voltage reflected how far the dopants had moved and thus how much the resistance of the device had changed.\nWe finally had a model we could use to engineer our switches, which we had by now positively identified as memristors. Now we could use all the theoretical machinery Chua had created to help us design new circuits with our devices.\nTriumphantly, I showed the group my results and immediately declared that we had to take the molecule monolayers out of our devices. Skeptical after years of false starts and failed hypotheses, my team reminded me that we had run control samples without molecule layers for every device we had ever made and that those devices had never switched. And getting the recipe right turned out to be tricky indeed. We needed to find the exact amounts of titanium and oxygen to get the two layers to do their respective jobs. By that point we were all getting impatient. In fact, it took so long to get the first working device that in my discouragement I nearly decided to put the molecule layers back in.\nA month later, it worked. We not only had working devices, but we were also able to improve and change their characteristics at will.\nBut here is the real triumph. The resistance of these devices stayed constant whether we turned off the voltage or just read their states (interrogating them with a voltage so small it left the resistance unchanged). The oxygen vacancies didn\u2019t roam around; they remained absolutely immobile until we again applied a positive or negative voltage. That\u2019s memristance: the devices remembered their current history. We had coaxed Chua\u2019s mythical memristor off the page and into being.\nEmulating the behavior of a single memristor, Chua showed, requires a circuit with at least 15 transistors and other passive elements. The implications are extraordinary: just imagine how many kinds of circuits could be supercharged by replacing a handful of transistors with one single memristor.\nThe most obvious benefit is to memories. In its initial state, a crossbar memory has only open switches, and no information is stored. But once you start closing switches, you can store vast amounts of information compactly and efficiently. Because memristors remember their state, they can store data indefinitely, using energy only when you toggle or read the state of a switch, unlike the capacitors in conventional DRAM, which will lose their stored charge if the power to the chip is turned off. Furthermore, the wires and switches can be made very small: we should eventually get down to a width of around 4 nm, and then multiple crossbars could be stacked on top of each other to create a ridiculously high density of stored bits.\nGreg Snider and I published a paper last year showing that memristors could vastly improve one type of processing circuit, called a field-programmable gate array, or FPGA. By replacing several specific transistors with a crossbar of memristors, we showed that the circuit could be shrunk by nearly a factor of 10 in area and improved in terms of its speed relative to power-consumption performance. Right now, we are testing a prototype of this circuit in our lab.\nAnd memristors are by no means hard to fabricate. The titanium dioxide structure can be made in any semiconductor fab currently in existence. (In fact, our hybrid circuit was built in an HP fab used for making inkjet cartridges.) The primary limitation to manufacturing hybrid chips with memristors is that today only a small number of people on Earth have any idea of how to design circuits containing memristors. I must emphasize here that memristors will never eliminate the need for transistors: passive devices and circuits require active devices like transistors to supply energy.\nThe potential of the memristor goes far beyond juicing a few FPGAs. I have referred several times to the similarity of memristor behavior to that of synapses. Right now, Greg is designing new circuits that mimic aspects of the brain. The neurons are implemented with transistors, the axons are the nanowires in the crossbar, and the synapses are the memristors at the cross points. A circuit like this could perform real-time data analysis for multiple sensors. Think about it: an intelligent physical infrastructure that could provide structural assessment monitoring for bridges. How much money\u2014and how many lives\u2014could be saved?\nI\u2019m convinced that eventually the memristor will change circuit design in the 21st century as radically as the transistor changed it in the 20th. Don\u2019t forget that the transistor was lounging around as a mainly academic curiosity for a decade until 1956, when a killer app\u2014the hearing aid\u2014brought it into the marketplace. My guess is that the real killer app for memristors will be invented by a curious student who is now just deciding what EE courses to take next year.\nAbout the Author\nR. STANLEY WILLIAMS, a senior fellow at Hewlett-Packard Labs, wrote this month\u2019s cover story, \u201dHow We Found the Missing Memristor.\u201d Earlier this year, he and his colleagues shook up the electrical engineering community by introducing a fourth fundamental circuit design element. The existence of this element, the memristor, was first predicted in 1971 by IEEE Fellow Leon Chua, of the University of California, Berkeley, but it took Williams 12 years to build an actual device.", "id": "<urn:uuid:fc30d469-0a3c-4993-a11a-95b648c6e637>", "dump": "CC-MAIN-2013-20", "url": "http://spectrum.ieee.org/semiconductors/processors/how-we-found-the-missing-memristor/5", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9618540406227112, "token_count": 6717, "score": 3.171875, "int_score": 3}
{"text": "Belgian physicist Francois Englert, left, speaks with British physicist\u2026 (Fabrice Coffrini / AFP/Getty\u2026)\nFor physicists, it was a moment like landing on the moon or the discovery of DNA.\nThe focus was the Higgs boson, a subatomic particle that exists for a mere fraction of a second. Long theorized but never glimpsed, the so-called God particle is thought to be key to understanding the existence of all mass in the universe. The revelation Wednesday that it -- or some version of it -- had almost certainly been detected amid more than hundreds of trillions of high-speed collisions in a 17-mile track near Geneva prompted a group of normally reserved scientists to erupt with joy.\nFor The Record\nLos Angeles Times Friday, July 06, 2012 Home Edition Main News Part A Page 4 News Desk 1 inches; 48 words Type of Material: Correction\nLarge Hadron Collider: In some copies of the July 5 edition, an article in Section A about the machine used by physicists at the European Organization for Nuclear Research to search for the Higgs boson referred to the $5-billion Large Hadron Collider. The correct amount is $10 billion.\nPeter Higgs, one of the scientists who first hypothesized the existence of the particle, reportedly shed tears as the data were presented in a jampacked and applause-heavy seminar at CERN, the European Organization for Nuclear Research.\n\"It's a gigantic triumph for physics,\" said Frank Wilczek, an MIT physicist and Nobel laureate. \"It's a tremendous demonstration of a community dedicated to understanding nature.\"\nThe achievement, nearly 50 years in the making, confirms physicists' understanding of how mass -- the stuff that makes stars, planets and even people -- arose in the universe, they said.\nIt also points the way toward a new path of scientific inquiry into the mass-generating mechanism that was never before possible, said UCLA physicist Robert Cousins, a member of one of the two research teams that has been chasing the Higgs boson at CERN.\n\"I compare it to turning the corner and walking around a building -- there's a whole new set of things you can look at,\" he said. \"It is a beginning, not an end.\"\nLeaders of the two teams reported independent results that suggested the existence of a previously unseen subatomic particle with a mass of about 125 to 126 billion electron volts. Both groups got results at a \"five sigma\" level of confidence -- the statistical requirement for declaring a scientific \"discovery.\"\n\"The chance that either of the two experiments had seen a fluke is less than three parts in 10 million,\" said UC San Diego physicist Vivek Sharma, a former leader of one of the Higgs research groups. \"There is no doubt that we have found something.\"\nBut he and others stopped just shy of saying that this new particle was indeed the long-sought Higgs boson. \"All we can tell right now is that it quacks like a duck and it walks like a duck,\" Sharma said.\nIn this case, quacking was enough for most.\n\"If it looks like a duck and quacks like a duck, it's probably at least a bird,\" said Wilczek, who stayed up past 3 a.m. to watch the seminar live over the Web while vacationing in New Hampshire.\nCertainly CERN leaders in Geneva, even as they referred to their discovery simply as \"a new particle,\" didn't bother hiding their excitement.\nThe original plan had been to present the latest results on the Higgs search at the International Conference on High Energy Physics, a big scientific meeting that began Wednesday in Melbourne.\nBut as it dawned on CERN scientists that they were on the verge of \"a big announcement,\" Cousins said, officials decided to honor tradition and instead present the results on CERN's turf.\nThe small number of scientists who theorized the existence of the Higgs boson in the 1960s -- including Higgs of the University of Edinburgh -- were invited to fly to Geneva.\nFor the non-VIP set, lines to get into the auditorium began forming late Tuesday. Many spent the night in sleeping bags.\nAll the hubbub was due to the fact that the discovery of the Higgs boson is the last piece of the puzzle needed to complete the so-called Standard Model of particle physics -- the big picture that describes the subatomic particles that make up everything in the universe, and the forces that work between them.\nOver the course of the 20th century, as physicists learned more about the Standard Model, they struggled to answer one very basic question: Why does matter exist?\nHiggs and others came up with a possible explanation: that particles gain mass by traveling through an energy field. One way to think about it is that the field sticks to the particles, slowing them down and imparting mass.\nThat energy field came to be known as the Higgs field. The particle associated with the field was dubbed the Higgs boson.\nHiggs published his theory in 1964. In the 48 years since, physicists have eagerly chased the Higgs boson. Finding it would provide the experimental confirmation they needed to show that their current understanding of the Standard Model was correct.\nOn the other hand, ruling it out would mean a return to the drawing board to look for an alternative Higgs particle, or several alternative Higgs particles, or perhaps to rethink the Standard Model from the bottom up.\nEither outcome would be monumental, scientists said.", "id": "<urn:uuid:fb237ffb-9cc0-4077-99d5-56c6fce1ca5f>", "dump": "CC-MAIN-2013-20", "url": "http://articles.latimes.com/2012/jul/05/science/la-sci-higgs-boson-new-particle-20120705", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9634510278701782, "token_count": 1134, "score": 2.59375, "int_score": 3}
{"text": "The scientific world is abuzz with news of the ratification of the existence of the subatomic particle called the Higgs boson - or more colloquially, the 'God particle.' This subatomic particle's existence - which was verified recently (with virtually near certainty) by experiments at the Large Hadron Collider in Switzerland - lends credence to several long-standing physical theories such as the so-called Standard Model and the Big Bang Theory.\nThe nickname God particle is ironic for two reasons. First, generally, the nuclear physicists who deal with these matters - postulating the fundamental physical laws of the universe and then setting about to either verify or refute them - tend not to be regular church-goers. While there are some highly prominent scientists who balance personal, religious beliefs with professional, scientific quests, most probably go along with the thoughts of the world-famous physicist, Stephen Hawking:\nI regard the brain as a computer which will stop working when its components fail. There is no heaven or afterlife for broken down computers; that is a fairy story for people afraid of the dark. [Interview in The Guardian, 7/9/12]\nSpontaneous creation is the reason there is something rather than nothing, why the universe exists, why we exist. It is not necessary to invoke God... [from his book; The Grand Design, 2010]\nSo it is a bit ironic that physics' most famous quest has resulted in the discovery of the 'God particle.' Most physicists are quite comfortable having their names associated with famous - even if dead - humans like Newton, Einstein or the afore-mentioned Hawking. One will find few, if any, attributions to deities in the objects that physicists discover and name or the theories they propose.\nSecond, and more importantly, the discovery that the God particle really exists does not - as the name suggests - imply that God played some role in the creation of the universe. In fact, quite the opposite. The matter is discussed at some length in the July 9 Daily Beast by Lawrence Kraus, a well-known physicist/cosmologist from Arizona State University:\nThis term [God particle] appeared first in the unfortunate title of a book written by physicist Leon Lederman two decades ago, and while to my knowledge it was never used by any scientist (including Lederman) before or since, it has captured the media's imagination.\nWhat makes this term particularly unfortunate is that nothing could be further from the truth. Assuming the particle in question is indeed the Higgs, it validates an unprecedented revolution in our understanding of fundamental physics and brings science closer to dispensing with the need for any supernatural shenanigans all the way back to the beginning of the universe...If these bold, some would say arrogant, notions derive support from the remarkable results at the Large Hadron Collider, they may reinforce two potentially uncomfortable possibilities: first, that many features of our universe, including our existence, may be accidental consequences of conditions associated with the universe's birth; and second, that creating \"stuff\" from \"no stuff\" seems to be no problem at all-everything we see could have emerged as a purposeless quantum burp in space or perhaps a quantum burp of space itself. Humans, with their remarkable tools and their remarkable brains, may have just taken a giant step toward replacing metaphysical speculation with empirically verifiable knowledge. The Higgs particle is now arguably more relevant than God.\nSo the term God particle was first used by a scientist, but was picked up and popularized by the media. It's catchy and enhances interest in the subject among the public. But like so much else that the media promotes, it is misleading and inappropriate.", "id": "<urn:uuid:ed184b23-5659-4b91-97c0-fd818297d417>", "dump": "CC-MAIN-2013-20", "url": "http://www.americanthinker.com/blog/2012/07/does_the_god_particle_prove_that_god_does_or_does_not_exist.html", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9590326547622681, "token_count": 743, "score": 2.546875, "int_score": 3}
{"text": "If superparticles were to exist the decay would happen far more often. This test is one of the \"golden\" tests for supersymmetry and it is one that on the face of it this hugely popular theory among physicists has failed.\nProf Val Gibson, leader of the Cambridge LHCb team, said that the new result was \"putting our supersymmetry theory colleagues in a spin\".\nThe results are in fact completely in line with what one would expect from the Standard Model. There is already concern that the LHCb's sister detectors might have expected to have detected superparticles by now, yet none have been found so far.This certainly does not rule out SUSY, but it is getting to the same level as cold fusion if positive experimental result does not come soon.", "id": "<urn:uuid:72def0d3-296d-49d8-bdf5-73c351dd6672>", "dump": "CC-MAIN-2013-20", "url": "http://physicsandphysicists.blogspot.com/2012/11/more-results-not-in-favor-of-susy.html", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9730085134506226, "token_count": 163, "score": 2.6875, "int_score": 3}
{"text": "In 2006, two scientists announced that they had cooked an egg by placing it in between two cell phones. It has been thrououghly disproven and analyzed since the surface of the claim, but it is still an excellent opportunity to use the Scientific Thinking Principles on!\n#1: Ruling Out Rival Hypotheses\nThis principle isn't the most relevant because the experiment doesn't exactly prove any hypotheses. But it can still apply to the attempt to cook an egg between two cell phones because there could be other effects causing that outcome.\n#2: Correlation vs. Causation\nThere are so many other reasons that the egg could've cooked! Maybe it was really hot out? Or the cell phone egg set up was within a microwave? Not the most probable of all possible causations, but it proves the point. There could be many other ways this egg could've cooked (or in actuality, the fact that it didn't cook at all) that we need to examine or at least acknowledge that they could be there.\nThis claim is very out there so it has a really good chance of being able to be falsified. As we will see in #4, after replicating the incident one can find almost instantly that it is in fact a hoax.\nAs many people did, reproducing the egg cooking experiement will prove that it is in fact a hoax. Every reproduction that was prodcued failed to yield the same results as the first, which made everything make sense when the site's webmaster that published the article stepped forward to say it was in fact completely fake.\n#5: Extraordinary Claims\nThe claim that you can cook an egg with two cell phones is pretty extraordinary yet there is no extraordinary evidence to back it up! In fact, it is just too extraordinary to be real.\n#6: Occam's Razor\nIn my mind, the simpliest explaination would be that it simply is not true. The end.\nI'm not going to lie, if this claim was true I would be thrilled. In addition to being in awe of the power of technology, it would make cooking meals for myself in my dorm room a whole lot easier! Unfortunately though, this is a hoax. With the help of the Six Principles of Scientific Thinking, I will never fall for this or any other raw food cooking claim ever again!", "id": "<urn:uuid:87230adf-bb1f-4b4b-9c4e-ebaed1589758>", "dump": "CC-MAIN-2013-20", "url": "http://blog.lib.umn.edu/hamdi002/blog/2011/09/cooking-an-egg-between-two-cell-phones-not-the-best-way-to-get-out-of-dorm-food.html", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9744361042976379, "token_count": 484, "score": 2.875, "int_score": 3}
{"text": "Dinosaur Bones \"Buried\" By Evolutionists!\nA recent presentation at the 2012 Western Pacific Geophysics meeting in Singapore showed C-14 dates of soft tissue found in dinosaur bones to be in the range from 22,000 - 39,000 years old. Previously, it has been assumed that dinosaurs died out over 65 million years ago, so these new findings are astonishing and should have made international news. But shortly after the presentation was made at the meeting, the abstract was removed from the meeting\u2019s website!\nSo it appears that instead of making international news, these findings have been buried. First, let\u2019s look at what was actually being presented.\nThe organic matter (collagen) and hard carbonate bone mineral (bioapatite) in the bone samples were analyzed. The samples came from several species of dinosaurs (acrocanthosaur, hadrosaurus, triceratops and apatosaurus) taken from various sites in Texas, Colorado, Arkansas and Montana. The samples were meticulously handled and cleaned to avoid possible contamination. The carbon-14 (C-14) levels in these samples were measured using Accelerator Mass Spectrometry (AMS). The resultant C-14 ages obtained from these samples were consistently in the 22,000 - 39,000 years range. The fact that the samples were from a variety of species and sites all giving consistent results greatly reduces the chance that the results are from contamination.\nThe theoretical upper limit for C-14 dating is ten times the half-life, or about 57,000 years. The proposed practical upper limit for C-14 dating is between 40,000 - 50,000 years. While some samples fell close to the 40,000 year upper limit, 16 out of 20 (80%) were aged at 35,000 years or younger, well within the acceptable upper C-14 dating limits.\nWhile other researchers have found soft tissue in dinosaur bones and C-14 dates in these ranges, this current study has been the most comprehensive. The fact that there is any collagen at all remaining in these bone samples is amazing, considering that they are supposed to be older than 65 million years. Protein just doesn\u2019t hang around that long! And that there is any C-14 in them also is reason to possibly question conventional wisdom. But why have we not heard about any of this in the news? Shouldn't there have been at least a 15-second blip from one of the media outlets? But the media have been silent and the abstract pulled from the meeting proceedings. The sacred cow of evolution once again remains intact.\nWe truly are living in an age of great deception. \u201cAnd for this cause God shall send them strong delusion, that they should believe a lie.\u201d (2 Thessalonians 2:11)\nThis article, by Creation Moments board member Dr. Don Clark, is based on an interview broadcast by Broken Road Radio. To hear the interview and many others on biblical creation topics, go to http://brokenroadradio.com/morning-show-september-17-2012/", "id": "<urn:uuid:6cb92412-144d-4c9c-b1c1-b3638ba84556>", "dump": "CC-MAIN-2013-20", "url": "http://www.creationmoments.net/resources/blog/201210/dinosaur-bones-buried-evolutionists", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9631441831588745, "token_count": 636, "score": 3.09375, "int_score": 3}
{"text": "Schopf, J. William\nAdult Nonfiction QH325 .S384 1999\nSummary: One of the greatest mysteries in reconstructing the history of life on Earth has been the apparent absence of fossils dating back more than 550 million years. We have long known that fossils of sophisticated marine life-forms existed at the dawn of the Cambrian Period, but until recently scientists had found no traces of Precambrian fossils. The quest to find such traces began in earnest in the mid-1960s and culminated in one dramatic moment in 1993 when William Schopf identified fossilized microorganisms three and a half \"billion years old. This startling find opened up a vast period of time--some eighty-five percent of Earth's history--to new research and new ideas about life's beginnings. In this book, William Schopf, a pioneer of modern paleobiology, tells for the first time the exciting and fascinating story of the origins and earliest evolution of life and how that story has been unearthed. Gracefully blending his personal story of discovery with the basics needed to understand the astonishing science he describes, Schopf has produced an introduction to paleobiology for the interested reader as well as a primer for beginning s\nQuestion about returns, requests or other account details?\nAdd a Comment", "id": "<urn:uuid:0956edc3-3852-4559-bcaf-343f4b07a097>", "dump": "CC-MAIN-2013-20", "url": "http://www.hclib.org/pub/bookspace/discuss/?bib=642972&theTab=Summary", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9253062009811401, "token_count": 261, "score": 3.65625, "int_score": 4}
{"text": "Jim Lake and Maria Rivera, at the University of California-Los Angeles (UCLA), report their finding in the Sept. 9 issue of the journal Nature.\nScientists refer to both bacteria and Archaea as \"prokaryotes\"--a cell type that has no distinct nucleus to contain the genetic material, DNA, and few other specialized components. More-complex cells, known as \"eukaryotes,\" contain a well-defined nucleus as well as compartmentalized \"organelles\" that carry out metabolism and transport molecules throughout the cell. Yeast cells are some of the most-primitive eukaryotes, whereas the highly specialized cells of human beings and other mammals are among the most complex.\n\"A major unsolved question in biology has been where eukaryotes came from, where we came from,\" Lake said. \"The answer is that we have two parents, and we now know who those parents were.\"\nFurther, he added, the results provide a new picture of evolutionary pathways. \"At least 2 billion years ago, ancestors of these two diverse prokaryotic groups fused their genomes to form the first eukaryote, and in the processes two different branches of the tree of life were fused to form the ring of life,\" Lake said.\nThe work is part of an effort supported by the National Science Foundation--the federal agency that supports research and education across all disciplines of science and engineering--to re-examine historical schemes for classifying Earth's living creatures, a process that was once based on easily observable traits. Microbes, plants or animals wer\nContact: Leslie Fink\nNational Science Foundation", "id": "<urn:uuid:baf824b2-7e06-471a-8510-efd5abab1567>", "dump": "CC-MAIN-2013-20", "url": "http://news.bio-medicine.org/biology-news-2/Complex-cells-likely-arose-from-combination-of-bacterial-and-extreme-microbe-genomes-284-1/", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9549511671066284, "token_count": 335, "score": 3.796875, "int_score": 4}
{"text": "The bacterium Micavibrio aeruginosavorus (yellow), leeching\non a Pseudomonas aeruginosa bacterium (purple).\nWhat\u2019s the news: If bacteria had blood, the predatory microbe Micavibrio aeruginosavorus would essentially be a vampire: it subsists by hunting down other bugs, attaching to them, and sucking their life out. For the first time, researchers have sequenced the genome of this strange microorganism, which was first identified decades ago in sewage water. The sequence will help better understand the unique bacterium, which has potential to be used as a \u201cliving antibiotic\u201d due to its ability to attack drug-resistant biofilms and its apparent fondness for dining on pathogens.\nAnatomy of a Vampire:\n- The bacterium has an interesting multi-stage life history. During its migratory phase it sprouts a single flagellum and goes hunting for prey. Once it find a delectable morsel of bacterium, it attacks and irreversibly attaches to the surface, and sucks out all of the good stuff: carbohydrates, amino acids, proteins, DNA, etc.\n- Sated, the cell divides in two via binary fission, and the now-depleted host is left for dead.\nHungry for Pathogens:\n- M. aeruginosavorus cannot be grown by itself; it must be cultured along with another bacteria to feed upon. A 2006 study found that it only grew upon three bacterial species, all of which can cause pneumonia-like disease in humans. A more recent study showed that it can prey upon a wider variety of microbes, most of them potentially pathogenic, like E. coli.\n- These studies also found that M. aeruginosavorus has a knack for disrupting biofilms, the dense collection of bacteria that cause harmful plaques on teeth and medical implants alike, and can be up to 1,000 more resistant to antibiotics than free-swimming bugs.\n- The bacteria can also swim through viscous fluids like mucous and kills Pseudomonas aeruginosa, the bacterium that can colonize lungs of cystic fibrosis patients and form a glue-like film.\n- These qualities have caught the eye of researchers who think it could be used as a living antibiotic to treat biofilms and various types of drug-resistant bacteria, which are a growing problem in medicine. Sequencing the organism\u2019s genome is an important step in understanding its biochemistry and how it preys on other microbes.\nClues From the Vampire Code:\n- The new study found that each phase of life involves the use (or expression) of different sets of genes. The migratory/hunting phase involves many segments that code for flagellum formation and genes involved in quorum sensing. The attachment phase involves a wide variety of secreted chemicals and enzymes that facilitate the flow of materials from the host.\n- Micavibrio aeruginosavorus possesses no genes for amino acid transporters, a rather rare trait only seen in a few other bacterial species that depend heavily upon their host to help them shuttle these vital protein building-blocks. This absence helps explain the bacterium\u2019s dependence on a narrow range of prey, from which it directly steals amino acids. Although it remains unclear exactly how the microbe attaches to and infiltrates other cells.\nThe Future Holds:\n- The range of microbes upon which Micavibrio aeruginosavorus can survive is expanding; after being kept in laboratory conditions for years it has apparently evolved a more diverse diet. If this expansion continues, that could be a real problem for its use as an antibiotic; it could begin to eat beneficial gut bacteria, for example.\n- Researchers claim it is harmless to friendly gut microbes, but it hasn\u2019t been tested on all the varieties of bacteria present in humans.\n- Several important steps must be taken before testing in people, like learning more about what traits makes another bacteria tasty to Micavibrio aeruginosavorus. Researchers speculate the bacterium may need to be genetically altered in order to go after specific pathogens, or to reduce the risk of it causing unforeseen complications.\nReference: Zhang Wang, Daniel E Kadouri, Martin Wu. Genomic insights into an obligate epibiotic bacterial predator: Micavibrio aeruginosavorus ARL-13. BMC Genomics, 2011; 12 (1): 453 DOI: 10.1186/1471-2164-12-453\nImage credit: University of Virginia", "id": "<urn:uuid:d904d662-9bf2-45c5-84ed-06cf69edb907>", "dump": "CC-MAIN-2013-20", "url": "http://blogs.discovermagazine.com/80beats/?p=33049", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9158055782318115, "token_count": 954, "score": 3.921875, "int_score": 4}
{"text": "Outmaneuvering Foodborne Pathogens\nAt various locations, ARS scientists are doing research to make leafy greens and other fresh produce safer for consumers. Produce and leafy greens in the photo are (clockwise from top): romaine lettuce, cabbage, cilantro in a bed of broccoli sprouts, spinach and other leafy greens, green onions, tomatoes, and green leaf lettuce.\nIf pathogens like E. coli O157:H7 or Salmonella had a motto for survival, it might be: \u201cFind! Bind! Multiply!\u201d\nThat pretty much sums up what these food-poisoning bacteria do in nature, moving through our environment to find a host they can bind to and use as a staging area for multiplying and spreading.\nBut ARS food-safety scientists in California are determined to find out how to stop these and other foodborne pathogenic bacteria in their tracks, before the microbes can make their way to leafy greens and other favorite salad ingredients like tomatoes and sprouts.\nThe research is needed to help prevent the pathogens from turning up in fresh produce that we typically eat uncooked. That\u2019s according to Robert E. Mandrell, who leads the ARS Produce Safety and Microbiology Research Unit. His team is based at the agency\u2019s Western Regional Research Center in Albany, California.\nThe team is pulling apart the lives of these microbes to uncover the secrets of their success. It\u2019s a complex challenge, in part because the microbes seem to effortlessly switch from one persona to the next. They are perhaps best known as residents of the intestines of warm-blooded animals, including humans. For another role, the pathogens have somehow learned to find, bind, and multiply in the world of green plants.\nSometimes the pathogenic microbes need the help of other microbial species to make the jump from animal inhabitant to plant resident. Surprisingly little is known about these powerful partnerships, Mandrell says. That\u2019s why such alliances among microbes are one of several specific aspects of the pathogens\u2019 lifestyles that the Albany scientists are investigating. In all, knowledge gleaned from these and other laboratory, greenhouse, and outdoor studies should lead to new, effective, environmentally friendly ways to thwart the pathogens before they have a chance to make us ill.\nIn a greenhouse, microbiologist Maria Brandl examines cilantro that she uses as a model plant to investigate the behavior of foodborne pathogens on leaf surfaces.\nA Pathogen Targets Youngest Leaves\nKnowing pathogens\u2019 preferences is essential to any well-planned counter-attack. So microbiologist Maria T. Brandl is scrutinizing the little-understood ability of E. coli O157:H7 and Salmonella enterica to contaminate the elongated, slightly sweet leaves of romaine lettuce. With a University of California-Berkeley colleague, Brandl has shown that, if given a choice, E. coli has a strong preference for the young, inner leaves. The researchers exposed romaine lettuce leaves to E. coli and found that the microbe multiplied about 10 times more on the young leaves than on the older, middle ones. One explanation: The young leaves are a better nutrition \u201cbuy\u201d for E. coli. \u201cThese leaves exude about three times more nitrogen and about one-and-one-half times more carbon than do the middle leaves,\u201d says Brandl.\nScientists have known for decades that plants exude compounds from their leaves and roots that bacteria and fungi can use as food. But the romaine lettuce study, published earlier this year in Applied and Environmental Microbiology, is the first to document the different exudate levels among leaves of the two age classes. It\u2019s also the first to show that E. coli can do more than just bind to lettuce leaves: It can multiply and spread on them.\nResearch assistant Danielle Goudeau inoculates a lettuce leaf with E. coli O157:H7 in a biological safety cabinet to study the biology of the human pathogen on leafy greens.\nAdding nitrogen to the middle leaves boosted E. coli growth, Brandl found. \u201cIn view of the key role of nitrogen in helping E. coli multiply on young leaves,\u201d she says, \u201ca strategy that minimizes use of nitrogen fertilizer in romaine lettuce fields may be worth investigating.\u201d\nIn other studies using romaine lettuce and the popular herb cilantro as models, Brandl documented the extent to which E. coli and Salmonella are aided by Erwinia chrysanthemi, an organism that causes fresh produce to rot.\n\u201cWhen compared to plant pathogens, E. coli and Salmonella are not as \u2018fit\u2019 on plants,\u201d Brandl says. But the presence of the rot-producing microbe helped E. coli and Salmonella grow on lettuce and cilantro leaves.\n\u201cSoft rot promoted formation of large aggregates, called \u2018biofilms,\u2019 of E. coli and Salmonella and increased their numbers by up to 100-fold,\u201d she notes.\nThe study uncovered new details about genes that the food-poisoning pathogens kick into action when teamed up with plant pathogens such as soft rot microbes.\nBrandl, in collaboration with Albany microbiologist Craig Parker, used a technique known as \u201cmicroarray analysis\u201d to spy on the genes. \u201cThe assays showed that Salmonella cells\u2014living in soft rot lesions on lettuce and cilantro\u2014had turned on some of the exact same genes that Salmonella uses when it infects humans or colonizes the intestines of animals,\u201d she says. Some of these activated genes were ones that Salmonella uses to get energy from several natural compounds common to both green plants and to the animal intestines that Salmonella calls home.\nUsing a confocal laser scanning microscope, microbiologist Maria Brandl examines a mixed biofilm of Salmonella enterica (pink) and Erwinia chrysanthemi (green) in soft rot lesions on cilantro leaves (blue).\nA One-Two Punch to Tomatoes\nSalmonella also benefits from the presence of another plant pathogen, specifically, Xanthomonas campestris, the culprit in a disease known as \u201cbacterial leaf spot of tomato.\u201d But the relationship between Salmonella and X. campestris may be different than the relation of Salmonella to the soft rot pathogen. Notably, Salmonella benefits even if the bacterial spot pathogen is at very low levels\u2014so low that the plant doesn\u2019t have the disease or any visible symptoms of it.\nThat\u2019s among the first-of-a-kind findings that microbiologist Jeri D. Barak found in her tests with tomato seeds exposed to the bacterial spot microbe and then planted in soil that had been irrigated with water contaminated with S. enterica.\nIn a recent article in PLoS ONE, Barak reported that S. enterica populations were significantly higher in tomato plants that had also been colonized by X. campestris. In some cases, Salmonella couldn\u2019t bind to and grow on\u2014or in\u2014tomato plants without the presence of X. campestris, she found.\nListeria monocytogenes on this broccoli sprout shows up as green fluorescence. The bacteria are mainly associated with the root hairs.\n\u201cWe think that X. campestris may disable the plant immune response\u2014a feat that allows both it and Salmonella to multiply,\u201d she says.\nThe study was the first to report that even as long as 6 weeks after soil was flooded with Salmonella-contaminated water, the microbe was capable of binding to tomato seeds planted in the tainted soil and, later, of spreading to the plant.\n\u201cThese results suggest that any contamination that introduces Salmonella from any source into the environment\u2014whether that source is irrigation water, improperly composted manure, or even insects\u2014could lead to subsequent crop contamination,\u201d Barak says. \u201cThat\u2019s true even if substantial time has passed since the soil was first contaminated.\u201d\nCrop debris can also serve as a reservoir of viable Salmonella for at least a week, Barak\u2019s study showed. For her investigation, the debris was composed of mulched, Salmonella-contaminated tomato plants mixed with uncontaminated soil.\n\u201cReplanting fields shortly after harvesting the previous crop is a common practice in farming of lettuce and tomatoes,\u201d she says. The schedule allows only a very short time for crop debris to decompose. \u201cOur results suggest that fields known to have been contaminated with S. enterica could benefit from an extended fallow period, perhaps of at least a few weeks.\u201d\nOrdinary Microbe Foils E. coli\nWhile the bacterial spot and soft rot microbes make life easier for certain foodborne pathogens, other microbes may make the pathogens\u2019 existence more difficult. Geneticist Michael B. Cooley and microbiologist William G. Miller at Albany have shown the remarkable effects of one such microbe, Enterobacter asburiae. This common, farm-and-garden-friendly microorganism lives peaceably on beans, cotton, and cucumbers.\nIn one experiment, E. asburiae significantly reduced levels of E. coli and Salmonella when all three species of microbes were inoculated on seeds of thale cress, a small plant often chosen for laboratory tests.\nThe study, published in Applied and Environmental Microbiology in 2003, led to followup experiments with green leaf lettuce. In that battle of the microbes, another rather ordinary bacterium, Wausteria paucula, turned out to be E. coli\u2019s new best friend, enhancing the pathogen\u2019s survival sixfold on lettuce leaves.\n\u201cIt was the first clear example of a microbe\u2019s supporting a human pathogen on a plant,\u201d notes Cooley, who documented the findings in the Journal of Food Protection in 2006.\nBut E. asburiae more than evened the score, decreasing E. coli survival 20- to 30-fold on lettuce leaves exposed to those two species of microbes.\nThe mechanisms underlying the competition between E. asburiae and E. coli are still a mystery, says Cooley, \u201cespecially the competition that takes place on leaves or other plant surfaces.\u201d\nNevertheless, E. asburiae shows initial promise of becoming a notable biological control agent to protect fresh salad greens or other crops from pathogen invaders. With further work, the approach could become one of several science-based solutions that will help keep our salads safe.\u2014By Marcia Wood, Agricultural Research Service Information Staff.\nThis research is part of Food Safety, an ARS national program (#108) described on the World Wide Web at www.nps.ars.usda.gov.\nTo reach scientists mentioned in this article, contact Marcia Wood, USDA-ARS Information Staff, 5601 Sunnyside Ave., Beltsville, MD 20705-5129; phone (301) 504-1662, fax (301) 504-1486.\nListeria monocytogenes on this radish sprout shows up as green fluorescence. The bacteria are mainly associated with the root hairs.\nWhat Genes Help Microbes Invade Leafy Greens?\nWhen unwanted microbes form an attachment, the consequences\u2014for us\u2014can be serious.\nThat\u2019s if the microbes happen to be human pathogens like Listeria monocytogenes or Salmonella enterica and if the target of their attentions happens to be fresh vegetables often served raw, such as cabbage or the sprouted seeds of alfalfa.\nScientists don\u2019t yet fully understand how the malevolent microbes form colonies that cling stubbornly to and spread across plant surfaces, such as the bumpy leaves of a cabbage or the ultra-fine root hairs of a tender alfalfa sprout.\nBut food safety researchers at the ARS Western Regional Research Center in Albany, California, are putting together pieces of the pathogen puzzle.\nA 1981 food-poisoning incident in Canada, caused by L. monocytogenes in coleslaw, led microbiologist Lisa A. Gorski to study the microbe\u2019s interactions with cabbage. Gorski, with the center\u2019s Produce Safety and Microbiology Research Unit, used advanced techniques not widely available at the time of the cabbage contamination.\n\u201cVery little is known about interactions between Listeria and plants,\u201d says Gorski, whose study revealed the genes that Listeria uses during a successful cabbage-patch invasion.\nThe result was the first-ever documentation of Listeria genes in action on cabbage leaves. Gorski, along with coinvestigator Jeffrey D. Palumbo\u2014now with the center\u2019s Plant Mycotoxin Research Unit\u2014and others, documented the investigation in a 2005 article in Applied and Environmental Microbiology.\nListeria, Behaving Badly\n\u201cPeople had looked at genes that Listeria turns on, or \u2018expresses,\u2019 when it\u2019s grown on agar gel in a laboratory,\u201d says Gorski. \u201cBut no one had looked at genes that Listeria expresses when it grows on a vegetable.\n\u201cWe were surprised to find that when invading cabbage, Listeria calls into play some of the same genes routinely used by microbes that are conventionally associated with plants. Listeria is usually thought of as a pathogen of humans. We hadn\u2019t really expected to see it behaving like a traditional, benign inhabitant of a green plant.\n\u201cIt\u2019s still a relatively new face for Listeria, and requires a whole new way of thinking about it.\u201d\nIn related work, Gorski is homing in on genetic differences that may explain the widely varying ability of eight different Listeria strains to successfully colonize root hairs of alfalfa sprouts\u2014and to resist being washed off by water.\nIn a 2004 article in the Journal of Food Protection, Gorski, Palumbo, and former Albany associate Kimanh D. Nguyen reported those differences. Poorly attaching strains formed fewer than 10 Listeria cells per sprout during the lab experiment, while the more adept colonizers formed more than 100,000 cells per sprout.\nSalmonella\u2019s Cling Genes\nColleague Jeri D. Barak, a microbiologist at Albany, led another sprout investigation, this time probing the ability of S. enterica to attach to alfalfa sprouts. From a pool of 6,000 genetically different Salmonella samples, Barak, Gorski, and coinvestigators found 20 that were unable to attach strongly to sprouts.\nScientists elsewhere had already identified some genes as necessary for Salmonella to successfully invade and attach to the guts of animals such as cows and chickens. In the Albany experiments, some of those same genes were disrupted in the Salmonella specimens that couldn\u2019t cling to alfalfa sprouts.\nTheir 2005 article in Applied and Environmental Microbiology helped set the stage for followup studies to tease out other genes that Salmonella uses when it is living on and in plants.\nA deeper understanding of those and other genes may lead to sophisticated defense strategies to protect tomorrow\u2019s salad greens\u2014and us.\u2014By Marcia Wood, Agricultural Research Service Information Staff.\nGeneticist Michael Cooley collects a sediment sample to test for E. coli O157:H7. The pathogen was found near fields implicated in the 2006 outbreak of E. coli O157:H7 on baby spinach.\nEnvironmental Surveillance Exposes a Killer\nIt started as a manhunt for a microbe, but it became one of the nation\u2019s most intensive farmscape searches for the rogue pathogen E. coli O157:H7.\nARS microbiologist Robert E. Mandrell and geneticist Michael B. Cooley of the Produce Safety and Microbiology Research Unit in Albany, California, had already been collaborating in their own small-scale study of potential sources of E. coli O157:H7 in the state\u2019s produce-rich Salinas Valley when, in 2005, they were asked to join another one. The new investigation became a 19-month surveillance\u2014by the two scientists and other federal and state experts\u2014of E. coli in Salinas Valley watersheds.\n\u201cIt may seem like an obvious concept today,\u201d says Mandrell, \u201cbut at the time, there was little proof that E. coli contamination of produce before harvest could be a major cause of food-poisoning outbreaks.\u201d\nMandrell and Cooley aided the California Food Emergency Response Team, as this food-detective squad was named, in tracing movement of E. coli through the fertile valley. This surveillance showed that E. coli O157:H7 can travel long distances in streamwater and floodwater.\nIn 2006, E. coli O157:H7 strains indistinguishable from those causing human illness associated with baby spinach were discovered in environmental samples\u2014including water\u2014taken from a Salinas Valley ranch.\nWild pigs were added to the list of animal carriers of the pathogen when one of the so-called \u201coutbreak strains\u201d of E. coli O157:H7 was discovered in their dung. The team documented its work in 2007 in PLoS ONE and Emerging Infectious Diseases.\nThe Albany scientists used a relatively new technique to detect E. coli O157:H7 in water. Developed at the ARS Meat Animal Research Center in Clay Center, Nebraska, for animal hides, the method was adapted by the Albany team for the outdoor reconnaissance.\nBecause of their colleagues\u2019 work, says Cooley, \u201cWe had the right method at the right time.\u201d\u2014By Marcia Wood, Agricultural Research Service Information Staff.\n\"Outmaneuvering Foodborne Pathogens\" was published in the July 2008 issue of Agricultural Research magazine.", "id": "<urn:uuid:b366eb1a-184b-4885-b5a6-92a6d0c0f67f>", "dump": "CC-MAIN-2013-20", "url": "http://www.ars.usda.gov/is/AR/archive/jul08/pathogen0708.htm", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9299995303153992, "token_count": 3794, "score": 3.078125, "int_score": 3}
