{"text": "On May 15, the House Agriculture Committee passed its 2013 farm bill. The bill would cut the Supplemental Nutrition Assistance Program (SNAP, formerly known as the Food Stamp Program) by almost $21 billion over the next decade, eliminating food assistance to nearly 2 million low-income people, mostly working families with children and senior citizens.\nThe bill\u2019s SNAP cuts would come on top of an across-the-board reduction in benefits that every SNAP recipient will experience starting November 1, 2013.\nThe Supplemental Nutrition Assistance Program\u2019s (SNAP) primary purpose is to increase the food purchasing power of eligible low-income households in order to improve their nutrition and alleviate hunger and malnutrition. The program\u2019s success in meeting this core goal has been well documented. Less well understood is the fact that the program has become quite effective in supporting work and that its performance in this area has improved substantially in recent years.\nThe Supplemental Nutrition Assistance Program (SNAP, formerly known as the Food Stamp Program) is the nation\u2019s most important anti-hunger program. In 2012, it helped almost 47 million low-income Americans to afford a nutritionally adequate diet in a typical month.\nNearly 72 percent of SNAP participants are in families with children; more than one-quarter of participants are in households with seniors or people with disabilities.\nSNAP is the nation\u2019s most important anti-hunger program.\nThis chartbook highlights some of the key characteristics of the almost 47 million people using the program as well as trends and data on program administration and use.\nRelated: SNAP is Effective and Efficient\nSNAP, the nation\u2019s most important anti-hunger program, helps roughly 35 million low-income Americans to afford a nutritionally adequate diet. WIC \u2014 short for the Special Supplemental Nutrition Program for Women, Infants, and Children \u2014 provides nutritious foods, information on healthy eating, and health care referrals to about 8 million low-income pregnant and postpartum women, infants, and children under five. The School Lunch and School Breakfast programs provide free and reduced-price meals that meet federal nutritional standards to over 22 million school children from low-income families.\n- Introduction to SNAP\nThe Center designs and promotes polices to make the Food Stamp Program more adequate to help recipients afford an adequate diet, more accessible to eligible families and individuals, and easier for states to administer. We also help states design their own food stamp programs for persons ineligible for the federal program. Our work on the WIC program includes ensuring that sufficient federal funds are provided to serve all eligible applicants and on helping states contain WIC costs. Our work on child nutrition programs focuses on helping states and school districts implement recent changes in how they determine a child's eligibility for free or reduced-priced school meals.\nMay 17, 2013\nRevised May 16, 2013\nUpdated May 8, 2013\nRevised May 1, 2013\nUpdated May 1, 2013\n- View All By Date", "id": "<urn:uuid:bc53165f-1d0a-4c12-aaa0-5d50b436cadf>", "dump": "CC-MAIN-2013-20", "url": "http://www.cbpp.org/research/?fa=topic&id=31", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9420174956321716, "token_count": 609, "score": 3.3125, "int_score": 3}
{"text": "Our opinion: Fostering better nutrition is a key to reducing public health costs. That makes the scarcity of large grocery stores in some urban neighborhoods, especially poor ones, a matter of public policy.\nIt\u2019s easy, and comforting, to assume that in an modern, industrialized nation, everyone eats well. Surely with programs like food stamps, even needy people are doing OK, right?\nMore than 40,000 people in Albany and Schenectady live in what are known as food deserts \u2014 places where a grocery store is at least a mile away. Many are poor. The U.S. Department of Agriculture estimates that about 3,700 of the more than 23,000 people in Albany who don\u2019t live near a large grocer are poor. In Schenectady, about 5,400 of the more than 19,000 people for whom a supermarket trip is more like a trek are low income. Moreover, those estimates were based largely on information gathered before a recession that has made even more people poor.\nThat\u2019s not just an inconvenience. It\u2019s a public health issue in a society where the problems associated with poor nutrition and obesity are linked to the growing cost of public health care.\nOne suggestion for alleviating the problem comes from the American Cancer Society, which is reviving the controversial idea of a tax on soda and other sugar-sweetened drinks. The group suggests using the tax to help fund and evaluate child obesity programs. An even more productive use might be to devote a portion of such a tax to local efforts to lure larger grocery stores to urban neighborhoods that don\u2019t have them now.\nIt would seem doubtful, though, that the state Legislature will be any more inclined to implement a soda tax than it was last year, when it refused to do so even with the prospect of raising $400 million in new revenue and even with the urging of the state health commissioner and then-Gov. David Paterson.\nThat doesn\u2019t mean it\u2019s not worth trying for a soda tax \u2014 encouraging kids in particular to cut back on sugary beverages is not a bad goal at all, nor is raising funds that could be used to promote a healthier citizenry. But the more likely reality is that New York and communities will have to find ways to do something about poor nutrition and food deserts without the help of a soda tax.\nHere\u2019s one direction: Since last March, Capital District Community Gardens has been supplying nine convenience stores and markets with fresh produce under a five-year, $175,000-a-year grant from the state Health Department that also helps underwrite fitness programs. Veggie Mobile Sprout, as this program is called, delivers produce twice a week at wholesale prices, enabling urban stores in Albany and Schenectady to sell fresh food at supermarket rates and offer something better than their typical fare of often fattening and less nutritious processed food. The program\u2019s goal is to become self-sustaining.\nHow many more ideas to promote healthy and affordable food choices are there that might perhaps benefit from some initial public investment or other support? Why not make identifying and nurturing those ideas part of the mission of local community development or economic development entities?\nFunding for such programs would not be handouts; beyond improving nutrition and reducing obesity with a long-term eye toward reducing public health costs, they would help get greater value out of the public assistance that goes to people in low-income neighborhoods.\nSounds like a shopping cart full of public interest to us.", "id": "<urn:uuid:fc2b2237-bb58-4d64-ab60-655ff3250f48>", "dump": "CC-MAIN-2013-20", "url": "http://blog.timesunion.com/opinion/green-up-%E2%80%A8urban-deserts/16365/", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9627323150634766, "token_count": 725, "score": 2.65625, "int_score": 3}
{"text": "Nutrition has a big impact on health, including major diseases such as heart disease, osteoporosis, and cancer. Our work is designed to help people keep track of the nutrional content of foods they have eaten. Our work uses shopping receipts to generate suggestions about healthier food items that could help to supplement missing nutrients. Our application, based on shopping receipt data, provides access to ambiguous suggestions for more nutritious purchases.\nOur goal is to contribute a better understanding of how a sensor-based application can be integrated in everyday life. To do this, we chose an approach that can easily be replicated for many users, deployed, and tested for months at a time. We are currently in the process of conducting a diary study that can provide data on which we can train our prediction algorithms. We conducted a formative user study that suggested that receipts may provide enough information to extend our work by also estimating what people are actually eating, as opposed to simply what they are purchasing. We are also interviewing and observing people's shopping and food managing habits to further inform the system design.", "id": "<urn:uuid:0cbebbe3-f6c6-48cc-9401-6782625b11a6>", "dump": "CC-MAIN-2013-20", "url": "http://www.eecs.berkeley.edu/IPRO/Summary/Old.summaries/03abstracts/jmankoff.2.html", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9747440218925476, "token_count": 216, "score": 2.8125, "int_score": 3}
{"text": "Survey data is a snapshot of a population, a moment captured in numbers, like vital signs: height, weight, temperature, blood pressure, etc. People build trend lines and watch for changes, shifting strategies as they make educated guesses about what\u2019s going on. What\u2019s holding steady? What\u2019s spiking? What\u2019s on the decline?\nJust as a thermometer makes no judgment, the Pew Research Center provides data about the changing world around us. We don\u2019t advocate for outcomes or recommend policies. Rather, we provide an updated record so that others can make those pronouncements and recommendations based on facts.\nThe latest in our health research series is being released today. Health Online 2013 finds that internet access and interest in online health resources are holding steady in the U.S. For a quick overview, read on\u2026\nWhat is new?\n1 in 3 U.S. adults use the internet to diagnose themselves or someone else \u2013 and a clinician is more likely than not to confirm their suspicions. This is the first time we \u2013 or anyone else \u2013 has measured this in a straightforward, national survey question.\n1 in 4 people looking online for health info have hit a pay wall. This is the first data I know of that begins to answer the important question: what is the public impact of closed-access journals?\nWe added three new health topics:\n- 11% of internet users have looked online for information about how to control their health care costs.\n- 14% of internet users have looked online for information about caring for an aging relative or friend.\n- 16% of internet users have looked online for information about a drug they saw advertised.\n(A full list of all the health topics we\u2019ve included, 2002-10, is available here.)\nWhat has changed?\nThe percentage of people who have consulted online reviews of drugs and medical treatments dropped (and I don\u2019t know why \u2014 do you have a theory? Please post a comment.)\nRelated: why aren\u2019t health care review sites catching on? Pew Internet has tracked a boom in consumer reviews of other services and products \u2014 why not health care?\nWhat to keep an eye on?\nOne of my favorite survey questions is asked of all adults and attempts to capture a broad portrait of health care resources that someone might tap into when they\u2019re sick.\nIt\u2019s a useful question for keeping online resources in perspective. I think it\u2019s also going to prove useful in the coming years as the landscape shifts and people have more opportunities to connect with clinicians online. How fast will that \u201dYes, online\u201d group grow? Or will care always be hands-on at its core \u2014 and therefore we should see growth in the \u201cYes, both\u201d category?\nSpeaking of keeping things in perspective, I think it\u2019s important to remind ourselves that there are pockets of people who remain offline. Internet access drives information access.\nHere\u2019s a table from the Appendix that digs even deeper:\nIn other words, 64% of college educated adults in the U.S. have researched a specific disease online, compared with just 16% of U.S. adults who have not completed high school.\nThese are just a few highlights \u2014 please read the report, ask questions, and tell us what you think: How\u2019s the patient doing, based on this new set of vital signs? What do you prescribe?", "id": "<urn:uuid:06a1d3e1-b436-41ec-b891-5dc64a3f53ac>", "dump": "CC-MAIN-2013-20", "url": "http://e-patients.net/archives/2013/01/health-online-2013-survey-data-as-vital-sign.html", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.946489691734314, "token_count": 716, "score": 2.578125, "int_score": 3}
{"text": "Posted: Oct 9, 2012 3:00 PM by Robert Preidt\nTUESDAY, Oct. 9 (HealthDay News) -- A gene test that can identify people at risk for mouth cancer has been developed by British researchers.\nThe test detects precancerous cells in patients with benign-looking mouth lesions and could lead to earlier treatment for at-risk patients and improve their chances of survival, according to the team at Queen Mary, University of London.\nThey used the quantitative Malignancy Index Diagnostic System test -- which measures the level of 16 genes -- on more than 350 head and neck tissue specimens from nearly 300 patients and found that it had a cancer detection rate between 91 percent and 94 percent.\nThe study was published Oct. 4 in the International Journal of Cancer.\nMouth cancer affects more than half a million people worldwide each year, and that number is expected to rise above 1 million by 2030, according to World Health Organization figures. Most cases of mouth cancer are caused by either smoking or chewing tobacco, or drinking alcohol.\nMouth lesions are common, but only 5 percent to 30 percent may turn into cancers. Until now, no test has been able to accurately detect which lesions will become cancerous. Many mouth cancers are diagnosed at later stages, when the chances of survival are greatly reduced.\n\"A sensitive test capable of quantifying a patient's cancer risk is needed to avoid the adoption of a 'wait-and-see' intervention,\" study lead investigator and test inventor Dr. Muy-Teck Teh said in a university news release. \"Detecting cancer early, coupled with appropriate treatment, can significantly improve patient outcomes, reduce mortality and alleviate long-term public health care costs.\"\nAlthough this study shows that the test is effective for early cancer detection, further clinical trials are needed to evaluate its long-term clinical benefits.\nThe U.S. National Cancer Institute has more about mouth and other types of oral cancer.\nSOURCE: Queen Mary, University of London, news release, Oct. 4, 2012\nCopyright (c) 2012 HealthDay. All rights reserved.", "id": "<urn:uuid:dea47dad-7553-4f41-a171-3de4c48fe96c>", "dump": "CC-MAIN-2013-20", "url": "http://www.koaa.com/news/new-gene-test-predicts-whose-mouth-lesions-might-be-cancerous/", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9447839856147766, "token_count": 430, "score": 3.015625, "int_score": 3}
{"text": "A genome-wide association study appearing in PLOS Genetics that involved thousands of individuals of European ancestry identified five genetic loci that appear to be associated with facial features. Among them: variants in and around genes implicated in prior studies of conditions that are characterized by face and/or skull malformations. The researchers behind the study, members of the International Visible Trait Genetics, or VisiGen, Consortium, argue that the new findings could contribute to what's known about facial evolution and development in humans, while at once laying the foundation for forensic tools for predicting facial features based on DNA alone. For more on this study, see this story from our sister publication GenomeWeb Daily News.\nInvestigators from the Cleveland Clinic and elsewhere used post-mortem brain samples to look at the epigenetic and transcriptional profiles associated with autism spectrum disorder. As they reported in PLOS One, the researchers relied on arrays and bisulfite sequencing to assess genome-wide gene expression and DNA methylation profiles in two brain regions \u2014 the cerebellar hemisphere cortex and the Brodmann area 19 occipital cortex \u2014 in samples from nine males with idiopathic cases of autism spectrum disorder and nine unaffected male controls in the same age range. Overall brain expression patterns varied from one individual with ASD to the next. But the team did uncover some shared features within the ASD samples, including lower-than-usual expression of genes in mitochondrial oxidative phosphorylation and protein production pathways in the brain samples from individuals with autism and shifts in the expression of certain brain-related genes.\nA PLOS Pathogens study of dengue virus by French researchers explores the basis for the pronounced conservation that's been noted in nucleotides found at the ends of the virus' RNA-based genome. Using chemical synthesis experiments, assays, and other analyses, the group determined that the virus, a representative of the Flavivirus genus, relies on an RNA end-repair process that involves the RNA-dependent RNA polymerase produced by a dengue virus gene called NS5. \"Our findings provide a direct demonstration of the implication of a viral RNA polymerase in the conservation and repair of genome ends,\" the study's authors wrote. \"Other polymerases from other RNA virus families are likely to employ similar mechanisms.\"", "id": "<urn:uuid:e52c0e3f-4d18-4b99-91e5-6894e85b12b2>", "dump": "CC-MAIN-2013-20", "url": "http://www.genomeweb.com/blog/week-plos-204", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9166100025177002, "token_count": 461, "score": 3.390625, "int_score": 3}
{"text": "Our opinion on ...\n- Executive Summary\n- Necessity for a response\n- Genetic testing\n- General principles\n- Other considerations\nThe present paper constitutes the input of Alzheimer Europe and its member organisations to the ongoing discussions within Europe about genetic testing (in the context of Alzheimer's disease and other forms of dementia).\nAlzheimer Europe would like to recall some general principles which guide this present response:\n- Having a gene associated with Alzheimer's disease or another form of dementia does not mean that a person has the disease.\n- People who have a gene linked to Alzheimer's disease or another form of dementia have the same rights as anyone else.\n- Genetic testing does not only affect the person taking the test. It may also reveal information about other relatives who might not want to know.\n- No genetic test is 100% accurate.\n- The extent to which health cover is provided to citizens by the State social security system and/or privately contracted by individuals differs from one country to the next.\nOn the basis of these principles, Alzheimer Europe has developed the following position with regard to genetic testing:\n- Alzheimer Europe firmly believes that the use and/or possession of genetic information by insurance companies should be prohibited.\n- Alzheimer Europe strongly supports research into the genetic factors linked to dementia which might further our understanding of the cause and development of the disease and possibly contribute to future treatment.\n- Based on its current information, Alzheimer Europe does not encourage the use of any genetic test for dementia UNLESS such test has a high and proven success rate either in assessing the risk of developing the disease (or not as the case may be) or in detecting the existence of it in a particular individual.\n- Alzheimer Europe requests further information on the accuracy, reliability and predictive value of any genetic tests for dementia.\n- Genetic testing should always be accompanied by adequate pre- and post-test counselling.\n- Anonymous testing should be possible so that individuals can ensure that such information does not remain in their medical files against their will.\nIt is extremely important for people with dementia to be diagnosed as soon as possible. In the case of Alzheimer\u2019s disease, an early diagnosis may enable the person concerned to benefit from medication, which treats the global symptoms of the disease and is most effective in the early to mid stages of the disease. Most forms of dementia involve the gradual deterioration of mental faculties (e.g. memory, language and thinking etc.) but in the early stages, it is still possible for the person affected to make decisions concerning his/her finances and care etc. \u2013 hence the importance of an early diagnosis.\nIf it were possible to detect dementia before the first symptoms became obvious, this would give people a greater opportunity to make informed decisions about their future lives. This is one of the potential benefits of genetic testing.\nOn the other hand, such information could clearly be used in ways which would be contrary to their personal interests, perhaps resulting in employment discrimination, loss of opportunities, stigmatisation, increased health insurance costs or even loss of health insurance to name but a few examples.\nThe present discussion paper outlines some of the recommendations of Alzheimer Europe and its member organisations and raises a few points which deserve further clarification and discussion.\nThe necessity for a response by Alzheimer Europe\nIn the last few years, the issue of genetic testing has been increasingly debated. In certain European countries there are already companies offering such tests. Unfortunately, the general public do not always fully understand what the results of such tests imply and there are no regulations governing how they are carried out i.e. what kind of information people receive, how the results are presented, whether there is any kind of counselling afterwards and the issue of confidentiality etc.\nIn order to provide information to people with dementia and other people interesting in knowing about their own state of health and in order to protect them from the unscrupulous use of the results of genetic tests, Alzheimer Europe has developed the present Position Paper.\nThese general principles as well as the Convention of Human Rights and Biomedicine and the Universal Declaration on the Human Genome and Human Rights dictate Alzheimer Europe\u2019s position with regard to genetic testing.\nAlzheimer Europe would like to draw a distinction between tests which detect existing Alzheimer's disease and tests which assess the risk of developing dementia Alzheimer's disease at some time in the future:\n- Diagnostic testing : Familial early onset Alzheimer\u2019s disease (FAD) is associated with 3 genes. These are the amyloid precursor protein (APP), presenilin-1 and presenilin-2. These genetic mutations can be detected by genetic testing. However, it is important to note that the test only relates to those people with FAD (i.e. about 1% of all people with Alzheimer\u2019s disease). In the extremely limited number of families with this dominant genetic disorder, family members inherit from one of their parents the part of the DNA (the genetic make-up), which causes the disease. On average, half the children of an affected parent will develop the disease. For those who do, the age of onset tends to be relatively low, usually between 35 and 60.\n- Assessment for risk testing : Whether or not members of one\u2019s family have Alzheimer\u2019s disease, everyone risks developing the disease at some time. However, it is now known that there is a gene, which can affect this risk. This gene is found on chromosome 19 and it is responsible for the production of a protein called apolipoprotein E (ApoE). There are three main types of this protein, one of which (ApoE4), although uncommon, makes it more likely that Alzheimer\u2019s disease will occur. However, it does not cause the disease, but merely increases the likelihood. For example, a person of 50, would have a 2 in 1,000 chance of developing Alzheimer\u2019s disease instead of the usual 1 in 1,000, but might never actually develop it. Only 50% of people with Alzheimer\u2019s disease have ApoE4 and not everyone with ApoE4 suffers from it.\nThere is no way to accurately predict whether a particular person will develop the disease. It is possible to test for the ApoE4 gene mentioned above, but strictly speaking such a test does not predict whether a particular person will develop Alzheimer\u2019s disease or not. It merely indicates that he or she is at greater risk. There are in fact people who have had the ApoE4 gene, lived well into old age and never developed Alzheimer\u2019s disease, just as there are people who did not have ApoE4, who did develop the disease. Therefore taking such a test carries the risk of unduly alarming or comforting somebody.\nAlzheimer Europe agrees with diagnostic genetic testing provided that pre- and post-test counselling is provided, including a full discussion of the implications of the test and that the results remain confidential.\nWe do not actually encourage the use of genetic testing for assessing the risk of developing Alzheimer's disease. We feel that it is somewhat unethical as it does not entail any health benefit and the results cannot actually predict whether a person will develop dementia (irrespective of the particular form of ApoE s/he may have).\nWe are totally opposed to insurance companies having access to results from genetic tests for the following reasons:\n- This would be in clear opposition to the fundamental principle of insurance which is the mutualisation of risk through large numbers (a kind of solidarity whereby the vast majority who have relatively good health share the cost with those who are less fortunate).\n- Failure to respect this principle would create an uninsurable underclass and lead to a genetically inferior group.\n- This in turn could entail the further stigmatisation of people with dementia and their carers.\n- In some countries, insurance companies manage to reach decisions on risk and coverage without access to genetic data.\n- We therefore urge governments and the relevant European bodies to take the necessary action to prohibit the use or possession of genetic data by insurance companies.\nAlzheimer Europe recognises the importance of research into the genetic determinants of Alzheimer\u2019s disease and other forms of dementia. Consequently,\n- we support the use of genetic testing for the purposes of research provided that the person concerned has given informed consent and that the data is treated with utmost confidentiality; and\n- we would also welcome further discussion about the problem of data management.\nIn our opinion, any individual who wishes to take a genetic test should be able to choose to do so anonymously in order to ensure that such information does not remain in his/her medical file.\nAt its Annual General Meeting in Munich on 15 October 2000, Alzheimer Europe adopted recommendations on how to improve the legal rights and protection of adults with incapacity due to dementia. This included a section on bioethical issues. These recommendations obviously need to guide any response of the organisation regarding genetic testing for people who suspect or fear they may have dementia and also those who have taken the test and did develop dementia.\n- The adult with incapacity has the right to be informed about his/her state of health.\n- Information should, where appropriate, cover the following: the diagnosis, the person's general state of health, treatment possibilities, potential risks and consequences of having or not having a particular treatment, side-effects, prognosis and alternative treatments.\n- Such information should not be withheld solely on the grounds that the adult is suffering from dementia and/or has communication difficulties. Attempts should be made to provide information in such a way as to maximise his/her ability to understand, making use of technology and other available techniques to enhance communication. Attention should be paid to any possible difficulty understanding, retaining information and communicating, as well as his/her level of education, reasoning capacity and cultural background. Care should be taken to avoid causing unnecessary anxiety and suffering.\n- Written as well as verbal information should always be provided as a back-up. The adult should be granted access to his/her medical file(s). S/he should also have the opportunity to discuss the contents of the medical file(s) with a person of his/her choice (e.g. a doctor) and/or to appoint someone to receive information on his/her behalf.\n- Information should not be given against the will of the adult with incapacity.\n- The confidentiality of information should extend beyond the lifetime of the adult with incapacity. If any information is used for research or statistical purposes, the identity of the adult with incapacity should remain anonymous and the information should not be traceable back to him/her (in accordance with the provisions of national laws on respect for the confidentiality of personal information). Consideration should be given to access to information where abuse is suspected.\n- A clear refusal by the adult with incapacity to grant access to information to any third party should be respected regardless of the extent of his/her incapacity, unless this would be clearly against his/her best interests e.g. carers should have provided to them information on a need to know basis to enable them to care effectively for the adult with incapacity.\n- People who receive information about an adult with incapacity in connection with their work (either voluntary or paid) should be obliged to treat such information with confidentiality.\nPeople who take genetic tests and do not receive adequate pre and post test counselling may suffer adverse effects.\nFear of discrimination based on genetic information may deter people from taking genetic tests which could be useful for research into the role of genes in the development of dementia.\nCertain tests may be relevant for more than one medical condition. For example, the ApoE test is used in certain countries as part of the diagnosis and treatment of heart disease. There is therefore a risk that a person might consent for one type of medical test and have the results used for a different reason.\nLast Updated: jeudi 06 ao\u00fbt 2009", "id": "<urn:uuid:62210bfc-b709-4c59-93ac-36ec9784506d>", "dump": "CC-MAIN-2013-20", "url": "http://www.alzheimer-europe.org/FR%C5%A0%C2%B7%C5%A0%20/Policy-in-Practice2/Our-opinion-on/Genetic-testing", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9431832432746887, "token_count": 2443, "score": 2.625, "int_score": 3}
{"text": "- Our Story\n- In Memory\nVaccination and Immunotherapy for Alzheimer\u2019s Disease\nVaccination against amyloid is a promising approach for the development of Alzheimer\u2019s disease (AD) therapeutics. Approximately half of the investigational new therapeutics in human clinical trials for AD are active or passive immunotherapeutics.\nActive vaccination involves the injection of an antigen and relies on the production of antibodies in the vaccinated patient. Four human clinical trials of active vaccination currently are under way. Passive immunization is also a promising strategy that involves the production of antibodies outside of the patient and injection of these antibodies. There are currently 12 clinical trials of passive immunization. You can check for Alzheimer therapeutics in human clinical trials by visiting www.clinicaltrials.gov and searching for key words \u201cAlzheimer\u2019s and immunotherapy.\u201d\nThinking out of the box\nThe development of vaccinations as a strategy for treating or preventing Alzheimer\u2019s is an example of thinking out of the box. Vaccinations commonly are associated with infectious diseases, like influenza, small pox and polio, which appear to have little in common with neurodegenerative diseases, like Alzheimer\u2019s. Moreover, the brain is an immunoprivileged site with little access to antibodies, so it seems unlikely antibodies would be protective in the brain.\nResearchers were pleasantly surprised when Dale Schenk and co-workers at Elan Inc. reported that vaccination of transgenic mouse models of AD against the amyloid A\u00df peptide prevented amyloid deposition in young animals and removed pre-existing amyloid deposits in older animals. Subsequent work showed that immunization against A\u00df prevented or reversed many other pathological features and prevented cognitive dysfunction in transgenic mice and non-human primates. This vaccine (Elan AN1792) was tested in human clinical trials, where it showed similar beneficial effects of removing amyloid deposits and slowing cognitive decline in patients with significant levels of anti-A\u00df antibodies, but the clinical trial was halted because 6 percent of the patients developed meningoencephalitis, an inflammatory side effect.\nSecond-generation vaccines and passive immunization\nTo circumvent the unwanted inflammatory side effects, second-generation active vaccines have been developed and passive immunization strategies have been explored. The second-generation vaccines use small pieces of the amyloid A\u00df sequence to avoid activating the T-cells responsible for meningoencephalitis, while passive immunization bypasses the human immune response by directly supplying antibodies. These newer strategies have shown the same beneficial effects in transgenic mice and passive immunization has shown some promise in a subset of patients in human trials, but they have raised new questions about their effectiveness and potential new side effects. Elan/Wyeth reported preliminary results from clinical trials of their monoclonal antibody, Bapineuzimab, that demonstrated only a small benefit in a subgroup of patients who lack the apoE4 genotype. They also failed to observe an improved benefit with an increased dose of antibody and reported side effects, like a buildup of fluid in the brain. Results of active vaccination human clinical trials with second-generation vaccines remain to be reported.\nThird-generation vaccines and antibodies: Thinking perpendicular to the box\nBoth second-generation vaccines and antibodies suffer from a common problem. They both target linear amino acid sequences found in normal human proteins (the amyloid precursor protein) and in the amyloid deposits themselves. Making antibodies against normal human proteins can cause autoimmune side effects, in which the immune system is attacking normal human cells in addition to the Alzheimer\u2019s pathology. Fortunately, it is difficult to make antibodies against self-proteins because of immune suppression of auto antibodies. Third-generation vaccines seek to overcome these problems of autoimmune side effects and autoimmune suppression by using antibodies that target structures specific to the amyloid aggregates and that do not react with normal human proteins.\nCure Alzheimer\u2019s Fund has been supporting two projects that seek to develop third-generation immunotherapeutics. Dr. Charles Glabe\u2019s laboratory is developing active vaccines and monoclonal antibodies that recognize conformations of the amyloid peptide that only occur in the pathological amyloid oligomer aggregates, while Dr. Rob Moir\u2019s lab is working on cross-linked amyloid peptides (CAPs) that are only found in disease-related aggregates. Dr. Glabe\u2019s strategy relies on the fact that when the A\u00df peptide aggregates into \u00df-sheet oligomers, it creates new antibody recognition sites, known as epitopes, that are not found on native proteins. The surprising finding is that these oligomer-specific antibodies recognize amyloid oligomers from other diseases that involve amyloids formed from sequences unrelated to A\u00df. This means the same antibodies also may be effective for other amyloid-related neurodegenerative diseases, like Parkinson\u2019s disease.\nThe explanation for why the antibodies are specific for amyloid oligomers that involve several individual peptide strands arranged in a sheet and yet recognize these sheets when they are formed from other amino acid sequences is simple and elegant (Figure 1). It is now known that most pathological amyloids aggregate into simple and very regular structures where the peptide strands are arranged in parallel and where the amino acid sequence is in exact register. This is like a sheet of paper upon which the same sentence is written on each line. The individual amino acids line up and down the sheet in homogeneous tracts, known as \u201csteric zippers.\u201d The steric zippers do not occur in normal protein structures and the oligomer-specific antibodies are thought to recognize these steric zipper patterns on the surface of the sheets. Since all proteins are made up using the same 20 amino acids, any sequence in this parallel, in-register structure gives rise to the same steric zippers regardless of the linear sequence, which can explain why the antibodies recognize the oligomers formed by different proteins.\nDr. Moir\u2019s group is working on CAPs, where A\u00df is cross-linked by oxidation of a tyrosine residue at position 10 of the peptides\u2019 sequence. A\u00df is oxidized after it is produced from the amyloid precursor protein as a consequence of the abnormally high level of oxidative activity in a brain with AD and the peptides\u2019 propensity to bind redox active metals. Excessive CAPs generation is associated with the disease state and is not a normal feature of A\u00df biology. The cross-linking at tyrosine 10 that gives rise to CAPs may serve to align the peptides in a parallel, in-register fashion and promote the generation of still-larger oligomeric aggregates that display steric zippers on their surface.\nDr. Moir and Dr. Rudy Tanzi\u2019s labs found that natural antibodies to CAPs are reduced in the blood of patients with AD. More recently, evidence published by Tony Weiss-Coray\u2019s group at Stanford University supports the idea that antibodies that recognize steric zippers and CAPs may be important for protecting against Alzheimer\u2019s disease. The levels of these antibodies that target the zippers and CAPs were among the highest in young, normal humans; levels dropped with aging and with AD. Furthermore, the results of a recent study supported by Baxter Biosciences of patients that received human antibodies purified from normal individuals (IVIg) reported that antibody treatment reduced the risk of being diagnosed with AD by 42 percent over the five-year study period. This is one of the most remarkable reports of prevention of AD by any therapy. Although the normal human antibodies that target amyloid primarily recognize the steric zippers and CAPs, these antibodies are present at relatively low levels. It is reasonable to imagine that an even greater protective effect might be achieved by boosting the levels of these protective antibodies by either active vaccination or passive immunization.\nFigure 1 shows how the same steric zipper patterns are formed on parallel, in-register oligomers from completely different sequences. A segment of the A\u00df sequences is shown in the upper left corner and a random sequence is shown in the upper right. Each amino acid is designated by a capital letter. Typical antibodies recognize the linear sequence (from left to right) indicated in the horizontal boxes, which is unique to each sequence. When the peptides aggregate to form pathological oligomers, they line up in a parallel, in-register fashion, shown below. This gives rise to steric zippers that run up and down the sheet perpendicular to the sequence, shown in vertical boxes. Aggregation-dependent, disease-specific antibodies recognize the steric zippers from many different amyloid sequences. Zippers from F and V amino acids are shown in boxes, but there are potentially 20 different zippers; one for each of the 20 amino acids.\nThe fact that a completely random sequence can form the same type of steric zipper as is found in A\u00df amyloid in Alzheimer\u2019s disease means we can use a non-human, random peptide sequence as a vaccine to produce a protective immune response that has a very low potential for autoimmune side effects. Vaccines based on non-human peptides, like diphtheria and pertussis toxin, are so safe they routinely are given to infants. There is no reason to expect that a vaccine for AD that targets the disease-specific steric zippers wouldn\u2019t be as safe and free of side effects. A goal of the research funded by Cure Alzheimer\u2019s Fund is to do the preclinical investigations that are a necessary prelude to getting these third-generation vaccines and monoclonal antibodies that target disease-specific epitopes into human clinical trials.", "id": "<urn:uuid:063831fb-bde6-43d4-95fe-4286849bf7d1>", "dump": "CC-MAIN-2013-20", "url": "http://www.curealzfund.org/2009/10/vaccination-and-immunotherapy-alzheimer%E2%80%99s-disease?page=70", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.940550684928894, "token_count": 2013, "score": 2.890625, "int_score": 3}
{"text": "Ethics of dementia research\nWhat are clinical trials and how are they controlled/governed?\nA clinical trial is a biomedical/health-related study into the effects on humans of a new medical treatment (medicine/drug, medical device, vaccine or new therapy), sometimes called an investigational medicinal product (IMP). Before a new drug is authorised and can be marketed, it must pass through several phases of development including trial phases in which its safety, efficacy, risks, optimal use and/or benefits are tested on human beings. Existing drugs must also undergo clinical testing before they can be used to treat other conditions than that for which they were originally intended.\nOrganisations conducting clinical trials in the European Union must, if they wish to obtain marketing authorisation, respect the requirements for the conduct of clinical trials. These can be found in the Clinical Trials Directive (\u201cDirective 2001/20/EC of the European Parliament and of the Council of 4 April 2001 on the approximation of the laws, regulations and administrative provisions of the Member States relating to the implementation of good clinical practice in the conduct of clinical trials on medicinal products for human use\u201d).\nThere are also guidelines to ensure that clinical trials are carried out in accordance with good clinical practice. These are contained in the \u201cCommission Directive 2005/28/EC of 8 April 2005 laying down principles and detailed guidelines for good clinical practice as regards investigational medicinal products for human use, as well as the requirements for authorisation of the manufacturing or importation of such products\u201d (also known as the Good Clinical Practice or GCP for short). This document provides more concrete guidelines and lends further support to the Clinical Trials Directive.\nThe London-based European Medicines Agency (EMA) has published additional, more specific guidelines which must also be respected. These include guidelines on inspection procedures and requirements related to quality, safety and efficacy.\nCopies of the above-mentioned documents in 22 languages can be found at: http://ec.europa.eu/enterprise/pharmaceuticals/clinicaltrials/clinicaltrials_en.htm\nThe protection of people participating in clinical trials (and in most cases in other types of research) is further promoted by provisions of:\n- the European Convention on Human Rights and Biomedicine (Oviedo Convention, Act 2619/1998),\n- the Additional protocol to the Oviedo Convention concerning Biomedical Research\n- the Nuremberg Code of 1949,\n- the revised Helsinki Declaration of the World Medical Association regarding Ethical Principles for Medical Research Involving Human Subjects,\n- The Belmont Report of 18 April 1979 on the Ethical Principles and Guidelines for the Protection of Human Subjects of Research.\nWhat are the different phases of trials?\nTesting an experimental drug or medical procedure is usually an extremely lengthy process, sometimes lasting several years. The overall procedure is divided into a series of stages (known as phases) which are described below.\nClinical testing on humans can only begin after a pre-clinical phase, involving laboratory studies (in vitro) and tests on animals, which has shown that the experimental drug is considered safe and effective.\nWhilst a certain amount of testing can be carried out by means of computer modelling and by isolating cells and tissue, it becomes necessary at some point in time to test the drug on a living creature. Animal testing is an obligatory stage in the process of obtaining regulatory approval for new drugs and medicines, and hence a legal requirement (EU Directive 2001/83/EC relating to Medicinal Products for Human Use). The necessity of carrying out prior testing on animals is also stated in the World Medical Association\u2019s \u201cEthical Principles for Medical Research Involving Human Subjects.\nIn order to protect the well-being of research animals, researchers are guided by three principles which are called the 3Rs:\nReduce the number of animals used to a minimum\nRefine the way that experiments are carried out so that the effect on the animal is minimised and animal welfare is improved\nReplace animal experiments with alternative (non-animal) techniques wherever possible.\nIn addition, most countries will have official regulatory bodies which control animal research. Most animals involved in research are mice. However, no animal is sufficiently similar to humans (even genetically modified ones) to make human testing unnecessary. For this reason, the experimental drug must also be tested on humans.\nThe main phases of clinical trials\nClinical trials on humans can be divided into three main phases (literally, phase I, II and III). Each phase has specific objectives (please see below) and the number of people involved increases as the trial progresses from one phase to the next.\nPhase I trials\nPhase 1 trials are usually the first step in testing a new drug or treatment on humans after successful laboratory and animal testing. They are usually quite small scale and usually involve healthy subjects or sub-groups of patients who share a particular characteristic. The aims of these trials are:\n- to assess the safety of experimental drugs,\n- to evaluate any possible side effects,\n- to determine a safe dose range,\n- to see how the body reacts to the drug (how it is absorbed, distributed and eliminated from the body, the effects that it has on the body and the effects it has on biomarkers).\nDose ranging, sometimes called dose escalation, studies may be used as a means to determine the most appropriate dosage, but the doses administered to the subjects should only be a fraction of those which were found to cause harm to animals in the pre-clinical studies.\nThe process of determining an optimal dose in phase I involves quite a high degree of risk because this is the first time that the experimental treatment or drug has been administered to humans. Moreover, healthy people\u2019s reactions to drugs may be different to those of the target patient group. For this reason, drugs which are considered to have a potentially high toxicity are usually tested on people from the target patient group.\nThere are a few sequential approaches to phase I trials e.g. single ascending dose studies, multiple ascending dose studies and food effect.\nIn single ascending dose studies (SAD), a small group of subjects receive a very low dose of the experimental drug and are then observed in order to see whether that dose results in side effects. For this reason, trials are usually conducted in hospital settings. If no adverse side effects are observed, a second group of subjects are given a slightly higher dose of the same drug and also monitored for side-effects. This process is repeated until a dose is reached which results in intolerable side effects. This is defined as the maximum tolerated dose (MTD).\nMultiple ascending dose studies (MAD) are designed to test the pharmacokinetics and pharmacodynamics of multiple doses of the experimental drug. A group of subjects receives multiple doses of the drug, starting at the lowest dose and working up to a pre-determined level. At various times during the period of administration of the drug, and particularly whenever the dose is increased, samples of blood and other bodily fluids are taken. These samples are analysed in order to determine how the drug is processed within the body and how well it is tolerated by the body.\nFood effect studies are investigations into the effect of food intake on the absorption of the drug into the body. This involves two groups of subjects being given the same dose of the experimental drug but for one of the groups when fasting and for the other after a meal. Alternatively, this could be done in a cross-over design whereby both groups receive the experimental drug in both conditions in sequence (e.g. when fasting and on another occasion after a meal). Food effect studies allow researchers to see whether eating before the drug is given has any effect on the absorption of the drug by the body.\nPhase II trials\nHaving demonstrated the initial safety of the drug (often on a relatively small sample of healthy individuals), phase II clinical trials can begin. Phase II studies are designed to explore the therapeutic efficacy of a treatment or drug in people who have the condition that the drug is intended to treat. They are sometimes called therapeutic exploratory trials and tend to be larger scale than Phase I trials.\nPhase II trials can be divided into Phase IIA and Phase IIB although sometimes they are combined.\nPhase IIA is designed to assess dosing requirements i.e. how much of the drug should patients receive and up to what dose is considered safe? The safety assessments carried out in Phase I can be repeated on a larger subject group. As more subjects are involved, some may experience side effects which none of the subjects in the Phase I experienced. The researchers aim to find out more about safety, side effects and how to manage them.\nPhase IIB studies focus on the efficacy of the drug i.e. how well it works at the prescribed doses. Researchers may also be interested in finding out which types of a specific disease or condition would be most suitable for treatment.\nPhase II trials can be randomised clinical trials which involve one group of subjects being given the experimental drug and others receiving a placebo and/or standard treatment. Alternatively, they may be case series which means that the drug\u2019s safety and efficacy is tested in a selected group of patients. If the researchers have adequately demonstrated that the experimental drug (or device) is effective against the condition for which it is being tested, they can proceed to Phase III.\nPhase III trials\nPhase III trials are the last stage before clinical approval for a new drug or device. By this stage, there will be convincing evidence of the safety of the drug or device and its efficacy in treating people who have the condition for which it was developed. Such studies are carried out on a much larger scale than for the two previous phases and are often multinational. Several years may have passed since the original laboratory and animal testing.\nThe main aims of Phase III trials are:\nto demonstrate that the treatment or drug is safe and effective for use in patients in the target group (i.e. in people for whom it is intended)\nto monitor side effects\nto test different doses or different ways of administering the drug\nto determine whether the drug could be used at different stages of the disease.\nto provide sufficient information as a basis for marketing approval\nResearchers may also be interested in showing that the experimental drug works for additional groups of people with conditions other than that for which the drug was initially developed. For example, they may be interested in testing a drug for inflammation on people with Alzheimer\u2019s disease. The drug would have already have proven safe and obtained marketing approval but for a different condition, hence the need for additional clinical testing.\nOpen label extension trails\nOpen label extension studies are often carried out immediately after a double blind randomised clinical trial of an unlicensed drug. The aim of the extended study is to determine the safety and tolerability of the experimental drug over a longer period of time, which is generally longer than the initial trial and may extend up until the drug is licensed. Participants all receive the experimental drug irrespective of which arm of the previous trial they were in. Consequently, the study is no longer blind in that everybody knows that each participant is receiving the experimental drug but the participants and researchers still do not know which group participants were in during the initial trial.\nPost-marketing surveillance studies (phase IV)\nAfter the three phases of clinical testing and after the treatment has been approved for marketing, there may be a fourth phase to study the long-term effects of drugs or treatment or to study the impact of another factor in combination with the treatment (e.g. whether a particular drug reduces agitation).\nUsually, such trials are sponsored by pharmaceutical companies and described as pharmacovigilance. They are not as common as the other types of trials (as they are not necessary for marketing permission). However, in some cases, the EMA grants restricted or provisional marketing authorisation, which is dependent on additional phase IV trails being conducted.\nExpanded access to a trial\nSometimes, a person might be likely to benefit from a drug which is at various stages of testing but does not fulfil the conditions necessary for participation in the trial (e.g. s/he may have other health problems). In such cases and if the person has a life-threatening or serious condition for which there is no effective treatment, s/he may benefit from \u201cexpanded access\u201d use of the drug. There must, however, be evidence that the drug under investigation has some likelihood of being effective for that patient and that taking it would not constitute an unreasonable risk.\nThe use of placebo and other forms of comparison\nThe main purpose of clinical drug studies is to distinguish the effect of the trial drug from other influences such as spontaneous change in the course of the disease, placebo effect, or biased observation. A valid comparison must be made with a control. The American Food and Drugs Administration recognises different types of control namely,\n- active treatment with a known effective therapy or\n- no treatment,\n- historical treatment (which could be an adequately documented natural history of the disease or condition, or the results of active treatment in comparable patients or populations).\nThe EMA considers three-armed trials (including the experimental medicine, a placebo and an active control) as a scientific gold standard and that there are multiple reasons to support their use in drug development  .\nParticipants in clinical trials are usually divided into two or more groups. One group receives the active treatment with the experimental substance and the other group receives a placebo, a different drug or another intervention. The active treatment is expected to have a positive curative effect whereas the placebo is expected to have zero effect. With regard to the aim to develop more effective treatments, there are two possibilities:\n1. the experimental substance is more effective than the current treatment or\n2. it is more effective than no treatment at all.\nAccording to article 11 of the International Ethical Guidelines for Biomedical Research (IEGBR) of 2002, participants allocated to the control group in a trial for a diagnostic, therapeutic or preventive intervention should receive an established effective intervention but it may in some circumstances be considered ethically acceptable to use a placebo (i.e. no treatment). In article 11 of the IEGBR, reasons for the use of placebo are:\n1. that there is no established intervention\n2. that withholding an established effective intervention would expose subjects to, at most, temporary discomfort or delay in relief of symptoms\n3. that use of an established effective intervention as comparator would not yield scientifically reliable results and use of placebo would not add any risk of serious or irreversible harm to the subjects.\n November 2010, EMA/759784/2010 Committee for Medicinal Products for Human Use\nThe use of placebo and the issue of irreversible harm\nIt has been suggested that clinical trials are only acceptable in ethical terms if there is uncertainty within the medical community as to which treatment is most suitable to cure or treat a disease (National Bioethics Commission of Greece, 2005). In the case of dementia, whilst there is no cure, there are a few drugs for the symptomatic treatment of dementia. Consequently, one could ask whether it is ethical to deprive a group of participants of treatment which would have most likely improved their condition for the purpose of testing a potentially better drug (National Bioethics Commission of Greece, 2005). Can they be expected to sacrifice their own best interests for those of other people in the future? It is also important to ask whether not taking an established effective intervention is likely to result in serious or irreversible harm.\nIn the 2008 amended version of the Helsinki Declaration  (World Medical Association, 1964), the possible legitimate use of placebo and the need to protect subjects from harm are addressed.\n\u201c32. The benefits, risks, burdens and effectiveness of a new intervention must be tested against those of the best current proven intervention, except in the following circumstances:\nThe use of placebo, or no treatment, is acceptable in studies where no current proven intervention exists; or\nWhere for compelling and scientifically sound methodological reasons the use of placebo is necessary to determine the efficacy or safety of an intervention and the patients who receive placebo or no treatment will not be subject to any risk of serious or irreversible harm. Extreme care must be taken to avoid abuse of this option.\u201d (WMA, 1964 with amendments up to 2008)\nThe above is also quite similar to the position supported by the Presidential Commission for the Study of Bioethical Issues (PCSBI) (2011). In its recently published report entitled \u201cMoral science: protecting participants in human subjects research  \u201d, the Presidential Commission argues largely in favour of a \u201cmiddle ground\u201d for ethical research, citing the work of Emanuel and Miller (2001) who state:\n\u201cA placebo-controlled trial can sometimes be considered ethical if certain methodological and ethical standards are met. It these standards cannot be met, then the use of placebos in a clinical trial is unethical.\u201d (Emanuel and Miller, 2001 cited in PCSBI, 2011, p. 89).\nOne of the standards mentioned is the condition that withholding proven effective treatment will not cause more than minimal harm.\nThe importance of placebo groups for drug development\nThe ethical necessity to include a placebo arm in a clinical trial may differ depending on the type of drug being developed and whether other comparable drugs exist. For example, a placebo arm would be absolutely necessary in the testing of a new compound for which no drug has yet been developed. This would be combined with comparative arms involving other alternative drugs which have already been proven effective. For studies involving the development of a drug based on an existing compound, a comparative trial would be necessary but not necessarily with a placebo arm, or at least with a smaller placebo arm Nevertheless, the EMA emphasises the value of placebo-controlled trials in the development of new medicinal products even in cases where a proven effective drug exists:\n\u201cforbiddingplacebo-controlled trials in therapeutic areas where there are proven, therapeutic methods would preclude obtaining reliable scientific evidence for the evaluation of new medicinal products, and be contrary to public health interest as there is a need for both new products and alternatives to existing medicinal products.\u201d (EMA, 2001).\nIn 2001, concerns were raised about the interpretation of paragraph 29 of the 2000 version of the Helsinki Declaration in which prudence was called for in the use of placebo in research trials and it was advised that placebo should only be used in cases where there was no proven therapy for the condition under investigation. A document clarifying the position of the WMA regarding the use of placebo was issued by the WMA in 2001 in which it was made clear that the use of placebo might be ethically acceptable even if proven therapy was available. The current version of this statement is article 32 of the 2008 revised Helsinki Declaration (quoted in sub-section 7.2.1).\nThe PCSBI (2011) highlight the importance of ensuring that the design of clinical trials enables the researchers to resolve controversy and uncertainty over the merits of the trial drug and whether the trial drug is better than an existing drug if there is one. They suggest that studies which cannot resolve such questions or uncertainty are likely to be ignored by the scientific community and this would be unethical as it would mean that people had been unnecessarily exposed to risk without there being any social benefit.\nReasons for participation\nPeople with dementia who take part in clinical trials may do so for a variety of reasons. One possible reason is that they hope to receive some form of treatment that will improve their condition or even result in a cure. This is sometimes called the \u201ctherapeutic misconception\u201d. In such cases, clinical trials may seem unethical in that advantage is being taken of the vulnerability of some of the participants. On the other hand, the possibility of participating in such a trial may help foster hope which may even enable a person to maintain their morale.\nA review of 61 studies on attitudes to trials has shed some light on why people participate in clinical trials (Edwards, Lilford and Hewison, 1998). In this review, it was found that over 60% of participants in seven studies stated that they did or would participate in clinical trials for altruistic reasons. However, in 4 studies, over 70% of people stated that they participated out of self-interest and in two studies over 50% of people stated that they would participate in such a study out of self-interest. As far as informed consent is concerned, in two studies (which were also part of this review) 47% of responding doctors thought that few patients were actually aware that they were taking part in a clinical trial. On the other hand, an audit of four further studies revealed that at least 80% of participants felt that they had made an autonomous decision. There is no proof whether such perceptions were accurate or not. The authors conclude that self-interest was more common than altruism amongst the reasons given for participating in clinical trials but draw attention to the poor quality of some of the studies reviewed thereby suggesting the need for further research. It should not be necessary for people to justify why they are willing to participate in clinical trials. Reasons for participating in research are further discussed in section 3.2.4 insofar as they relate to end-of-life research.\nIn a series of focus groups organised in 8 European countries plus Israel and covering six conditions including dementia, helping others was seen as the main reason why people wanted to take part in clinical trials (Bartlam et al., 2010). In a US trial of anti-inflammatory medication in Alzheimer\u2019s disease in which 402 people were considered eligible, of the 359 who accepted, their main reasons for wanting to participate were altruism, personal benefit and family history of Alzheimer\u2019s disease.\nRandom assignment to study groups\nAs people are randomly assigned to the placebo or the active treatment group, everyone has an equal chance of receiving the active ingredient or whichever other control groups are included in the study. There are possible advantages and drawbacks to being in each group and people are likely to have preferences for being a particular study group but randomization means that allocation is not in any way linked to the best interests of each participant from a medical perspective. This is not an ethical issue provided that each participant fully understands that the purpose of research is not to provide a tailor-made response to an individual\u2019s medical condition and that while some participants benefit from participation, others do not.\nThere are, however, medical issues to consider. In the case in double-blind studies, neither the participant nor the investigator knows to which groups a participant has been allocated. Consequently, if a participant encounters medical problems during the study, it is not immediately known whether this is linked to the trial drug or another unrelated factor, but the problems must be addressed and possible contraindications avoided, which may necessitate \u201cde-blinding\u201d (DuBois, 2008).\nAlthough many people would perhaps like to benefit from a new drug which is more effective than existing drugs, people have different ideas about what is an acceptable risk and different reasons for taking part in clinical trials. People who receive the placebo are not exposed to the same potential risks as those given the experimental drug. On the other hand, they have no possibility to benefit from the advantages the drug may offer. Those receiving a drug commonly considered as the standard therapy are not necessarily better off than those receiving a placebo as some participants may already know that they do not respond well to the accepted treatment (DuBois, 2008).\nIf people who participate in a clinical trial are not informed which arm of the trial they were in, valuable information is lost which might have otherwise contributed towards to treatment decisions made after the clinical trial. Taylor and Wainwright (2005) suggest that \u201cunblinding\u201d should occur at the end of all studies and so as not to interfere with the analysis of data, this could be done by a person who is totally independent of the analysis. This would, however, have implications for open label extended trials as in that case participants, whilst better equipped to give informed consent would have more information than the researchers and this might be conveyed to researchers in anad hocmanner.\nOpen label extension trails\nOpen label extension studies (mentioned in sub-section 7.1.8) seem quite fair as they give each participant the opportunity to freely consent to continuing with the study in the full knowledge that s/he will receive the experimental drug. However, Taylor and Wainwright (2005) have highlighted a couple of ethical concerns linked to the consent process, the scientific value of such studies and issues linked to access to drugs at the end of the prior study.\nWith regard to consent, they argue that people may have had a positive or negative experience of the trial but do not know whether this was due to the experimental drug, another drug or a placebo. They may nevertheless base their decision whether to continue on their experience so far. For those who were not taking the experimental drug, their experience in the follow-up trial may turn out to be very different. Also, if they are told about the possibility of the open label extension trial when deciding whether or not to take part in the initial trial (i.e. with the implication that whatever group they are ascribed to, in the follow-up study they will be guaranteed the experimental drug), this might induce them to participate in the initial study which could be considered as a form of subtle coercion. Finally, researchers may be under pressure to recruit as they can only recruit people in an open label extended trial who took part in the initial study. This may lead them in turn to put pressure (even inadvertently) on participants to continue with the study.\nThe scientific validity of open label extension trials is questioned by Taylor and Wainwright (2005) on the grounds that people from the experimental arm of the first study who did not tolerate the drug would be unlikely to participate in the extension trial and this would lead to bias in the results. In addition, open-label trials often lack a precise duration other than \u201cuntil the drug is licensed\u201d which casts doubt on there being a valid research purpose.\nThe above authors suggest that open label extension studies are dressed up marketing activities which lack the ethical justification for biomedical research which is the prospect of finding new ways of benefiting people\u2019s health. However, it could be argued that the aim of assessing long-term tolerability of a new drug is a worthwhile pursuit and if conducted in a scientific manner could be considered as research. Moreover, not all open label extension trials are open-ended with regard to their duration. The main problem in interpreting open label extension studies is that little is known about the natural course of the disease.\nProtecting participants\u2019 well-being at the end of the clinical trial\nSome people who participate in a clinical trial and who receive the experimental drug experience an improvement in their condition. This is to be hoped even if benefit to the health of individuals is not the aim of the study. However, at the end of the study, the drug is not yet licenced and there is no legal right to continue taking it. This could be psychologically disturbing to the participants in the trial and also to their families who may have seen a marked improvement in their condition.\nTaylor and Wainwright (2005) suggest that the open label trials may serve the purpose of prescribing an unlicensed drug on compassionate grounds, which whilst laudable, should not be camouflaged as scientific research. Rather governments should take responsibility and set up the appropriate legal mechanisms to make it possible for participants whose medical condition merits prolonged treatment with the experimental drug to have access to it.\nMinimising pain and discomfort\nCertain procedures to which people with dementia or their representatives consent may by burdensome or painful or simply worrying but in accordance with the principles of autonomy or justice/equity, people with dementia have the right to participate. The fact that they have made an informed decision to participate and are willing to tolerate such pain or burden does not release researchers from the obligation to try to minimise it. For example, if repeated blood samples are going to be necessary, an indwelling catheter could be inserted under local anaesthetic to make it easier or medical staff should provide reassurance about the use of various scanning equipment which might be worrying or enable the person\u2019s carer to be present. In order to minimize fear, trained personnel are needed who have experience dealing with people with dementia. The advice of the carer, if there is one, could also be sought.\nDrug trials in countries with less developed safeguards\nClinical trials are sometimes carried out in countries where safeguards are not well developed and where the participants and even the general population are likely to have less possibility to benefit from the results of successful trials. For example, some countries have not signed the Convention for the Protection of Human Rights and Dignity of the Human Being with regard to the Application of Biology and Medicine (1997) (referred to in section 188.8.131.52). The participants in those countries may be exposed to possible risks but have little chance of future medical benefit if the trial is successful. Yet people in countries with stricter safeguards for participants (which are often richer countries) stand to benefit from their efforts and from the risks they take, as they are more likely to be able to afford the drugs once developed. This raises ethical issues linked to voluntariness because there may be, in addition to the less developed safeguards, factors which make participation in such trials more attractive to potential participants. Such practices also represent a lack of equity in the distribution of risk, burden and possible benefit within society and could be interpreted as using people as a means to an end.\nParallels can also be drawn to the situation whereby people in countries where stem cell research is banned profit from the results of studies carried out in countries where it is permitted or to the results of studies carried out in countries where research ethics are slack or inexistent.\nFor a detailed discussion of the ethical issues linked to the involvement in research of people in other countries, particularly lower and middle income countries where standards of protection may by lower, please refer to the afore-mentioned report by the Presidential Commission for the Study of Bioethical Issues.\n- Researchers should consider including a placebo arm in clinical trials when there are compelling and sound methodological reasons for doing so.\n- Researchers should ensure that patients are aware that the aim of a randomised controlled trial is to test a hypothesis and provide generalizable knowledge leading to the development of a medical drug or procedure. They should explain how this differs from medical treatment and care which are aimed at enhancing the health and wellbeing of individual patients and where there is a reasonable expectation that this will be successful.\n- Researchers should ensure that potential participants understand that they may be allocated to the placebo group.\n- It should not be presumed that the treating doctor or contact person having proposed the participant for a trial has been successful in communicating the above information.\n- Researchers conducting clinical trials may need training in how to ensure effective communication with people with dementia.\n- Appropriate measures should be taken by researchers to minimize fear, pain and discomfort of participants.\n- All participants should, when possible, preferably have the option of receiving the experimental drug (if proven safe) after completion of the study.\n- Pharmaceutical companies should not be discouraged from carrying out open-label extension studies but this should not be the sole possibility for participants to access the trial drug after the end of the study if it is proving beneficial to them.\n- In multi-centre clinical trials, where data is transferred to another country in which data protection laws are perhaps less severe, the data should be treated as stated in the consent form signed by the participant.\nLast Updated: jeudi 29 mars 2012", "id": "<urn:uuid:9d3f2101-f19c-4a5e-a7b4-dcd94a6d33f1>", "dump": "CC-MAIN-2013-20", "url": "http://www.alzheimer-europe.org/FR%20%20%20%20%20%20%20%20%20%20%20%20%EF%BF%BD%20%EF%BF%BD%C2%B3/Ethics/Ethical-issues-in-practice/Ethics-of-dementia-research/Clinical-trials", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.955619752407074, "token_count": 6489, "score": 3.640625, "int_score": 4}
{"text": "- weak drug regulatory control and enforcement;\n- scarcity and/or erratic supply of basic medicines;\n- unregulated markets and distribution chains;\n- high drug prices and/or\n- significant price differentials.\nAt national level, governments, law enforcement agencies, heath professionals, the pharmaceutical industry, importers, distributors, and consumer organizations should adopt a shared responsibility in the fight against counterfeit drugs. Cooperation between countries, especially trading partners is very useful for combating counterfeiting. Cooperation should include the timely and appropriate exchange of information and the harmonization of measures to prevent the spread of counterfeit medicines.\nThe World Health Organization has developed and published guidelines, Guidelines for the development of measures to combat counterfeit medicines. These guidelines provide advice on measures that should be taken by the various stakeholders and interested parties to combat counterfeiting of medicines. Governments and all stakeholders are encouraged to adapt or adopt these guidelines in their fight against counterfeiting of medicines.\n- Guidelines for the development of measures to combat counterfeit medicines\n- Rapid Alert System for counterfeit medicines\nCommunication and advocacy - creating public awareness\nPatients and consumers are the primary victims of counterfeit medicines. In order to protect them from the harmful effects of counterfeit medicines it is necessary to provide them with appropriate information and education on the consequences of counterfeit medicines.\nPatients and consumers expect to get advice from national authorities, health-care providers, health professionals and others from where they should buy or get their medicines; what measures they should take in case they come across such medicines or are affected by the use of such medicines.\nMinistries of health, national medicines regulators, health professional associations, nongovernmental organizations and other stakeholders have the responsibility to participate in campaign activities targeting patients and consumers to promote awareness of the problem of counterfeit medicines. Posters, brochures, radio and television programmes are useful means for disseminating messages and advice.", "id": "<urn:uuid:3ffdac17-ada1-42bf-b987-66bc26ca97f6>", "dump": "CC-MAIN-2013-20", "url": "http://who.int/impact/activities/en/", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9296919703483582, "token_count": 378, "score": 3.21875, "int_score": 3}
{"text": "Nursing a critically ill state back to health\n|Indranill Basu Ray highlights the core problems that afflict Bengal's health sector and suggests a few ways to improve the situation|\nDespite many technological and other achievements that have propelled India from being a developing nation to one of the top economies of the world, one field that India continues to lag behind in is health. This is why stories of babies dying in large numbers haunt newspaper headlines. India is behind Bangladesh and Sri Lanka in life expectancy at birth or under-five mortality level. India accounts for about 17 per cent of the world population, but it contribute to a fifth of the world's share of diseases. A third of all diarrhoeal diseases in the world occurs in India. The country has the second largest number of HIV/AIDS cases after South Africa. It is home to one-fifth of the world's population afflicted with diabetes and cardiovascular diseases.\nA common excuse that I often hear is that we have limited resources to tackle the huge and burgeoning health problems. But even the richest country on earth, the United States of America, has failed to provide appropriate health services to a large section of the populace. The problem in India is quite different. Apart from being a poor nation with limited resources, it also has a sizeable population in need of basic health services. Furthermore, the lack of appropriate sanitary measures and education ensures an ever increasing presence of communicable disease that have been controlled and even eradicated in the developed nations.\nIndia's list of woes does not stop here. Lack of foresight on the part of successive governments and selective and fragmented strategies to counter daily problems without a definite public health goal have been the mainstay of India's health policies. Resource allocation to this sector is influenced by the prevailing fiscal situation as well as by the priorities of the reigning government. Unfortunately, in Bengal \u2014 a state that faces a dismal fiscal situation \u2014 the government's priorities have been skewed as a result of political necessities. Although we have a new government at the helm, it is important to realize that gross changes at the practical level cannot be initiated without having a team with experience and knowledge define a well-thought-out strategy. It is also essential to have a government that is willing to fulfil the financial needs necessary for the strategy to work.\nIt is difficult, if not impossible, to paint a picture of the present state of public health in West Bengal and to suggest measures to rectify the same in a short article like this. My intention is to highlight the core problems plaguing the system and to suggest solutions based on accepted principles of public health and healthcare management. The steps that need to be taken are as follows: reducing disease burden, including infectious diseases as well as non-communicable epidemics like diabetes mellitus and coronary heart disease; restructuring the existing primary healthcare system to make it more accountable; creating a skilled and professional workforce which is quality driven; financial planning to bring more investment to the health sector.\nReducing disease burden is the cornerstone of any good health policy. The factors that help reduce communicable diseases are clean drinking water, improved sanitation and an effective vaccination programme. A paradigm shift, from the prevalent curative approach to a preventive approach, including health promotion by inculcating behavioural changes, is imperative to reduce disease burden. West Bengal is one of four states that urgently needs high investment in safe drinking water and toilet facilities. It is estimated that Rs 18,000 crore is required to provide effective drinking water and sanitation facilities for the entire country. Kerala, Maharashtra, West Bengal and Odisha would account for more than 60 per cent of the total outlay.\nSimilarly, a huge investment is required to provide nutritional supplements to malnourished children and pregnant and lactating mothers living below the poverty line. According to a report by the national commission on macroeconomics and health, West Bengal would need to harness an additional resource requirement of rupees (in crore) 1,286, 2,459, 4,693, 13,811 and 8,485 in sectors such as health, water and sanitation, nutrition, primary schooling and roads. It has been projected that in the next five years West Bengal will spend a large portion of its revenues on wages and salaries, interest payments and pensions, leaving very little for discretionary expenditure in the field of health. It is imperative that the present government rethink and strategize in collaboration with the Centre to ensure the appropriate funding necessary to make the state healthy.\nRestructuring the present healthcare delivery system is also equally important. Most primary healthcare centres are old, dilapidated buildings with few or no facilities. Some do not even have basic resources like healthcare workers or pharmacists. What is required is a radical overhaul of the existing system. There are differences in health systems of different countries. A State-run health system, such as the one in Canada, suffers from delayed medical care. A privately-run health system like the one in the US provides only limited health services to its poor. India's healthcare should carve out the best of both systems. Private healthcare is thriving in India. It is uncontrolled and aimed at profit-making. Government-run hospitals are poorly managed, providing few or no facilities to those living below the poverty line.\nDifferent models have been suggested to take care of this disparity. While private investment will always be geared towards profit-making, it is mandatory to rein in these bodies under well-defined rules. Large private hospitals in the US are non-profit bodies, which have to follow stringent rules in patient care. At the other end of the spectrum is the National Health Service in Britain in which small, medium and even a few large hospitals are making way for a more competent and accountable government-controlled health system with fewer hospitals.\nHuman resource management is very important in running an effective health system. One of the biggest lacunae of government health service is its poor human-resource management. Many physicians are not paid appropriate salaries or are posted in places that are not of their choice. Political intervention and favouritism play a big role in posting physicians. Consequently, dedicated physicians who want to serve the public or work in the academic setting found in government hospitals are forced to remain in private hospitals. To boost morale and efficacy, discipline needs to be instituted in the system and a transparent posting policy adopted. The doctor-population ratio needs to be improved by filling up vacancies in the West Bengal health service. It is important to free postings from the grip of bureaucrats to ensure the registration of quality candidates. Physicians failing to report to duty or indulging in indiscipline must be punished. Doctors who do sign up need to provide relevant and quality medical care. This can only be done if some form of recertification of doctors is made mandatory once every 10 years. Physicians' salaries in the state health service must be made on a par with those of the Central government to make sure that it remains a lucrative option. Senior physicians providing exemplary public service must be rewarded for the same. A commonly-held notion is that most physicians run after the lucrative salaries that are offered in private hospitals. Hence it is difficult to retain them in the government sector. This, however, is true of a minority. The majority of physicians are willing to work in a healthy, progressive and academic environment if there are appropriate non-financial incentives. Let us take the example of Christian Medical College, Vellore. Most of the faculty there are paid salaries that are much lower than those of the private sector. However, physicians are provided with other facilities such as good housing, free schools, free-to-highly-subsidized college education and, most importantly, a progressive and research oriented work environment.\nWest Bengal lags behind many other states when it comes to medical education. There is an urgent need to increase the number of medical colleges in the state. Private investment for the same should be welcomed but appropriate laws must be instituted so that huge capitation fees are not charged for seats. Furthermore, selection should be made through competitive examinations. A certain percentage of seats can be reserved for the economically weaker sections. Students passing out of such medical colleges must be given postings in rural hospitals. This has been true on paper for many decades now, but the rule has been poorly implemented even in government-run medical colleges.\nInnovative schemes ought to be thought of to involve the cash-rich private sector to service the medical needs of the state. Private institutions using government money or land must be asked to provide free service to 20 per cent of their capacity. Appropriate punitive measures \u2014 such as temporarily withholding or cancelling licences \u2014 can be taken when a private institution fails to honour this commitment. Institutions willing to set up large hospitals, particularly around Calcutta, must be helped through the provision of low-cost land. But in return, promises to set up satellite hospitals in far-flung district headquarters have to be met.\nThe biggest challenge to the rejuvenation of the healthcare system is the garnering of funds. West Bengal is financially broke, thanks to the misrule of the communists. Unlike most other communist rulers, our home-grown variants failed to provide basic sanitation, good roads, a working healthcare system and appropriate nutritional supplements to women and children. The lack of social services resulted in poor health and in increased mortality among the vulnerable sections of society. Government efforts to improve basic health services must fund programmes that provide sanitation, nutritional supplements, and daily meals for school-going children. Substantial investments in these sectors can reduce mortality in children. It is popular to blame doctors for not being able to save severely ill, malnourished children. But things won't change unless determined steps are taken to root out the problems, such as poor funds, minimal resources and an incompetent workforce, that affect the West Bengal health service.\nIn the next five years, in collaboration with the Centre and the non-government organizations involved in public health, the state government must chalk out a definitive strategy to improve the supply of clean drinking water, provide better sanitation and one full meal to school-going children and arrange for nutritional supplements to pregnant women. Private investment should be wooed in the health sector to set up hospitals in large metropolitan areas as well as in small district towns. While government land is needed at an appropriate price to help investors build hospitals, steps must be taken to bring about the inclusion of the deprived sections in their service plans. Strong regulatory bodies that can monitor private hospitals and nursing homes must be instituted. Many of the profiteering health institutions do not provide basic facilities, lack trained nurses and paramedical staff, and some are even run by quacks without medical degrees. It is of utmost importance that a regulatory body conducts surprise checks on these institutions, registers complaints and takes remedial steps.\nMany NGOs have been able to set up large projects benefiting thousands of people. They have also succeeded in bringing foreign aid to tackle malaria and HIV. The state government should help these NGOs achieve their goals while exercising control to prevent financial irregularities. Their services ought to be applauded and single-window processing of applications instituted to help them tackle bureaucratic delays. Health is a service industry and not a lucrative business. Unfortunately, in Bengal, most large hospitals are owned by corporates. Only a few are owned or run by doctors. There is thus a sustained effort to make profit. Poor consumer protection makes the man on the street vulnerable to substandard service at high prices.\nThese are trying times for Bengal, after years of mismanagement in the health sector. It is important for the present rulers to rectify the situation by laying down the stepping stones for a better tomorrow.\nTuesday, November 22, 2011\nNursing a critically ill state back to health Indranill Basu Ray highlights the core problems that afflict Bengal\u2019s health sector and suggests a few ways to improve the situation", "id": "<urn:uuid:a51737a0-6a1a-4721-a739-791f50bfecba>", "dump": "CC-MAIN-2013-20", "url": "http://basantipurtimes.blogspot.com/2011/11/nursing-critically-ill-state-back-to.html", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9593021273612976, "token_count": 2399, "score": 2.703125, "int_score": 3}
{"text": "Hospitals across the country are diligently working to reduce infection rates. According to the World Health Organization, hospital-acquired infections affect as many as 1.7 million patients in the United States each year. These infections come at an annual cost of $6.5 billion and contribute to more than 90,000 deaths.\nProper hand hygiene is essential in helping to prevent hospital-acquired infections. A recent study performed by French researchers examined three types of healthcare workers. The first type spent a large amount of time with a discreet group of patients like a nurse would. The second group saw more patients but spent less time with each one - similar to doctors. Group three consisted of healthcare workers who interacted with every patient every day like therapists. The study found that if a healthcare worker in group three failed to wash their hands, the spread of disease was three times worse than if someone from group one or two didn't. The study was published online in Proceedings of the National Academy of Sciences. To read more about the study, continue here.\nTo read another take on hand hygiene and about the Joint Commission's national hand hygiene project, click here.\nPhoto Credit: Jessica Flavin\nAlmost two million patients hospitalized in the U.S. each year develop an infection. These infections occur in as many as one in every 10 patients, result in close to 100,000 deaths and cost upwards of $6 billion. The Wall Street Journal created a top 10 list of infection prevention strategies based on interviews with medical professionals, administrators a non profit company and the Association for Professionals in Infection Control and Epidemiology.\n- Undercover Operations - Dr. Philip Carling, an epidemiologist at Caritas Carney Hospital in Dorchester, Mass. developed a solution to uncover how well patient rooms are cleaned. His invisible solution contains fluorescent markers which glow in black light. After spraying patient rooms with the solution, cleaning crews were brought in to perform their normal routine. Later, rooms were examined with a black light and areas missed by the cleaners glowed fluorescent. Sharing results with cleaners helped boost compliance with proper cleaning techniques.\n- High-Tech Cleaning Systems - When hospital equipment is disinfected by hand, bacteria often remains. For more thorough disinfecting hospitals are utilizing machines such as Bioquell which sprays a disinfecting hydrogen-peroxide vapor.\n- Data Mining - Many hospitals are tracking data to determine how to prevent infections. Lee Memorial Health System in Florida tracks infection rates by surgeon and reports on the results. Low ranking surgeons can then make adjustments to lower their infection rates and improve their ranking.\n- Patient Hygiene - Research suggests a daily wash with mild antibacterial soap can dramatically reduce the rate of bloodstream infections. The recommended cleanser is chlorohexidine glutonate.\n- Reporting Crackdown - Numerous states have passed laws which require hospitals to report on infection rates. In many cases the reports are publicly available. In addition, Medicare is limiting reimbursement for treatment of hospital-acquired infections.\n- Clean hands - Hospitals that utilize strategically-placed dispensers of hand sanitizer have noticed an increase in hand hygiene compliance from less than 50% to more than 80%.\n- Embracing the Checklist - Incorporating checklists into bedside medical charts can help reduce rates of infection by requiring shift nurses to answer questions such as: Does this patient have a catheter? If so, is it still necessary?\n- Portable Kits - Utilizing all-inclusive kits for common procedures such as intravenous line insertions or dressing changes can limit the possibility for infection. Kits contain all the items needed for procedures and prevent the nurse from running in and out of the patient room during a procedure to find a forgotten item.\n- Mouth Maintenance - Regularly cleaning patients' mouths, gums and teeth can help prevent ventilator-associated pneumonia, a common infection found in intensive care units.\n- Infection ID - Quick diagnostic tests can identify infected patients in a matter of hours rather than days. This allows for a quick response when patients show symptoms, are tested and found to be infected.\nTo read the complete article with expanded descriptions of the top 10, click here.\nPhoto Credit: Presta\nHospitals in Michigan lowered the rate of bloodstream infections in their patients by following a five-step checklist. The study published in the New England Journal of Medicine\nfound that implementing the checklist reduced the rate of bloodstream infections related to catheter use by 66%. Despite this success, utilization of the checklist remains limited. The checklist itself isn't complicated:\n- Wash hands\n- Clean patient's skin with chlorohexidine\n- Wear protective cap and gown and use a surgical drape during the procedure\n- Avoid catheter insertion through the groin if possible\n- Remove unnecessary catheters\nPeter Pronovost, the patient-safety expert who led the study, spoke with The Wall Street Journal to share insights on why more hospitals haven't benefited from using the checklist. To read excerpts from his interview, click here.\nPhoto Credit: Adesigna\nA recent study published in the American Journal of Infection Control examined the levels of bacteria on healthcare workers' lab coats. The study involved a cross section of medical and surgical grand rounds attendees at a large teaching hospital. Participants completed a survey and cultured their lab coat using a moistened swab on the lapels, pocket and cuffs. Of the 149 white coats in the study, 34 (23%) were contaminated with S aureus, of which 6 (18%) were methicillin-resistant S aureus (MRSA). Providers working with patients had higher contamination levels and the study suggests that white coats may contribute to patient-to-patient transmission of S aureus. Read the entire study in the March 2009 issue of the American Journal of Infection Control, the official journal of the Association for Professionals in Infection Control and Epidemiology (APIC).\nPhoto Credit: Estherase\nCentral venous catheters (CVC) are essential for treating children with cancer. They reduce the need for multiple needlesticks and the associated pain and anxiety. In addition, they can be used to deliver chemotherapy, parenteral fluids, blood products and analgesics. Despite the positives, children with CVCs are at increased risk for bloodstream infections. Complications associated with CVCs include pneumothorax, air embolism, nerve injury, catheter malposition, infection and occlusion.\nA recent study had four objectives:\n1. To decrease CVC-related bloodstream infection rates in children with cancer through a comprehensive educational intervention.\n2. To determine if the frequency of catheter hub colonization of CVCs in children with cancer would decrease following the educational intervention.\n3. To evaluate nurses' knowledge of CVC care.\n4. To determine risk factors influencing CVC-related bloodstream infections in children with cancer.\nThe study was conducted in the cancer center of a large children's hospital and included patients ranging in age from infancy to 18 years. A 45 minute educational program on CDC guidelines, most frequent guideline violations and information on catheter-related infections was presented to all caregivers. Following the educational presentation, catheter-related bloodstream infections were tracked for six months in order to determine the rate of infection. Study findings showed that the educational program increased nurses' knowledge and instances of catheter-related bloodstream infections decreased. You can read the full article in the March 2009 issue of Oncology Nursing Forum or purchase it online here.\nPhoto Credit: Gulf Coast Regional Blood Center\nAccording to a 2009 study, approximately 5 million central venous catheters are placed each year. Implantable ports provide reliable venous, arterial, epidural and peritoneal access and can be used to administer IV fluids, medications and to obtain blood samples. However complications including occlusion, infection, catheter migration and catheter separation from portal body can frequently occur.\nA recent study conducted in a rural hematology-oncology clinic focused on infection. A port infection can present as local tenderness, pain, erythema, induration or edema at the insertion or exit site or over the port pocket. Patients may also have purulent or serous drainage, fever and chills. To prevent infection, aseptic technique should be utilized for dressing changes. In addition, clinicians should follow accessing and deaccessing procedures and keep the exit clear of potential sources of infection. The 62 patients included in the study were receiving a minimum of two complete cycles of chemotherapy after port insertion. Ports were accessed and deaccessed following outlined protocol.\n*Steps for Accessing Ports:\n- Wash hands. Assess the port site for erythema, warmth or drainage.\n- Palpate the outline of the portal body.\n- Wash hands.\n- Apply nonsterile gloves. Cleanse port site with chlorohexidine swab in a circular motion for 30 seconds. Allow to dry for 30 seconds.\n- Spray ethyl chloride.\n- Stabilize portal body with one hand. Insert Huber needle (link to EZ Huber product page) into septum with other hand. Ensure patency by blood return. If no blood return, use interventions to assess port's patency.\n- Stabilize port with gauze and tape or apply transparent dressing.\n*Steps for Deaccessing Ports:\n- Wash hands. Apply nonsterile gloves.\n- Inspect exit site.\n- Flush device with 20 ml normal saline followed by 5 ml heparin flush (100 units/ml). During final flush, clamp tubing to port.\n- Stabilize port and remove needle.\n- Apply bandage.\nSix of the 62 patients in the study experienced a port infection, with four of the six ports requiring removal. The total number of catheter days for the implanted ports was 7,277. Patient catheter days ranged from 32-288. The study concluded that consistent, routine care is the best preventative measure against port complications. The entire study can be found in the October 2009 issue of the Clinical Journal of Oncology Nursing.\n*The port access and de-access protocols are those that were used by the authors for this study. Please follow institutional policies and procedures regarding port access and de-access.\nAlthough many infection headlines are related to hospitals, individual doctor's offices are facing similar challenges. Almost 30 cases of hepatitis B were recently tied to one doctor's office in New Jersey. When health inspectors visited the office they found blood on the floor of a room where chemotherapy was administered, blood in a bin where blood vials were stored, unsterile saline and gauze as well as open medication vials. Inspectors also noticed cross-contamination of pens, refrigerators and countertops, use of contaminated gloves and misuse of antiseptics.\nPatients were sent a letter from state epidemiologist Dr. Christina Chan urging testing for hepatitis B. \"Evidence gathered at this time suggests that since 2002, some clinic staff provided care in a manner that puts patients at risk for infection caused by bloodborne viruses, including hepatitis B,\" the letter told patients. \"The investigation to date suggests that hepatitis B infections identified may be associated with the method by which medications were administered and procedures performed at the practice.\"\nNumerous checklists and recommendations have been published around infection control. The American Academy of Pediatrics Committee on Infectious Diseases and Committee on Practice and Ambulatory Medicine offers these infection control musts:\n- Hand washing\n- Barrier precautions to prevent skin and mucous membrane exposure\n- Proper handling of sharps and contaminated waste\n- Appropriate cleaning and disinfecting of surfaces and equipment\n- Aseptic technique for invasive procedures\nFor the full recommendation on infection control in physician's offices, click here.\nTo read more about the hepatitis B outbreak in New Jersey, continue reading here.\nPhoto Credit: Hollywood Pimp\nThe Joint Commission Center for Transforming Healthcare is working on its first improvement venture: The Hand Hygiene Project. According to the Centers for Disease Control and Prevention, an estimated 2 million patients get a hospital-related infection every year and 90,000 die from their infection.\nCauses of Failure to Clean Hands\n- Ineffective placement of dispensers or sinks\n- Hand hygiene compliance data are not collected or reported accurately or frequently\n- Lack of accountability and just-in-time coaching\n- Safety culture does not stress hand hygiene at all levels\n- Ineffective or insufficient education\n- Hands full\n- Wearing gloves interferes with process\n- Perception that hand hygiene is not needed if wearing gloves\n- Healthcare workers forget\nEarly results of the program found on average that caregivers washed their hands less than 50 percent of the time. \"Demanding that healthcare workers try harder is not the answer. These healthcare organizations have the courage to step forward to tackle the problem of hand washing by digging deep to find out where the breakdowns take place so we can create targeted solutions that will work now and keep working in the future,\" said Mark R. Chassin, M.D., M.P.P, M.P.H., president, The Joint Commission.\nBy January, 2010, the Joint Commission Center for Transforming Healthcare plans to have data to demonstrate whether the proposed hand hygiene solutions can be sustained to achieve a 90+ percent compliance rate.\nEight hospitals are participating in this project:\n- Cedars-Sinai Health System, Los Angeles, California\n- Exempla Lutheran Medical Center, Wheat Ridge, Colorado\n- Froedtert Hospital, Milwaukee, Wisconsin\n- The Johns Hopkins Hospital and Health System, Baltimore, Maryland\n- Memorial Hermann Health Care System, Houston, Texas\n- Trinity Health, Novi, Michigan\n- Virtua, Marlton, New Jersey\n- Wake Forest University Baptist Medical Center, Winston-Salem, North Carolina\nTo read the full release from the Joint Commission for Transforming Healthcare, click here.\nPhoto Credit: Mag3737\nHealthcare providers are on alert due to an increase in a new strain of hospital-acquired infections. A recent study released by Arlington Medical Resources (AMR) and Decision Resources, found that recurrent Clostridium difficile\nis difficult to treat in a hospital setting.\nClostridium difficile is a bacterium that can cause symptoms as minor as diarrhea and as life threatening as severe inflammation of the colon. The elderly are most at risk and the Centers for Medicare and Medicaid services is considering adding Clostridium difficile to its list of \"never events\" or preventable hospital-acquired infections. Hospitals will receive reduced or no Medicare payments for infections on the \"never events\" list.\nRead more about how the study was conducted as well as more information on Clostridium difficile here.\nPhoto Credit: Big Grey Mare\nJeanne Hahne was working as a nurse in a burn ward when inspiration struck. Because the patients were so vulnerable to infection, Hahne and other healthcare providers had to wear full protective gear including a cap to cover her hair and a mask that covered the majority of her face. Even though she worked with many of the burn patients every day, most couldn't recognize her.\nFlash forward almost 30 years and Hahne has designed a face mask made of clear plastic so patients can see her smile. Hahne believes she can reassure patients with a smile and help decrease their anxiety. The masks also have utility for patients and healthcare providers with hearing loss since they allow for lip reading. In addition, the masks have helped improve communication between healthcare workers which can help decrease the chance for mistakes or misunderstanding. To read more and see pictures of the face mask, click here.\nPhoto Credit: Christiana Care", "id": "<urn:uuid:c2edfd5f-7efa-4718-90fc-2492422d24c1>", "dump": "CC-MAIN-2013-20", "url": "http://info.navilystmedical.com/Blog/?Tag=infection", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9321461319923401, "token_count": 3248, "score": 3.140625, "int_score": 3}
{"text": "Wars have given us the Jeep, the computer and even the microwave.\nWill the war in Iraq give us the Tiger?\nMilitary scientists at Edgewood Chemical Biological Center at Aberdeen Proving Ground hope so. The machine - its full name is the Tactical Garbage to Energy Refinery - combines a chute, an engine, chemical tanks and other components, giving it the appearance of a lunar rover. It's designed to turn food and waste into fuel. If it works, it could save scores of American and Iraqi lives.\nAmong the biggest threats that soldiers face in the war in Iraq are the roadside bombs that have killed or maimed thousands since the U.S.-led invasion in 2003. Because some military bases lack a landfill, transporting garbage to dumps miles away in the desert has become a potentially fatal routine for U.S. troops and military contractors.\nThe Tiger would attempt to solve two problems at once: It would sharply reduce those trash hauls and provide the military with an alternative source of fuel.\nIt is the latest in a long line of wartime innovations, from can openers to desert boots. The conflict in Iraq has produced innovations such as \"warlocks,\" which jam electronic signals from cell phones, garage door openers and other electronic devices that insurgents use to detonate roadside bombs, according to Inventors Digest.\n\"In wartime, you're not worried about making a profit necessarily. You're worried about getting the latest technology on the street,\" said Peter Kindsvatter, a military historian at Aberdeen Proving Ground, who added that money is spent more freely for research when a nation is at war. \"Basically, you find yourself in a technology race with your enemy.\"\nThe Tiger, now being tested in Baghdad, would not be the first device to turn garbage into energy - a large incinerator near Baltimore's downtown stadiums does it. But it would be among the first to attempt to do it on a small scale. Its creators say it could one day become widely used in civilian life, following the lead of other wartime innovations.\nDuring World War II, contractors developed the Jeep to meet the military's desire for a light, all-purpose vehicle that could transport supplies.\nThe development of radar technology to spot Nazi planes led to the microwave, according to historians.\nThe World War II era also gave birth to the first electronic digital computer, the Electronic Numerical Integrator and Computer, or ENIAC. Funded by the Defense Department, the machine was built to compute ballistics tables that soldiers used to mechanically aim large guns. For years it was located at Aberdeen Proving Ground.\nThis decade, the Pentagon determined that garbage on military bases poses a serious logistical problem.\n\"When you're over in a combat area and people are shooting at you, you still have to deal with your trash,\" said John Spiller, project officer with the Army's Rapid Equipping Force, which is funding the Tiger project. \"How would you feel if somebody was shooting at you every other time you pushed it down the curb?\"\nHe and other Army officials said they could not recall any specific attacks against troops or contractors heading to dumpsites.For years, large incinerators have burned trash to generate power. Baltimore Refuse Energy Systems Co., the waste-to-energy plant near the stadiums, consumes up to 2,250 tons of refuse a day while producing steam and electricity.\nThe process is so expensive that it has only made sense to do it on a large scale, scientists say.\nThe military has spent almost $3 million on two Tiger prototypes, each weighing nearly 5 tons and small enough to fit into a 20- to 40-foot wide container. The project is being developed by scientists from the Edgewood, Va.-based Defense Life Sciences LLC and Indiana's Purdue University.\nThe biggest challenge was getting the parts to work together, said Donald Kennedy, an Edgewood spokesman. Because the Tiger is a hybrid consisting of a gasifier, bioreactor and generator, much of it is built with off-the-shelf items, including a grinder.\nAnother big challenge: expectations.\n\"When we would initially talk to people about the Tiger system, a large percentage would refuse to believe it could actually work,\" Kennedy wrote in an e-mail. \"Alternatively, a similar percentage would be so intrigued by the idea that they would demand to know when they could buy one for their neighborhood.\"\nThe Tiger works like this: A shredder rips up waste and soaks it in water. A bioreactor metabolizes the sludge into ethanol. A pelletizer compresses undigested waste into pellets that are fed into a gasification unit, which produces composite gas.\nThe ethanol, composite gas and a 10-percent diesel drip are injected into a diesel generator to produce electricity, according to scientists. It takes about six hours for the Tiger to power up. When it works, the device can power a 60-kilowatt generator.\nThe prototypes are being tested at Camp Victory in Baghdad\nInitial runs proved successful. The prototypes have been used to power an office trailer. At their peak, they could power two to three trailers.\nIn recent weeks, the scientists suffered a setback: The above-100 degree temperatures caused a chiller device to overheat and shut off occasionally. A new chiller from Edgewood just arrived at the site, Kennedy said.\nAfter the 90-day testing phase that ends Aug. 10, the Army will decide whether to fund the project further.\nIts developers envision the device being used to respond to crises such as Hurricane Katrina, when there is no lack of garbage but a great need for electricity.\nSpiller, of the Army's Rapid Equipping Force, said he is optimistic.\n\"The mere fact we wrote a check means we think it's got a high chance of success,\" Spiller said.", "id": "<urn:uuid:749f4f2e-01bf-42ab-ac03-1dfa84af34dc>", "dump": "CC-MAIN-2013-20", "url": "http://articles.baltimoresun.com/2008-07-21/news/0807200131_1_garbage-aberdeen-proving-ground-war-in-iraq", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9598866701126099, "token_count": 1205, "score": 2.828125, "int_score": 3}
{"text": "How We Found the Missing Memristor\nThe memristor--the functional equivalent of a synapse--could revolutionize circuit design\nImage: Bryan Christie Design\nTHINKING MACHINE This artist's conception of a memristor shows a stack of multiple crossbar arrays, the fundamental structure of R. Stanley Williams's device. Because memristors behave functionally like synapses, replacing a few transistors in a circuit with memristors could lead to analog circuits that can think like a human brain.\nIt\u2019s time to stop shrinking. Moore\u2019s Law, the semiconductor industry\u2019s obsession with the shrinking of transistors and their commensurate steady doubling on a chip about every two years, has been the source of a 50-year technical and economic revolution. Whether this scaling paradigm lasts for five more years or 15, it will eventually come to an end. The emphasis in electronics design will have to shift to devices that are not just increasingly infinitesimal but increasingly capable.\nEarlier this year, I and my colleagues at Hewlett-Packard Labs, in Palo Alto, Calif., surprised the electronics community with a fascinating candidate for such a device: the memristor. It had been theorized nearly 40 years ago, but because no one had managed to build one, it had long since become an esoteric curiosity. That all changed on 1 May, when my group published the details of the memristor in Nature.\nCombined with transistors in a hybrid chip, memristors could radically improve the performance of digital circuits without shrinking transistors. Using transistors more efficiently could in turn give us another decade, at least, of Moore\u2019s Law performance improvement, without requiring the costly and increasingly difficult doublings of transistor density on chips. In the end, memristors might even become the cornerstone of new analog circuits that compute using an architecture much like that of the brain.\nFor nearly 150 years, the known fundamental passive circuit elements were limited to the capacitor (discovered in 1745), the resistor (1827), and the inductor (1831). Then, in a brilliant but underappreciated 1971 paper, Leon Chua, a professor of electrical engineering at the University of California, Berkeley, predicted the existence of a fourth fundamental device, which he called a memristor. He proved that memristor behavior could not be duplicated by any circuit built using only the other three elements, which is why the memristor is truly fundamental.\nMemristor is a contraction of \u201dmemory resistor,\u201d because that is exactly its function: to remember its history. A memristor is a two-terminal device whose resistance depends on the magnitude and polarity of the voltage applied to it and the length of time that voltage has been applied. When you turn off the voltage, the memristor remembers its most recent resistance until the next time you turn it on, whether that happens a day later or a year later.\nThink of a resistor as a pipe through which water flows. The water is electric charge. The resistor\u2019s obstruction of the flow of charge is comparable to the diameter of the pipe: the narrower the pipe, the greater the resistance. For the history of circuit design, resistors have had a fixed pipe diameter. But a memristor is a pipe that changes diameter with the amount and direction of water that flows through it. If water flows through this pipe in one direction, it expands (becoming less resistive). But send the water in the opposite direction and the pipe shrinks (becoming more resistive). Further, the memristor remembers its diameter when water last went through. Turn off the flow and the diameter of the pipe \u201dfreezes\u201d until the water is turned back on.\nThat freezing property suits memristors brilliantly for computer memory. The ability to indefinitely store resistance values means that a memristor can be used as a nonvolatile memory. That might not sound like very much, but go ahead and pop the battery out of your laptop, right now\u2014no saving, no quitting, nothing. You\u2019d lose your work, of course. But if your laptop were built using a memory based on memristors, when you popped the battery back in, your screen would return to life with everything exactly as you left it: no lengthy reboot, no half-dozen auto-recovered files.\nBut the memristor\u2019s potential goes far beyond instant-on computers to embrace one of the grandest technology challenges: mimicking the functions of a brain. Within a decade, memristors could let us emulate, instead of merely simulate, networks of neurons and synapses. Many research groups have been working toward a brain in silico: IBM\u2019s Blue Brain project, Howard Hughes Medical Institute\u2019s Janelia Farm, and Harvard\u2019s Center for Brain Science are just three. However, even a mouse brain simulation in real time involves solving an astronomical number of coupled partial differential equations. A digital computer capable of coping with this staggering workload would need to be the size of a small city, and powering it would require several dedicated nuclear power plants.\nMemristors can be made extremely small, and they function like synapses. Using them, we will be able to build analog electronic circuits that could fit in a shoebox and function according to the same physical principles as a brain.\nA hybrid circuit\u2014containing many connected memristors and transistors\u2014could help us research actual brain function and disorders. Such a circuit might even lead to machines that can recognize patterns the way humans can, in those critical ways computers can\u2019t\u2014for example, picking a particular face out of a crowd even if it has changed significantly since our last memory of it.\nThe story of the memristor is truly one for the history books. When Leon Chua, now an IEEE Fellow, wrote his seminal paper predicting the memristor, he was a newly minted and rapidly rising professor at UC Berkeley. Chua had been fighting for years against what he considered the arbitrary restriction of electronic circuit theory to linear systems. He was convinced that nonlinear electronics had much more potential than the linear circuits that dominate electronics technology to this day.\nChua discovered a missing link in the pairwise mathematical equations that relate the four circuit quantities\u2014charge, current, voltage, and magnetic flux\u2014to one another. These can be related in six ways. Two are connected through the basic physical laws of electricity and magnetism, and three are related by the known circuit elements: resistors connect voltage and current, inductors connect flux and current, and capacitors connect voltage and charge. But one equation is missing from this group: the relationship between charge moving through a circuit and the magnetic flux surrounded by that circuit\u2014or more subtly, a mathematical doppelg\u00e4nger defined by Faraday\u2019s Law as the time integral of the voltage across the circuit. This distinction is the crux of a raging Internet debate about the legitimacy of our memristor [see sidebar, \u201dResistance to Memristance \u201d].\nChua\u2019s memristor was a purely mathematical construct that had more than one physical realization. What does that mean? Consider a battery and a transformer. Both provide identical voltages\u2014for example, 12 volts of direct current\u2014but they do so by entirely different mechanisms: the battery by a chemical reaction going on inside the cell and the transformer by taking a 110\u00e2\u00bf\u00bfV ac input, stepping that down to 12 V ac, and then transforming that into 12 V dc. The end result is mathematically identical\u2014both will run an electric shaver or a cellphone, but the physical source of that 12 V is completely different.\nConceptually, it was easy to grasp how electric charge could couple to magnetic flux, but there was no obvious physical interaction between charge and the integral over the voltage.\nChua demonstrated mathematically that his hypothetical device would provide a relationship between flux and charge similar to what a nonlinear resistor provides between voltage and current. In practice, that would mean the device\u2019s resistance would vary according to the amount of charge that passed through it. And it would remember that resistance value even after the current was turned off.\nHe also noticed something else\u2014that this behavior reminded him of the way synapses function in a brain.\nEven before Chua had his eureka moment, however, many researchers were reporting what they called \u201danomalous\u201d current-voltage behavior in the micrometer-scale devices they had built out of unconventional materials, like polymers and metal oxides. But the idiosyncrasies were usually ascribed to some mystery electrochemical reaction, electrical breakdown, or other spurious phenomenon attributed to the high voltages that researchers were applying to their devices.\nAs it turns out, a great many of these reports were unrecognized examples of memristance. After Chua theorized the memristor out of the mathematical ether, it took another 35 years for us to intentionally build the device at HP Labs, and we only really understood the device about two years ago. So what took us so long?\nIt\u2019s all about scale. We now know that memristance is an intrinsic property of any electronic circuit. Its existence could have been deduced by Gustav Kirchhoff or by James Clerk Maxwell, if either had considered nonlinear circuits in the 1800s. But the scales at which electronic devices have been built for most of the past two centuries have prevented experimental observation of the effect. It turns out that the influence of memristance obeys an inverse square law: memristance is a million times as important at the nanometer scale as it is at the micrometer scale, and it\u2019s essentially unobservable at the millimeter scale and larger. As we build smaller and smaller devices, memristance is becoming more noticeable and in some cases dominant. That\u2019s what accounts for all those strange results researchers have described. Memristance has been hidden in plain sight all along. But in spite of all the clues, our finding the memristor was completely serendipitous.\nIn 1995, I was recruited to HP Labs to start up a fundamental research group that had been proposed by David Packard. He decided that the company had become large enough to dedicate a research group to long-term projects that would be protected from the immediate needs of the business units. Packard had an altruistic vision that HP should \u201dreturn knowledge to the well of fundamental science from which HP had been withdrawing for so long.\u201d At the same time, he understood that long-term research could be the strategic basis for technologies and inventions that would directly benefit HP in the future. HP gave me a budget and four researchers. But beyond the comment that \u201dmolecular-scale electronics\u201d would be interesting and that we should try to have something useful in about 10 years, I was given carte blanche to pursue any topic we wanted. We decided to take on Moore\u2019s Law.\nAt the time, the dot-com bubble was still rapidly inflating its way toward a resounding pop, and the existing semiconductor road map didn\u2019t extend past 2010. The critical feature size for the transistors on an integrated circuit was 350 nanometers; we had a long way to go before atomic sizes would become a limitation. And yet, the eventual end of Moore\u2019s Law was obvious. Someday semiconductor researchers would have to confront physics-based limits to their relentless descent into the infinitesimal, if for no other reason than that a transistor cannot be smaller than an atom. (Today the smallest components of transistors on integrated circuits are roughly 45 nm wide, or about 220 silicon atoms.)\nThat\u2019s when we started to hang out with Phil Kuekes, the creative force behind the Teramac (tera-operation-per-second multiarchitecture computer)\u2014an experimental supercomputer built at HP Labs primarily from defective parts, just to show it could be done. He gave us the idea to build an architecture that would work even if a substantial number of the individual devices in the circuit were dead on arrival. We didn\u2019t know what those devices would be, but our goal was electronics that would keep improving even after the devices got so small that defective ones would become common. We ate a lot of pizza washed down with appropriate amounts of beer and speculated about what this mystery nanodevice would be.\nWe were designing something that wouldn\u2019t even be relevant for another 10 to 15 years. It was possible that by then devices would have shrunk down to the molecular scale envisioned by David Packard or perhaps even be molecules. We could think of no better way to anticipate this than by mimicking the Teramac at the nanoscale. We decided that the simplest abstraction of the Teramac architecture was the crossbar, which has since become the de facto standard for nanoscale circuits because of its simplicity, adaptability, and redundancy.\nThe crossbar is an array of perpendicular wires. Anywhere two wires cross, they are connected by a switch. To connect a horizontal wire to a vertical wire at any point on the grid, you must close the switch between them. Our idea was to open and close these switches by applying voltages to the ends of the wires. Note that a crossbar array is basically a storage system, with an open switch representing a zero and a closed switch representing a one. You read the data by probing the switch with a small voltage.\nLike everything else at the nanoscale, the switches and wires of a crossbar are bound to be plagued by at least some nonfunctional components. These components will be only a few atoms wide, and the second law of thermodynamics ensures that we will not be able to completely specify the position of every atom. However, a crossbar architecture builds in redundancy by allowing you to route around any parts of the circuit that don\u2019t work. Because of their simplicity, crossbar arrays have a much higher density of switches than a comparable integrated circuit based on transistors.\nBut implementing such a storage system was easier said than done. Many research groups were working on such a cross-point memory\u2014and had been since the 1950s. Even after 40 years of research, they had no product on the market. Still, that didn\u2019t stop them from trying. That\u2019s because the potential for a truly nanoscale crossbar memory is staggering; picture carrying around the entire Library of Congress on a thumb drive.\nOne of the major impediments for prior crossbar memory research was the small off-to-on resistance ratio of the switches (40 years of research had never produced anything surpassing a factor of 2 or 3). By comparison, modern transistors have an off-to-on resistance ratio of 10 000 to 1. We calculated that to get a high-performance memory, we had to make switches with a resistance ratio of at least 1000 to 1. In other words, in its off state, a switch had to be 1000 times as resistive to the flow of current as it was in its on state. What mechanism could possibly give a nanometer-scale device a three-orders-of-magnitude resistance ratio?\nWe found the answer in scanning tunneling microscopy (STM), an area of research I had been pursuing for a decade. A tunneling microscope generates atomic-resolution images by scanning a very sharp needle across a surface and measuring the electric current that flows between the atoms at the tip of the needle and the surface the needle is probing. The general rule of thumb in STM is that moving that tip 0.1 nm closer to a surface increases the tunneling current by one order of magnitude.\nWe needed some similar mechanism by which we could change the effective spacing between two wires in our crossbar by 0.3 nm. If we could do that, we would have the 1000:1 electrical switching ratio we needed.\nOur constraints were getting ridiculous. Where would we find a material that could change its physical dimensions like that? That is how we found ourselves in the realm of molecular electronics.\nConceptually, our device was like a tiny sandwich. Two platinum electrodes (the intersecting wires of the crossbar junction) functioned as the \u201dbread\u201d on either end of the device. We oxidized the surface of the bottom platinum wire to make an extremely thin layer of platinum dioxide, which is highly conducting. Next, we assembled a dense film, only one molecule thick, of specially designed switching molecules. Over this \u201dmonolayer\u201d we deposited a 2- to 3-nm layer of titanium metal, which bonds strongly to the molecules and was intended to glue them together. The final layer was the top platinum electrode.\nThe molecules were supposed to be the actual switches. We built an enormous number of these devices, experimenting with a wide variety of exotic molecules and configurations, including rotaxanes, special switching molecules designed by James Heath and Fraser Stoddart at the University of California, Los Angeles. The rotaxane is like a bead on a string, and with the right voltage, the bead slides from one end of the string to the other, causing the electrical resistance of the molecule to rise or fall, depending on the direction it moves. Heath and Stoddart\u2019s devices used silicon electrodes, and they worked, but not well enough for technological applications: the off-to-on resistance ratio was only a factor of 10, the switching was slow, and the devices tended to switch themselves off after 15 minutes.\nOur platinum devices yielded results that were nothing less than frustrating. When a switch worked, it was spectacular: our off-to-on resistance ratios shot past the 1000 mark, the devices switched too fast for us to even measure, and having switched, the device\u2019s resistance state remained stable for years (we still have some early devices we test every now and then, and we have never seen a significant change in resistance). But our fantastic results were inconsistent. Worse yet, the success or failure of a device never seemed to depend on the same thing.\nWe had no physical model for how these devices worked. Instead of rational engineering, we were reduced to performing huge numbers of Edisonian experiments, varying one parameter at a time and attempting to hold all the rest constant. Even our switching molecules were betraying us; it seemed like we could use anything at all. In our desperation, we even turned to long-chain fatty acids\u2014essentially soap\u2014as the molecules in our devices. There\u2019s nothing in soap that should switch, and yet some of the soap devices switched phenomenally. We also made control devices with no molecule monolayers at all. None of them switched.\nWe were frustrated and burned out. Here we were, in late 2002, six years into our research. We had something that worked, but we couldn\u2019t figure out why, we couldn\u2019t model it, and we sure couldn\u2019t engineer it. That\u2019s when Greg Snider, who had worked with Kuekes on the Teramac, brought me the Chua memristor paper from the September 1971 IEEE Transactions on Circuits Theory. \u201dI don\u2019t know what you guys are building,\u201d he told me, \u201dbut this is what I want.\u201d\nTo this day, I have no idea how Greg happened to come across that paper. Few people had read it, fewer had understood it, and fewer still had cited it. At that point, the paper was 31 years old and apparently headed for the proverbial dustbin of history. I wish I could say I took one look and yelled, \u201dEureka!\u201d But in fact, the paper sat on my desk for months before I even tried to read it. When I did study it, I found the concepts and the equations unfamiliar and hard to follow. But I kept at it because something had caught my eye, as it had Greg\u2019s: Chua had included a graph that looked suspiciously similar to the experimental data we were collecting.\nThe graph described the current-voltage (I-V) characteristics that Chua had plotted for his memristor. Chua had called them \u201dpinched-hysteresis loops\u201d; we called our I-V characteristics \u201dbow ties.\u201d A pinched hysteresis loop looks like a diagonal infinity symbol with the center at the zero axis, when plotted on a graph of current against voltage. The voltage is first increased from zero to a positive maximum value, then decreased to a minimum negative value and finally returned to zero. The bow ties on our graphs were nearly identical [see graphic, \u201dBow Ties\u201d].\nThat\u2019s not all. The total change in the resistance we had measured in our devices also depended on how long we applied the voltage: the longer we applied a positive voltage, the lower the resistance until it reached a minimum value. And the longer we applied a negative voltage, the higher the resistance became until it reached a maximum limiting value. When we stopped applying the voltage, whatever resistance characterized the device was frozen in place, until we reset it by once again applying a voltage. The loop in the I-V curve is called hysteresis, and this behavior is startlingly similar to how synapses operate: synaptic connections between neurons can be made stronger or weaker depending on the polarity, strength, and length of a chemical or electrical signal. That\u2019s not the kind of behavior you find in today\u2019s circuits.\nLooking at Chua\u2019s graphs was maddening. We now had a big clue that memristance had something to do with our switches. But how? Why should our molecular junctions have anything to do with the relationship between charge and magnetic flux? I couldn\u2019t make the connection.\nTwo years went by. Every once in a while I would idly pick up Chua\u2019s paper, read it, and each time I understood the concepts a little more. But our experiments were still pretty much trial and error. The best we could do was to make a lot of devices and find the ones that worked.\nBut our frustration wasn\u2019t for nothing: by 2004, we had figured out how to do a little surgery on our little sandwiches. We built a gadget that ripped the tiny devices open so that we could peer inside them and do some forensics. When we pried them apart, the little sandwiches separated at their weakest point: the molecule layer. For the first time, we could get a good look at what was going on inside. We were in for a shock.\nWhat we had was not what we had built. Recall that we had built a sandwich with two platinum electrodes as the bread and filled with three layers: the platinum dioxide, the monolayer film of switching molecules, and the film of titanium.\nBut that\u2019s not what we found. Under the molecular layer, instead of platinum dioxide, there was only pure platinum. Above the molecular layer, instead of titanium, we found an unexpected and unusual layer of titanium dioxide. The titanium had sucked the oxygen right out of the platinum dioxide! The oxygen atoms had somehow migrated through the molecules and been consumed by the titanium. This was especially surprising because the switching molecules had not been significantly perturbed by this event\u2014they were intact and well ordered, which convinced us that they must be doing something important in the device.\nThe chemical structure of our devices was not at all what we had thought it was. The titanium dioxide\u2014a stable compound found in sunscreen and white paint\u2014was not just regular titanium dioxide. It had split itself up into two chemically different layers. Adjacent to the molecules, the oxide was stoichiometric TiO 2 , meaning the ratio of oxygen to titanium was perfect, exactly 2 to 1. But closer to the top platinum electrode, the titanium dioxide was missing a tiny amount of its oxygen, between 2 and 3 percent. We called this oxygen-deficient titanium dioxide TiO 2-x , where x is about 0.05.\nBecause of this misunderstanding, we had been performing the experiment backward. Every time I had tried to create a switching model, I had reversed the switching polarity. In other words, I had predicted that a positive voltage would switch the device off and a negative voltage would switch it on. In fact, exactly the opposite was true.\nIt was time to get to know titanium dioxide a lot better. They say three weeks in the lab will save you a day in the library every time. In August of 2006 I did a literature search and found about 300 relevant papers on titanium dioxide. I saw that each of the many different communities researching titanium dioxide had its own way of describing the compound. By the end of the month, the pieces had fallen into place. I finally knew how our device worked. I knew why we had a memristor.\nThe exotic molecule monolayer in the middle of our sandwich had nothing to do with the actual switching. Instead, what it did was control the flow of oxygen from the platinum dioxide into the titanium to produce the fairly uniform layers of TiO 2 and TiO 2-x . The key to the switching was this bilayer of the two different titanium dioxide species [see diagram, \u201dHow Memristance Works\u201d]. The TiO 2 is electrically insulating (actually a semiconductor), but the TiO 2-x is conductive, because its oxygen vacancies are donors of electrons, which makes the vacancies themselves positively charged. The vacancies can be thought of like bubbles in a glass of beer, except that they don\u2019t pop\u2014they can be pushed up and down at will in the titanium dioxide material because they are electrically charged.\nNow I was able to predict the switching polarity of the device. If a positive voltage is applied to the top electrode of the device, it will repel the (also positive) oxygen vacancies in the TiO 2-x layer down into the pure TiO 2 layer. That turns the TiO 2 layer into TiO 2-x and makes it conductive, thus turning the device on. A negative voltage has the opposite effect: the vacancies are attracted upward and back out of the TiO 2 , and thus the thickness of the TiO 2 layer increases and the device turns off. This switching polarity is what we had been seeing for years but had been unable to explain.\nOn 20 August 2006, I solved the two most important equations of my career\u2014one equation detailing the relationship between current and voltage for this equivalent circuit, and another equation describing how the application of the voltage causes the vacancies to move\u2014thereby writing down, for the first time, an equation for memristance in terms of the physical properties of a material. This provided a unique insight. Memristance arises in a semiconductor when both electrons and charged dopants are forced to move simultaneously by applying a voltage to the system. The memristance did not actually involve magnetism in this case; the integral over the voltage reflected how far the dopants had moved and thus how much the resistance of the device had changed.\nWe finally had a model we could use to engineer our switches, which we had by now positively identified as memristors. Now we could use all the theoretical machinery Chua had created to help us design new circuits with our devices.\nTriumphantly, I showed the group my results and immediately declared that we had to take the molecule monolayers out of our devices. Skeptical after years of false starts and failed hypotheses, my team reminded me that we had run control samples without molecule layers for every device we had ever made and that those devices had never switched. And getting the recipe right turned out to be tricky indeed. We needed to find the exact amounts of titanium and oxygen to get the two layers to do their respective jobs. By that point we were all getting impatient. In fact, it took so long to get the first working device that in my discouragement I nearly decided to put the molecule layers back in.\nA month later, it worked. We not only had working devices, but we were also able to improve and change their characteristics at will.\nBut here is the real triumph. The resistance of these devices stayed constant whether we turned off the voltage or just read their states (interrogating them with a voltage so small it left the resistance unchanged). The oxygen vacancies didn\u2019t roam around; they remained absolutely immobile until we again applied a positive or negative voltage. That\u2019s memristance: the devices remembered their current history. We had coaxed Chua\u2019s mythical memristor off the page and into being.\nEmulating the behavior of a single memristor, Chua showed, requires a circuit with at least 15 transistors and other passive elements. The implications are extraordinary: just imagine how many kinds of circuits could be supercharged by replacing a handful of transistors with one single memristor.\nThe most obvious benefit is to memories. In its initial state, a crossbar memory has only open switches, and no information is stored. But once you start closing switches, you can store vast amounts of information compactly and efficiently. Because memristors remember their state, they can store data indefinitely, using energy only when you toggle or read the state of a switch, unlike the capacitors in conventional DRAM, which will lose their stored charge if the power to the chip is turned off. Furthermore, the wires and switches can be made very small: we should eventually get down to a width of around 4 nm, and then multiple crossbars could be stacked on top of each other to create a ridiculously high density of stored bits.\nGreg Snider and I published a paper last year showing that memristors could vastly improve one type of processing circuit, called a field-programmable gate array, or FPGA. By replacing several specific transistors with a crossbar of memristors, we showed that the circuit could be shrunk by nearly a factor of 10 in area and improved in terms of its speed relative to power-consumption performance. Right now, we are testing a prototype of this circuit in our lab.\nAnd memristors are by no means hard to fabricate. The titanium dioxide structure can be made in any semiconductor fab currently in existence. (In fact, our hybrid circuit was built in an HP fab used for making inkjet cartridges.) The primary limitation to manufacturing hybrid chips with memristors is that today only a small number of people on Earth have any idea of how to design circuits containing memristors. I must emphasize here that memristors will never eliminate the need for transistors: passive devices and circuits require active devices like transistors to supply energy.\nThe potential of the memristor goes far beyond juicing a few FPGAs. I have referred several times to the similarity of memristor behavior to that of synapses. Right now, Greg is designing new circuits that mimic aspects of the brain. The neurons are implemented with transistors, the axons are the nanowires in the crossbar, and the synapses are the memristors at the cross points. A circuit like this could perform real-time data analysis for multiple sensors. Think about it: an intelligent physical infrastructure that could provide structural assessment monitoring for bridges. How much money\u2014and how many lives\u2014could be saved?\nI\u2019m convinced that eventually the memristor will change circuit design in the 21st century as radically as the transistor changed it in the 20th. Don\u2019t forget that the transistor was lounging around as a mainly academic curiosity for a decade until 1956, when a killer app\u2014the hearing aid\u2014brought it into the marketplace. My guess is that the real killer app for memristors will be invented by a curious student who is now just deciding what EE courses to take next year.\nAbout the Author\nR. STANLEY WILLIAMS, a senior fellow at Hewlett-Packard Labs, wrote this month\u2019s cover story, \u201dHow We Found the Missing Memristor.\u201d Earlier this year, he and his colleagues shook up the electrical engineering community by introducing a fourth fundamental circuit design element. The existence of this element, the memristor, was first predicted in 1971 by IEEE Fellow Leon Chua, of the University of California, Berkeley, but it took Williams 12 years to build an actual device.", "id": "<urn:uuid:fc30d469-0a3c-4993-a11a-95b648c6e637>", "dump": "CC-MAIN-2013-20", "url": "http://spectrum.ieee.org/semiconductors/processors/how-we-found-the-missing-memristor/5", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9618540406227112, "token_count": 6717, "score": 3.171875, "int_score": 3}
{"text": "If it wasn\u2019t for earthquakes, humans wouldn\u2019t have innovated architecture\nThey wouldn\u2019t have looked into new ways of building homes, but the problem is that we got good at it \u2013 good to the points our homes won\u2019t be destroyed frequently enough aka they won\u2019t evolute frequently.\nIf you look around you, there is very few free space - and in those spots you find big centers being eradicated everyday \u2013 safe and resistant enough \u2013 specially to earthquakes \u2013 what on earth will take down those inefficient dumb primitive beton monsters and make room for better buildings in the future ?\nSo the problem behind this is the ever expanding gap between technology and architecture : our homes will always be behind technology/progress \u2013 they will be always less optimal.\nI can only imagine how better the earth will be if our houses were \u201csmart\u201d or modern enough \u2013 it is not science fiction \u2013 the way we build stuff is very retarded to say the least when it comes to the material used, energy saving, what a home can \u201cdo\u201d and it is just not possible \u201cbusiness wise\u201d to say : ok, let us destroy and rebuild.\nBefore, nature took care of this, slowly and \u201cless painfully\u201d\nAs little earthquakes happened, our primitive cities got \u201cdevastated\u201d, we rebuilt them in a better way but the costs were small.\nWe kept gradually improving till our cities became resistant to medium/high earthquakes.\nWe reached this point of the graph where things slow down, become stable \u2013 it is cool not to have the tragedy and misery of earthquakes, but on the other hand there is the hidden and expensive cost of stability and non-progress. It is invisible and super slow but as devastating in its effect as that 2 minutes tragedy called earthquake\nOur homes are costing the earth dearly and suffocating it \u2013 we need earthquakes to give engineers another better large-scale chance/try.\nBefore I start sounding too embarrassingly enthusiastic about earthquakes and destruction, here is a link on list of earthquakes \u2013 it has\n- Main lists of earthquakes\n- Historical earthquakes (before 1901)\n- List of 20th century earthquakes (1901\u20132000)\n- List of 21st century earthquakes (2001\u2013present)\n- Lists of earthquakes by country\n- Largest earthquakes by magnitude\n- Deadliest earthquakes on record\nEnjoy the read !Read More", "id": "<urn:uuid:232a566f-fd11-4f6c-a2c2-97a9dce2484f>", "dump": "CC-MAIN-2013-20", "url": "http://mireille.it/2010/10/", "file_path": "s3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz", "language": "en", "language_score": 0.9593789577484131, "token_count": 499, "score": 2.65625, "int_score": 3}
